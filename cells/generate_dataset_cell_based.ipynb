{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_model</th>\n",
       "      <th>stain</th>\n",
       "      <th>margins</th>\n",
       "      <th>num_sample_per_class</th>\n",
       "      <th>stacks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>cell</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           network_model  stain  margins  num_sample_per_class stacks\n",
       "dataset_id                                                           \n",
       "20          Inception-BN  nissl  200/500                  1000  MD585\n",
       "21          Inception-BN  nissl  200/500                  1000  MD589\n",
       "22          Inception-BN  nissl  200/500                  1000  MD594\n",
       "99                  cell  nissl      500                  1000  MD589"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 99\n",
    "dataset_properties = dataset_settings.loc[dataset_id]\n",
    "\n",
    "num_samples_per_label = dataset_properties['num_sample_per_class']\n",
    "stacks = dataset_properties['stacks'].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "structures_to_sample = all_known_structures\n",
    "\n",
    "negative_labels_to_sample = [s + '_negative' for s in structures_to_sample]\n",
    "\n",
    "margins_to_sample = map(int, str(dataset_properties['margins']).split('/'))\n",
    "\n",
    "surround_positive_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix=surr_l) \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample \n",
    "                             for surr_l in structures_to_sample\n",
    "                             if surr_l != s]\n",
    "\n",
    "surround_noclass_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix='noclass') \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample]\n",
    "\n",
    "labels_to_sample = structures_to_sample + negative_labels_to_sample + \\\n",
    "surround_positive_labels_to_sample + surround_noclass_labels_to_sample + \\\n",
    "['noclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cell_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_regions_one_section_by_label(stack, sec, margins_to_sample, labeled_contours, num_samples_per_label=None):\n",
    "    \"\"\"\n",
    "    Sample region addresses in a particular section and associate them with different labels (positive, surround, negative, noclass, foreground, background, etc.).\n",
    "    \n",
    "    Args:\n",
    "        region_contours (list of nx2 arrays): list of contour vertices.\n",
    "        margins_to_sample (list of ints):\n",
    "        labeled_contours (dict of nx2 arrays): {label: contour vertices}\n",
    "        \n",
    "    Returns:\n",
    "        dict of 3-tuple list: {label: list of (stack, section, region_index)}        \n",
    "    \"\"\"\n",
    "\n",
    "    addresses = {}\n",
    "\n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        return\n",
    "\n",
    "    region_contours = load_cell_classifier_data(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "    region_labels = label_regions(stack=stack, section=sec, \n",
    "                                  region_contours=region_contours,\n",
    "                                  surround_margins=margins_to_sample,\n",
    "                                  labeled_contours=labeled_contours)\n",
    "\n",
    "    for label, region_indices in region_labels.iteritems():\n",
    "        if label == 'bg' or len(region_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        if num_samples_per_label is None:\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in region_indices]\n",
    "        else:\n",
    "            sampled_region_indices = np.random.choice(region_indices, min(num_samples_per_label, len(region_indices)), replace=False)\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in sampled_region_indices]\n",
    "\n",
    "    return addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cell_based_features_one_section_(stack, section, region_indices):\n",
    "    \"\"\"\n",
    "    Load pre-computed cell-based features for a list of regions on a particular section.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_features_all_regions = load_cell_classifier_data(what='region_features', stack=stack, sec=section, ext='hdf')\n",
    "    # Loading hdf ~ 2 seconds.\n",
    "    \n",
    "    features1 = np.asarray([rf['largeOrientationHist'] for rf in region_features_all_regions])\n",
    "    features2 = np.asarray([rf['largeSizeHist'] for rf in region_features_all_regions])\n",
    "    features3 = np.asarray([rf['largeLargeLinkLenHist'] for rf in region_features_all_regions])\n",
    "    features4 = np.asarray([rf['largeSmallLinkLenHist'] for rf in region_features_all_regions])\n",
    "    \n",
    "        \n",
    "    f1 = features1[region_indices]\n",
    "    f1n = f1/f1.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    f2 = features2[region_indices]\n",
    "    f2n = f2/f2.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    f3 = features3[region_indices]\n",
    "    f3n = f3/f3.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    f4 = features4[region_indices]\n",
    "    f4n = f4/f4.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    features = np.c_[f1n, f2n, f3n, f4n]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_cell_based(num_samples_per_label, stacks, labels_to_sample):\n",
    "    \"\"\"\n",
    "    Generate dataset.\n",
    "    - Extract addresses\n",
    "    - Map addresses to features\n",
    "    - Remove None features\n",
    "    \n",
    "    Returns:\n",
    "        features, addresseslab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample addresses\n",
    "    \n",
    "    addresses = defaultdict(list)\n",
    "    labels_found = set([])\n",
    "    addresses_by_section_by_label = []\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    for stack in stacks:\n",
    "        \n",
    "        first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "        t1 = time.time()\n",
    "\n",
    "        contours_df, _ = DataManager.load_annotation_v3(stack=stack)\n",
    "        labeled_contours = contours_df[(contours_df['orientation'] == 'sagittal') & (contours_df['downsample'] == 1)].drop_duplicates(subset=['section', 'name', 'side', 'filename', 'downsample', 'creator'])\n",
    "        labeled_contours = convert_annotation_v3_original_to_aligned_cropped(labeled_contours, stack=stack)\n",
    "\n",
    "        sys.stderr.write('Load annotation. Time: %.2f seconds.\\n' % (time.time() - t1))\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Sample addresses from each section\n",
    "\n",
    "        pool = Pool(NUM_CORES/2)\n",
    "        addresses_by_section_by_label_curr_stack = \\\n",
    "        pool.map(lambda sec: sample_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "                                                                 margins_to_sample=margins_to_sample,\n",
    "                                                                 labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "                                                                 num_samples_per_label=30),\n",
    "                 range(first_sec, last_sec+1))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "                \n",
    "        addresses_by_section_by_label += addresses_by_section_by_label_curr_stack\n",
    "\n",
    "        sys.stderr.write('Sample addresses (stack %s): %.2s seconds.\\n' % (stack, time.time() - t1))\n",
    "        \n",
    "    # Aggregate addresses sampled form each section\n",
    "    \n",
    "    addresses_by_label = defaultdict(list)\n",
    "    for addrs_by_label in addresses_by_section_by_label:\n",
    "        if addrs_by_label is not None:\n",
    "            for label, addrs in addrs_by_label.iteritems():\n",
    "                addresses_by_label[label] += addrs\n",
    "    addresses_by_label.default_factory = None\n",
    "    \n",
    "    if num_samples_per_label is not None:\n",
    "        import random\n",
    "        addresses_by_label = {label: random.sample(addrs, min(num_samples_per_label/len(stacks), len(addrs))) \n",
    "                              for label, addrs in addresses_by_label.iteritems()}\n",
    "    \n",
    "    sys.stderr.write('Sample addresses: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove unwanted labels\n",
    "    addresses_by_label = {label: addrs for label, addrs in addresses_by_label.iteritems() if label in labels_to_sample}\n",
    "    \n",
    "    # Map addresses to features\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    load_cell_based_features_given_address_list = lambda addrs: smart_map(addrs, keyfunc=lambda (st, se, ri): (st, se),\n",
    "                       func=lambda (st, se), gr: load_cell_based_features_one_section_(st, se, [ri for _,_,ri in gr]))\n",
    "    features_by_label = apply_function_to_dict(load_cell_based_features_given_address_list, addresses_by_label)\n",
    "    features_by_label = apply_function_to_dict(np.asarray, addresses_by_label)\n",
    "    \n",
    "    sys.stderr.write('Map addresses to features: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove features that are None\n",
    "\n",
    "    for name in labels_found:\n",
    "        valid = [(ftr, addr) for ftr, addr in zip(features_by_label[name], addresses_by_label[name])\n",
    "                    if ftr is not None]\n",
    "        res = zip(*valid)\n",
    "        features_by_label[name] = np.array(res[0])\n",
    "        addresses_by_label[name] = res[1]\n",
    "    \n",
    "    return features_by_label, addresses_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'No object named structures in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotation has no structures.\n",
      "Load annotation. Time: 1.40 seconds.\n",
      "Analyzing section 92..\n",
      "Analyzing section 93..\n",
      "Analyzing section 94..\n",
      "Analyzing section 95..\n",
      "Analyzing section 96..\n",
      "Analyzing section 101..\n",
      "Analyzing section 97..\n",
      "Analyzing section 102..\n",
      "Analyzing section 98..\n",
      "Analyzing section 103..\n",
      "Analyzing section 110..\n",
      "Analyzing section 99..\n",
      "Analyzing section 104..\n",
      "Analyzing section 100..\n",
      "Analyzing section 111..\n",
      "Analyzing section 119..\n",
      "Analyzing section 105..\n",
      "Analyzing section 112..\n",
      "Analyzing section 128..\n",
      "Analyzing section 120..\n",
      "Analyzing section 106..\n",
      "Analyzing section 113..\n",
      "Analyzing section 137..\n",
      "Analyzing section 107..\n",
      "Analyzing section 129..\n",
      "Analyzing section 121..\n",
      "Analyzing section 114..\n",
      "Analyzing section 146..\n",
      "Analyzing section 108..\n",
      "Analyzing section 130..\n",
      "Analyzing section 115..\n",
      "Analyzing section 122..\n",
      "Analyzing section 155..\n",
      "Analyzing section 109..\n",
      "Analyzing section 138..\n",
      "Analyzing section 116..\n",
      "Analyzing section 123..\n",
      "Analyzing section 147..\n",
      "Analyzing section 131..\n",
      "Analyzing section 164..\n",
      "Analyzing section 117..\n",
      "Analyzing section 124..\n",
      "Analyzing section 139..\n",
      "Analyzing section 173..\n",
      "Analyzing section 132..\n",
      "Analyzing section 125..\n",
      "Analyzing section 156..\n",
      "Analyzing section 148..\n",
      "Analyzing section 182..\n",
      "Analyzing section 126..\n",
      "Analyzing section 140..\n",
      "Analyzing section 174..\n",
      "Analyzing section 133..\n",
      "Analyzing section 149..\n",
      "Analyzing section 165..\n",
      "Analyzing section 127..\n",
      "Analyzing section 183..\n",
      "Analyzing section 157..\n",
      "Analyzing section 141..\n",
      "Analyzing section 134..\n",
      "Analyzing section 191..\n",
      "Analyzing section 175..\n",
      "Analyzing section 150..\n",
      "Analyzing section 166..\n",
      "Analyzing section 135..\n",
      "Analyzing section 142..\n",
      "Analyzing section 184..\n",
      "Analyzing section 192..\n",
      "Analyzing section 158..\n",
      "Analyzing section 151..\n",
      "Analyzing section 176..\n",
      "Analyzing section 136..\n",
      "Analyzing section 193..\n",
      "Analyzing section 143..\n",
      "Analyzing section 185..\n",
      "Analyzing section 167..\n",
      "Analyzing section 152..\n",
      "Analyzing section 177..\n",
      "Analyzing section 159..\n",
      "Analyzing section 194..\n",
      "Analyzing section 200..\n",
      "Analyzing section 144..\n",
      "Analyzing section 186..\n",
      "Analyzing section 168..\n",
      "Analyzing section 178..\n",
      "Analyzing section 153..\n",
      "Analyzing section 145..\n",
      "Analyzing section 195..\n",
      "Analyzing section 160..\n",
      "Analyzing section 201..\n",
      "Analyzing section 187..\n",
      "Analyzing section 196..\n",
      "Analyzing section 154..\n",
      "Analyzing section 202..\n",
      "Analyzing section 179..\n",
      "Analyzing section 169..\n",
      "Analyzing section 209..\n",
      "Analyzing section 161..\n",
      "Analyzing section 188..\n",
      "Analyzing section 197..\n",
      "Analyzing section 203..\n",
      "Analyzing section 180..\n",
      "Analyzing section 210..\n",
      "Analyzing section 189..\n",
      "Analyzing section 218..\n",
      "Analyzing section 198..\n",
      "Analyzing section 204..\n",
      "Analyzing section 170..\n",
      "Analyzing section 162..\n",
      "Analyzing section 199..\n",
      "Analyzing section 181..\n",
      "Analyzing section 211..\n",
      "Analyzing section 219..\n",
      "Analyzing section 205..\n",
      "Analyzing section 190..\n",
      "Analyzing section 171..\n",
      "Analyzing section 163..\n",
      "Analyzing section 227..\n",
      "Analyzing section 212..\n",
      "Analyzing section 220..\n",
      "Analyzing section 206..\n",
      "Analyzing section 172..\n",
      "Analyzing section 236..\n",
      "Analyzing section 228..\n",
      "Analyzing section 245..\n",
      "Analyzing section 229..\n",
      "Analyzing section 237..\n",
      "Analyzing section 222..\n",
      "Analyzing section 254..\n",
      "Analyzing section 213..\n",
      "Analyzing section 207..\n",
      "Analyzing section 246..\n",
      "Analyzing section 230..\n",
      "Analyzing section 238..\n",
      "Analyzing section 263..\n",
      "Analyzing section 255..\n",
      "Analyzing section 223..\n",
      "Analyzing section 214..\n",
      "Analyzing section 208..\n",
      "Analyzing section 247..\n",
      "Analyzing section 239..\n",
      "Analyzing section 231..\n",
      "Analyzing section 264..\n",
      "Analyzing section 256..\n",
      "Analyzing section 248..\n",
      "Analyzing section 240..\n",
      "Analyzing section 224..\n",
      "Analyzing section 232..\n",
      "Analyzing section 215..\n",
      "Analyzing section 272..\n",
      "Analyzing section 265..\n",
      "Analyzing section 249..\n",
      "Analyzing section 257..\n",
      "Analyzing section 225..\n",
      "Analyzing section 233..\n",
      "Analyzing section 241..\n",
      "Analyzing section 273..\n",
      "Analyzing section 250..\n",
      "Analyzing section 216..\n",
      "Analyzing section 266..\n",
      "Analyzing section 258..\n",
      "Analyzing section 242..\n",
      "Analyzing section 226..\n",
      "Analyzing section 234..\n",
      "Analyzing section 274..\n",
      "Analyzing section 243..\n",
      "Analyzing section 251..\n",
      "Analyzing section 259..\n",
      "Analyzing section 217..\n",
      "Analyzing section 267..\n",
      "Analyzing section 235..\n",
      "Analyzing section 281..\n",
      "Analyzing section 244..\n",
      "Analyzing section 275..\n",
      "Analyzing section 260..\n",
      "Analyzing section 252..\n",
      "Analyzing section 268..\n",
      "Analyzing section 290..\n",
      "Analyzing section 282..\n",
      "Analyzing section 253..\n",
      "Analyzing section 276..\n",
      "Analyzing section 269..\n",
      "Analyzing section 299..\n",
      "Analyzing section 261..\n",
      "Analyzing section 291..\n",
      "Analyzing section 308..\n",
      "Analyzing section 270..\n",
      "Analyzing section 283..\n",
      "Analyzing section 317..\n",
      "Analyzing section 277..\n",
      "Analyzing section 262..\n",
      "Analyzing section 301..\n",
      "Analyzing section 309..\n",
      "Analyzing section 292..\n",
      "Analyzing section 271..\n",
      "Analyzing section 318..\n",
      "Analyzing section 278..\n",
      "Analyzing section 326..\n",
      "Analyzing section 284..\n",
      "Analyzing section 319..\n",
      "Analyzing section 302..\n",
      "Analyzing section 310..\n",
      "Analyzing section 327..\n",
      "Analyzing section 335..\n",
      "Analyzing section 293..\n",
      "Analyzing section 320..\n",
      "Analyzing section 279..\n",
      "Analyzing section 328..\n",
      "Analyzing section 285..\n",
      "Analyzing section 311..\n",
      "Analyzing section 336..\n",
      "Analyzing section 303..\n",
      "Analyzing section 321..\n",
      "Analyzing section 329..\n",
      "Analyzing section 337..\n",
      "Analyzing section 330..\n",
      "Analyzing section 294..\n",
      "Analyzing section 322..\n",
      "Analyzing section 312..\n",
      "Analyzing section 280..\n",
      "Analyzing section 331..\n",
      "Analyzing section 286..\n",
      "Analyzing section 338..\n",
      "Analyzing section 304..\n",
      "Analyzing section 323..\n",
      "Analyzing section 339..\n",
      "Analyzing section 332..\n",
      "Analyzing section 313..\n",
      "Analyzing section 340..\n",
      "Analyzing section 333..\n",
      "Analyzing section 295..\n",
      "Analyzing section 325..\n",
      "Analyzing section 287..\n",
      "Analyzing section 344..\n",
      "Analyzing section 314..\n",
      "Analyzing section 341..\n",
      "Analyzing section 334..\n",
      "Analyzing section 305..\n",
      "Analyzing section 345..\n",
      "Analyzing section 342..\n",
      "Analyzing section 353..\n",
      "Analyzing section 346..\n",
      "Analyzing section 315..\n",
      "Analyzing section 343..\n",
      "Analyzing section 354..\n",
      "Analyzing section 347..\n",
      "Analyzing section 362..\n",
      "Analyzing section 355..\n",
      "Analyzing section 363..\n",
      "Analyzing section 348..\n",
      "Analyzing section 306..\n",
      "Analyzing section 288..\n",
      "Analyzing section 364..\n",
      "Analyzing section 316..\n",
      "Analyzing section 356..\n",
      "Analyzing section 365..\n",
      "Analyzing section 349..\n",
      "Analyzing section 296..\n",
      "Analyzing section 366..\n",
      "Analyzing section 357..\n",
      "Analyzing section 367..\n",
      "Analyzing section 358..\n",
      "Analyzing section 368..\n",
      "Analyzing section 350..\n",
      "Analyzing section 369..\n",
      "Analyzing section 359..\n",
      "Analyzing section 370..\n",
      "Analyzing section 351..\n",
      "Analyzing section 360..\n",
      "Analyzing section 307..\n",
      "Analyzing section 361..\n",
      "Analyzing section 352..\n",
      "Analyzing section 289..\n",
      "Analyzing section 297..\n",
      "Analyzing section 298..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'addresses_by_section_curr_stack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-34cad726baab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m features, addresses, labels_found = generate_dataset_cell_based(num_samples_per_label=num_samples_per_label, \n\u001b[1;32m      2\u001b[0m                                                      \u001b[0mstacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                                      labels_to_sample=['5N'])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-1cbcce621f1f>\u001b[0m in \u001b[0;36mgenerate_dataset_cell_based\u001b[0;34m(num_samples_per_label, stacks, labels_to_sample)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0maddresses_by_section_by_label\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maddresses_by_section_curr_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sample addresses (stack %s): %.2s seconds.\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 13 seconds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'addresses_by_section_curr_stack' is not defined"
     ]
    }
   ],
   "source": [
    "features, addresses, labels_found = generate_dataset_cell_based(num_samples_per_label=num_samples_per_label, \n",
    "                                                     stacks=stacks,\n",
    "                                                     labels_to_sample=['5N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save training features\n",
    "features_fp = os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_features.hdf')\n",
    "create_parent_dir_if_not_exists(features_fp)\n",
    "save_hdf_v2(features, features_fp)\n",
    "upload_from_ec2_to_s3(features_fp)\n",
    "# train_feat_dir = create_if_not_exists(os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset, 'patch_features'))\n",
    "# for label, feats in training_features.iteritems():\n",
    "#     bp.pack_ndarray_file(feats, os.path.join(train_feat_dir, label + '.bp'))\n",
    "\n",
    "# Save training addresses\n",
    "addresses_fp = os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_addresses.pkl')\n",
    "save_pickle(addresses, addresses_fp)\n",
    "upload_from_ec2_to_s3(addresses_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
