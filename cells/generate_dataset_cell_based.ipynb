{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_model</th>\n",
       "      <th>stain</th>\n",
       "      <th>margins</th>\n",
       "      <th>num_sample_per_class</th>\n",
       "      <th>stacks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>cell</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           network_model  stain  margins  num_sample_per_class stacks\n",
       "dataset_id                                                           \n",
       "20          Inception-BN  nissl  200/500                  1000  MD585\n",
       "21          Inception-BN  nissl  200/500                  1000  MD589\n",
       "22          Inception-BN  nissl  200/500                  1000  MD594\n",
       "99                  cell  nissl      500                  1000  MD589"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 99\n",
    "dataset_properties = dataset_settings.loc[dataset_id]\n",
    "\n",
    "num_samples_per_label = dataset_properties['num_sample_per_class']\n",
    "stacks = dataset_properties['stacks'].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "structures_to_sample = all_known_structures\n",
    "\n",
    "negative_labels_to_sample = [s + '_negative' for s in structures_to_sample]\n",
    "\n",
    "margins_to_sample = map(int, str(dataset_properties['margins']).split('/'))\n",
    "\n",
    "surround_positive_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix=surr_l) \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample \n",
    "                             for surr_l in structures_to_sample\n",
    "                             if surr_l != s]\n",
    "\n",
    "surround_noclass_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix='noclass') \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample]\n",
    "\n",
    "labels_to_sample = structures_to_sample + negative_labels_to_sample + \\\n",
    "surround_positive_labels_to_sample + surround_noclass_labels_to_sample + \\\n",
    "['noclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cell_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_regions_one_section_by_label(stack, sec, margins_to_sample, labeled_contours, num_samples_per_label=None):\n",
    "    \"\"\"\n",
    "    Sample region addresses in a particular section and associate them with different labels (positive, surround, negative, noclass, foreground, background, etc.).\n",
    "    \n",
    "    Args:\n",
    "        region_contours (list of nx2 arrays): list of contour vertices.\n",
    "        margins_to_sample (list of ints):\n",
    "        labeled_contours (dict of nx2 arrays): {label: contour vertices}\n",
    "        \n",
    "    Returns:\n",
    "        dict of 3-tuple list: {label: list of (stack, section, region_index)}        \n",
    "    \"\"\"\n",
    "\n",
    "    addresses = {}\n",
    "\n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        return\n",
    "\n",
    "    region_contours = load_cell_classifier_data(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "    region_labels = label_regions(stack=stack, section=sec, \n",
    "                                  region_contours=region_contours,\n",
    "                                  surround_margins=margins_to_sample,\n",
    "                                  labeled_contours=labeled_contours)\n",
    "\n",
    "    for label, region_indices in region_labels.iteritems():\n",
    "        if label == 'bg' or len(region_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        if num_samples_per_label is None:\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in region_indices]\n",
    "        else:\n",
    "            sampled_region_indices = np.random.choice(region_indices, min(num_samples_per_label, len(region_indices)), replace=False)\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in sampled_region_indices]\n",
    "\n",
    "    return addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cell_based_features_one_section_(stack, section, region_indices):\n",
    "    \"\"\"\n",
    "    Load pre-computed cell-based features for a list of regions on a particular section.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_features_all_regions = load_cell_classifier_data(what='region_features', stack=stack, sec=section, ext='hdf')\n",
    "    # Loading hdf ~ 2 seconds.\n",
    "    \n",
    "    features1 = np.asarray([rf['largeOrientationHist'] for rf in region_features_all_regions])\n",
    "    features2 = np.asarray([rf['largeSizeHist'] for rf in region_features_all_regions])\n",
    "    features3 = np.asarray([rf['largeLargeLinkLenHist'] for rf in region_features_all_regions])\n",
    "    features4 = np.asarray([rf['largeSmallLinkLenHist'] for rf in region_features_all_regions])\n",
    "    \n",
    "        \n",
    "    f1 = features1[region_indices]\n",
    "    f1n = f1/f1.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    f2 = features2[region_indices]\n",
    "    f2n = f2/f2.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    f3 = features3[region_indices]\n",
    "    f3n = f3/f3.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    f4 = features4[region_indices]\n",
    "    f4n = f4/f4.sum(axis=1)[:,None].astype(np.float)\n",
    "    \n",
    "    features = np.c_[f1n, f2n, f3n, f4n]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_cell_based(num_samples_per_label, stacks, labels_to_sample):\n",
    "    \"\"\"\n",
    "    Generate dataset.\n",
    "    - Extract addresses\n",
    "    - Map addresses to features\n",
    "    - Remove None features\n",
    "    \n",
    "    Returns:\n",
    "        features, addresseslab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample addresses\n",
    "    \n",
    "    addresses = defaultdict(list)\n",
    "    labels_found = set([])\n",
    "    addresses_by_section_by_label = []\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    for stack in stacks:\n",
    "        \n",
    "        first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "        t1 = time.time()\n",
    "\n",
    "        contours_df, _ = DataManager.load_annotation_v3(stack=stack)\n",
    "        labeled_contours = contours_df[(contours_df['orientation'] == 'sagittal') & (contours_df['downsample'] == 1)].drop_duplicates(subset=['section', 'name', 'side', 'filename', 'downsample', 'creator'])\n",
    "        labeled_contours = convert_annotation_v3_original_to_aligned_cropped(labeled_contours, stack=stack)\n",
    "\n",
    "        sys.stderr.write('Load annotation. Time: %.2f seconds.\\n' % (time.time() - t1))\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Sample addresses from each section\n",
    "\n",
    "        pool = Pool(NUM_CORES/2)\n",
    "        addresses_by_section_by_label_curr_stack = \\\n",
    "        pool.map(lambda sec: sample_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "                                                                 margins_to_sample=margins_to_sample,\n",
    "                                                                 labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "                                                                 num_samples_per_label=30),\n",
    "                 range(first_sec, last_sec+1))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "                \n",
    "        addresses_by_section_by_label += addresses_by_section_curr_stack\n",
    "\n",
    "        sys.stderr.write('Sample addresses (stack %s): %.2s seconds.\\n' % (stack, time.time() - t1)) # 13 seconds.\n",
    "        \n",
    "    # Aggregate addresses sampled form each section\n",
    "    \n",
    "    addresses_by_label = defaultdict(list)\n",
    "    for addrs_by_label in addresses_by_section_by_label:\n",
    "        if addrs_by_label is not None:\n",
    "            for label, addrs in addrs_by_label.iteritems():\n",
    "                addresses_by_label[label] += addrs\n",
    "    addresses_by_label.default_factory = None\n",
    "    \n",
    "    if num_samples_per_label is not None:\n",
    "        addresses_by_label = {label: random.sample(addrs, min(num_samples_per_label/len(stacks), len(addrs))) \n",
    "                              for label, addrs in addresses_by_label.iteritems()}\n",
    "    \n",
    "    sys.stderr.write('Sample addresses: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove unwanted labels\n",
    "    addresses_by_label = {label: addrs for label, addrs in addresses_by_label.iteritems() if label in labels_to_sample}\n",
    "    \n",
    "    # Map addresses to features\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    load_cell_based_features_given_address_list = lambda addrs: smart_map(addrs, keyfunc=lambda (st, se, ri): (st, se),\n",
    "                       func=lambda (st, se), gr: load_cell_based_features_one_section_(st, se, [ri for _,_,ri in gr]))\n",
    "    features_by_label = apply_function_to_dict(load_cell_based_features_given_address_list, addresses_by_label)\n",
    "    features_by_label = apply_function_to_dict(np.asarray, addresses_by_label)\n",
    "    \n",
    "    sys.stderr.write('Map addresses to features: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove features that are None\n",
    "\n",
    "    for name in labels_found:\n",
    "        valid = [(ftr, addr) for ftr, addr in zip(features_by_label[name], addresses_by_label[name])\n",
    "                    if ftr is not None]\n",
    "        res = zip(*valid)\n",
    "        features_by_label[name] = np.array(res[0])\n",
    "        addresses_by_label[name] = res[1]\n",
    "    \n",
    "    return features_by_label, addresses_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'No object named structures in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotation has no structures.\n",
      "Load annotation. Time: 1.40 seconds.\n",
      "Analyzing section 92..\n",
      "Analyzing section 93..\n",
      "Analyzing section 94..\n",
      "Analyzing section 95..\n",
      "Analyzing section 96..\n",
      "Analyzing section 101..\n",
      "Analyzing section 97..\n",
      "Analyzing section 102..\n",
      "Analyzing section 98..\n",
      "Analyzing section 103..\n",
      "Analyzing section 110..\n",
      "Analyzing section 99..\n",
      "Analyzing section 104..\n",
      "Analyzing section 100..\n",
      "Analyzing section 111..\n",
      "Analyzing section 119..\n",
      "Analyzing section 105..\n",
      "Analyzing section 112..\n",
      "Analyzing section 128..\n",
      "Analyzing section 120..\n",
      "Analyzing section 106..\n",
      "Analyzing section 113..\n",
      "Analyzing section 137..\n",
      "Analyzing section 107..\n",
      "Analyzing section 129..\n",
      "Analyzing section 121..\n",
      "Analyzing section 114..\n"
     ]
    }
   ],
   "source": [
    "features, addresses, labels_found = generate_dataset_cell_based(num_samples_per_label=num_samples_per_label, \n",
    "                                                     stacks=stacks,\n",
    "                                                     labels_to_sample=['5N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save training features\n",
    "features_fp = os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_features.hdf')\n",
    "create_parent_dir_if_not_exists(features_fp)\n",
    "save_hdf_v2(features, features_fp)\n",
    "upload_from_ec2_to_s3(features_fp)\n",
    "# train_feat_dir = create_if_not_exists(os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset, 'patch_features'))\n",
    "# for label, feats in training_features.iteritems():\n",
    "#     bp.pack_ndarray_file(feats, os.path.join(train_feat_dir, label + '.bp'))\n",
    "\n",
    "# Save training addresses\n",
    "addresses_fp = os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_addresses.pkl')\n",
    "save_pickle(addresses, addresses_fp)\n",
    "upload_from_ec2_to_s3(addresses_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
