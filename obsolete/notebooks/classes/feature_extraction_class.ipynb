{
 "metadata": {
  "name": "",
  "signature": "sha256:a0e8bad2d06dae32fd8ad327e286cb4b8073bdcd4b848e7b22811cd95f82e5d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %load_ext autoreload\n",
      "# %autoreload 2\n",
      "\n",
      "# import sigboost\n",
      "import numpy as np\n",
      "import cv2\n",
      "import argparse, os, json, pprint\n",
      "import random\n",
      "import itertools\n",
      "from skimage.filter import gabor_kernel\n",
      "\n",
      "from multiprocessing import Pool\n",
      "# from pathos.multiprocessing import ProcessingPool as Pool\n",
      "\n",
      "from scipy.signal import fftconvolve\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "\n",
      "\n",
      "def unwrap_convolve_per_proc(args, **kwargs):\n",
      "    return FeatureExtractor.convolve_per_proc(*args, **kwargs)\n",
      "\n",
      "def unwrap_compute_dist_per_proc(args, **kwargs):\n",
      "    return FeatureExtractor.compute_dist_per_proc(*args, **kwargs)\n",
      "\n",
      "class FeatureExtractor(object):\n",
      "\n",
      "    def __init__(self, img, param):\n",
      "        self.img = img\n",
      "        self.param = param\n",
      "        \n",
      "    def get_kernels(self):\n",
      "\n",
      "        theta_interval = self.param['theta_interval']\n",
      "        self.n_angle = int(180/theta_interval)\n",
      "        freq_step = self.param['freq_step']\n",
      "        freq_max = 1./self.param['min_wavelen']\n",
      "        freq_min = 1./self.param['max_wavelen']\n",
      "        bandwidth = self.param['bandwidth']\n",
      "        self.n_freq = int(np.log(freq_max/freq_min)/np.log(freq_step)) + 1\n",
      "        frequencies = freq_max/freq_step**np.arange(self.n_freq)\n",
      "\n",
      "        kernels = [gabor_kernel(f, theta=t, bandwidth=bandwidth) for f in frequencies \n",
      "                  for t in np.arange(0, np.pi, np.deg2rad(theta_interval))]\n",
      "        self.kernels = map(np.real, kernels)\n",
      "        self.n_kernel = len(kernels)\n",
      "\n",
      "        print '=== filter using Gabor filters ==='\n",
      "        print 'num. of kernels: %d' % (self.n_kernel)\n",
      "        print 'frequencies:', frequencies\n",
      "        print 'wavelength (pixels):', 1/frequencies\n",
      "\n",
      "        max_kern_size = np.max([kern.shape[0] for kern in kernels])\n",
      "        print 'max kernel matrix size:', max_kern_size\n",
      "\n",
      "    def convolve_per_proc(self, i):\n",
      "        return fftconvolve(self.img, self.kernels[i], 'same').astype(np.half)\n",
      "        \n",
      "    def compute_features(self):\n",
      "        self.get_kernels()\n",
      "        \n",
      "        filtered = Pool().map(unwrap_convolve_per_proc, zip([self]*self.n_kernel, range(self.n_kernel)))\n",
      "        \n",
      "        self.features = np.empty((self.img.shape[0], self.img.shape[1], self.n_kernel), dtype=np.half)\n",
      "        for i in range(self.n_kernel):\n",
      "            self.features[...,i] = filtered[i]\n",
      "\n",
      "        del filtered\n",
      "\n",
      "#         save_array(features, 'features')\n",
      "\n",
      "        self.n_feature = self.features.shape[-1]\n",
      "\n",
      "    \n",
      "    def compute_dist_per_proc(self, x):\n",
      "        X_partial, c_all_rot = x\n",
      "        D = cdist(X_partial, c_all_rot, 'sqeuclidean')\n",
      "        ci, ri = np.unravel_index(D.argmin(axis=1), (self.n_texton, self.n_angle))\n",
      "        return np.c_[ci, ri]\n",
      "    \n",
      "    def compute_texton(self):\n",
      "        print '=== compute rotation-invariant texton map using K-Means ==='\n",
      "\n",
      "        self.n_texton = int(self.param['n_texton'])\n",
      "\n",
      "        X = self.features.reshape(-1, self.n_feature)\n",
      "        n_data = X.shape[0]\n",
      "        n_splits = 1000\n",
      "        n_sample = int(self.param['n_sample'])\n",
      "        centroids = np.array(random.sample(X, self.n_texton))\n",
      "\n",
      "        n_iter = int(self.param['n_iter'])\n",
      "\n",
      "        for iteration in range(n_iter):\n",
      "\n",
      "            data = random.sample(X, n_sample)\n",
      "\n",
      "            print 'iteration', iteration\n",
      "            centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, self.n_freq), i)) \n",
      "                                    for c,i in itertools.product(centroids, range(self.n_angle))])\n",
      "\n",
      "            r = Pool().map(unwrap_compute_dist_per_proc, zip([self]*n_splits, zip(np.array_split(data, n_splits, axis=0), \n",
      "                                                itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "            \n",
      "#             r = Parallel(n_jobs=16)(delayed(compute_dist_per_proc)(x,c) \n",
      "#                             for x, c in zip(np.array_split(data, n_splits, axis=0), \n",
      "#                                             itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "            res = np.vstack(r)  \n",
      "\n",
      "            labels = res[:,0]\n",
      "            rotations = res[:,1]\n",
      "\n",
      "            centroids_new = np.zeros((self.n_texton, self.n_feature))\n",
      "            for d, l, r in itertools.izip(data, labels, rotations):\n",
      "                rot = np.concatenate(np.roll(np.split(d, self.n_freq), i))\n",
      "                centroids_new[l] += rot\n",
      "\n",
      "            counts = np.bincount(labels, minlength=self.n_texton)\n",
      "            centroids_new /= counts[:, np.newaxis]\n",
      "            centroids_new[counts==0] = centroids[counts==0]\n",
      "            print np.sqrt(np.sum((centroids - centroids_new)**2, axis=1)).mean()\n",
      "\n",
      "            centroids = centroids_new\n",
      "\n",
      "        print 'kmeans completes'\n",
      "        centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, self.n_freq), i)) \n",
      "                                    for c,i in itertools.product(centroids, range(self.n_angle))])\n",
      "\n",
      "        r = Pool().map(unwrap_compute_dist_per_proc, zip([self]*n_splits, zip(np.array_split(data, n_splits, axis=0), \n",
      "                                            itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "        \n",
      "#         r = Pool().map(compute_dist_per_proc, zip(np.array_split(X, n_splits, axis=0), \n",
      "#                                                 itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "        \n",
      "#         r = Parallel(n_jobs=16)(delayed(compute_dist_per_proc)(x,c) \n",
      "#                                 for x, c in zip(np.array_split(X, n_splits, axis=0), \n",
      "#                                                 itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "        res = np.vstack(r)\n",
      "\n",
      "        labels = res[:,0]\n",
      "        rotations = res[:,1]\n",
      "\n",
      "        textonmap = labels.reshape(self.features.shape[:2])\n",
      "        textonmap[~mask] = -1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}