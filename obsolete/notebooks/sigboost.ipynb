{
 "metadata": {
  "name": "",
  "signature": "sha256:2298d835520f759686ec9e3484f8794cf151ae00551d4c58b556504e7d466b98"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import random, itertools, sys, os\n",
      "from multiprocessing import Pool\n",
      "import json\n",
      "\n",
      "from skimage.segmentation import slic, mark_boundaries\n",
      "from skimage.measure import regionprops\n",
      "from skimage.util import img_as_ubyte\n",
      "from skimage.color import hsv2rgb, label2rgb, gray2rgb\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "from skimage.filter import gabor_kernel\n",
      "from skimage.transform import rescale, resize\n",
      "\n",
      "from scipy.ndimage import gaussian_filter, measurements\n",
      "from scipy.sparse import coo_matrix\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "from scipy.signal import fftconvolve\n",
      "\n",
      "from IPython.display import FileLink, Image, FileLinks\n",
      "\n",
      "import utilities\n",
      "from utilities import chi2\n",
      "\n",
      "from joblib import Parallel, delayed\n",
      "\n",
      "import glob, re, os, sys, subprocess, argparse\n",
      "import pprint\n",
      "import cPickle as pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parser = argparse.ArgumentParser(\n",
      "# formatter_class=argparse.RawDescriptionHelpFormatter,\n",
      "# description='Semi-supervised Sigboost',\n",
      "# epilog=\"\"\"%s\n",
      "# \"\"\"%(os.path.basename(sys.argv[0]), ))\n",
      "\n",
      "# parser.add_argument(\"stack_name\", type=str, help=\"stack name\")\n",
      "# parser.add_argument(\"resolution\", type=str, help=\"resolution string\")\n",
      "# parser.add_argument(\"slice_num\", type=str, help=\"slice number, zero-padded to 4 digits\")\n",
      "# parser.add_argument(\"param_id\", type=str, help=\"parameter identification name\")\n",
      "# args = parser.parse_args()\n",
      "\n",
      "class args:\n",
      "    stack_name = 'RS141'\n",
      "    resolution = 'x5'\n",
      "    slice_num = '0014'\n",
      "    params_name = 'redNissl'\n",
      "    models_fn = '/home/yuncong/BrainLocal/DavidData_v3/RS141/x5/RS141_x5_models.pkl'\n",
      "    labeling_fn = '/home/yuncong/BrainLocal/DavidData_v3/RS141/x5/0000/labelings/RS141_x5_0000_anon_10202014204123.pkl'\n",
      "\n",
      "data_dir = '/home/yuncong/BrainLocal/DavidData_v3'\n",
      "repo_dir = '/home/yuncong/BrainSaliencyDetection'\n",
      "params_dir = os.path.join(repo_dir, 'params')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stack_name = args.stack_name\n",
      "resolution = args.resolution\n",
      "slice_id = args.slice_num\n",
      "params_name = args.params_name\n",
      "\n",
      "results_dir = os.path.join(data_dir, stack_name, resolution, slice_id, params_name+'_pipelineResults')\n",
      "labelings_dir = os.path.join(data_dir, stack_name, resolution, slice_id, 'labelings')\n",
      "\n",
      "image_name = '_'.join([stack_name, resolution, slice_id])\n",
      "instance_name = '_'.join([stack_name, resolution, slice_id, params_name])\n",
      "\n",
      "_, _, _, username, logout_time = os.path.basename(args.labeling_fn)[:-4].split('_')\n",
      "parent_labeling_name = username + '_' + logout_time\n",
      "# parent_labeling_name = None\n",
      "\n",
      "def full_object_name(obj_name, ext):\n",
      "    return os.path.join(data_dir, stack_name, resolution, slice_id, params_name+'_pipelineResults', instance_name + '_' + obj_name + '.' + ext)\n",
      "\n",
      "\n",
      "# load parameter settings\n",
      "params_dir = os.path.realpath(params_dir)\n",
      "param_file = os.path.join(params_dir, 'param_%s.json'%params_name)\n",
      "param_default_file = os.path.join(params_dir, 'param_default.json')\n",
      "param = json.load(open(param_file, 'r'))\n",
      "param_default = json.load(open(param_default_file, 'r'))\n",
      "\n",
      "for k, v in param_default.iteritems():\n",
      "    if not isinstance(param[k], basestring):\n",
      "        if np.isnan(param[k]):\n",
      "            param[k] = v\n",
      "\n",
      "pprint.pprint(param)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'bandwidth': 2,\n",
        " u'beta': 1.0,\n",
        " u'freq_step': 1.5,\n",
        " u'frontier_contrast_diff_thresh': 0.2,\n",
        " u'lr_decision_thresh': 0.3,\n",
        " u'lr_grow_thresh': 0.1,\n",
        " u'max_wavelen': 40.0,\n",
        " u'min_wavelen': 5.0,\n",
        " u'n_iter': 10,\n",
        " u'n_models': 10,\n",
        " u'n_sample': 10000,\n",
        " u'n_superpixels': 2000.0,\n",
        " u'n_texton': 100,\n",
        " u'param_id': u'redNissl',\n",
        " u'slic_compactness': 20.0,\n",
        " u'slic_sigma': 5.0,\n",
        " u'theta_interval': 16}\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # def circle_list_to_labeling_field(self, circle_list):\n",
      "# #     label_circles = []\n",
      "# #     for c in circle_list:\n",
      "# #         label = np.where(np.all(self.colors == c.get_facecolor()[:3], axis=1))[0][0] - 1\n",
      "# #         label_circles.append((int(c.center[0]), int(c.center[1]), c.radius, label))\n",
      "# #     return label_circles\n",
      "\n",
      "\n",
      "# def labeling_field_to_labelmap(labeling_field, size):\n",
      "    \n",
      "#     labelmap = -1*np.ones(size, dtype=np.int)\n",
      "\n",
      "#     for cx,cy,cradius,label in labeling_field:\n",
      "#         for x in np.arange(cx-cradius, cx+cradius):\n",
      "#             for y in np.arange(cy-cradius, cy+cradius):\n",
      "#                 if (cx-x)**2+(cy-y)**2 <= cradius**2:\n",
      "#                     labelmap[int(y),int(x)] = label\n",
      "#     return labelmap\n",
      "\n",
      "    \n",
      "# def count_unique(keys):\n",
      "#     uniq_keys = np.unique(keys)\n",
      "#     bins = uniq_keys.searchsorted(keys)\n",
      "#     return uniq_keys, np.bincount(bins)\n",
      "\n",
      "\n",
      "# def label_superpixels(labelmap, segmentation):\n",
      "#     n_superpixels = len(np.unique(segmentation))\n",
      "#     labellist = -1*np.ones((n_superpixels,), dtype=np.int)\n",
      "#     for sp in range(n_superpixels):\n",
      "#         in_sp_labels = labelmap[segmentation==sp]\n",
      "#         labels, counts = count_unique(in_sp_labels)\n",
      "#         dominant_label = int(labels[counts.argmax()])\n",
      "#         if dominant_label != -1:\n",
      "#             labellist[sp] = dominant_label\n",
      "#     return labellist\n",
      "        \n",
      "        \n",
      "# def generate_models(labellist, sp_texton_hist_normalized):\n",
      "    \n",
      "#     models = []\n",
      "#     for i in range(np.max(labellist)+1):\n",
      "#         sps = np.where(labellist == i)[0]\n",
      "#         print i, sps\n",
      "#         model = {}\n",
      "#         if len(sps) > 0:\n",
      "#             texton_model = sp_texton_hist_normalized[sps, :].mean(axis=0)\n",
      "#             model['texton_hist'] = texton_model\n",
      "# #             dir_model = sp_dir_hist_normalized[sps, :].mean(axis=0)\n",
      "# #             model['dir_hist'] = dir_model\n",
      "#             models.append(model)\n",
      "\n",
      "#     n_models = len(models)\n",
      "#     print n_models, 'models'\n",
      "    \n",
      "#     return models\n",
      "\n",
      "# #     labelmap = labellist[segmentation]\n",
      "\n",
      "# #     for l in range(n_models):\n",
      "# #         matched_rows, matched_cols = np.where(labelmap == l)\n",
      "# #         ymin = matched_rows.min()\n",
      "# #         ymax = matched_rows.max()\n",
      "# #         xmin = matched_cols.min()\n",
      "# #         xmax = matched_cols.max()\n",
      "# #         models[l]['bounding_box'] = (xmin, ymin, xmax-xmin+1, ymax-ymin+1)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def get_max_kernel_size(param):\n",
      "\n",
      "#     theta_interval = param['theta_interval']\n",
      "#     n_angle = int(180/theta_interval)\n",
      "#     freq_step = param['freq_step']\n",
      "#     freq_max = 1./param['min_wavelen']\n",
      "#     freq_min = 1./param['max_wavelen']\n",
      "#     bandwidth = param['bandwidth']\n",
      "#     n_freq = int(np.log(freq_max/freq_min)/np.log(freq_step)) + 1\n",
      "#     frequencies = freq_max/freq_step**np.arange(n_freq)\n",
      "\n",
      "#     kernels = [gabor_kernel(f, theta=t, bandwidth=bandwidth) for f in frequencies \n",
      "#               for t in np.arange(0, n_angle)*np.deg2rad(theta_interval)]\n",
      "#     kernels = map(np.real, kernels)\n",
      "\n",
      "#     n_kernel = len(kernels)\n",
      "\n",
      "#     print '=== filter using Gabor filters ==='\n",
      "#     print 'num. of kernels: %d' % (n_kernel)\n",
      "#     print 'frequencies:', frequencies\n",
      "#     print 'wavelength (pixels):', 1/frequencies\n",
      "\n",
      "#     max_kern_size = np.max([kern.shape[0] for kern in kernels])\n",
      "#     print max_kern_size\n",
      "#     return max_kern_size\n",
      "\n",
      "# def models_from_labeling(labeling_fn):\n",
      "#     stack, resol, slice, username, logout_time = os.path.basename(labeling_fn)[:-4].split('_')\n",
      "#     img_fn = os.path.join(data_dir, stack, resol, slice, '_'.join([stack, resol, slice])+'.tif')\n",
      "#     img = cv2.imread(img_fn, 0)\n",
      "    \n",
      "#     cropImg_fn = os.path.join(data_dir, stack, resol, slice, params_name+'_pipelineResults', \n",
      "#                  '_'.join([stack, resol, slice, params_name]) + '_cropImg.tif')\n",
      "#     cropImg = cv2.imread(cropImg_fn, 0)\n",
      "    \n",
      "#     labeling = pickle.load(open(labeling_fn, 'r'))\n",
      "#     labelmap = labeling_field_to_labelmap(labeling['final_label_circles'], size=img.shape)\n",
      "    \n",
      "#     max_kern_size = get_max_kernel_size(param)\n",
      "    \n",
      "#     cropped_labelmap = labelmap[max_kern_size/2:-max_kern_size/2, max_kern_size/2:-max_kern_size/2]\n",
      "\n",
      "# #     show_labelmap(labelmap, img)\n",
      "\n",
      "    \n",
      "#     segmentation_fn = os.path.join(data_dir, stack, resol, slice, params_name+'_pipelineResults', \n",
      "#                  '_'.join([stack, resol, slice, params_name]) + '_segmentation.npy')\n",
      "#     print segmentation_fn\n",
      "#     segmentation = np.load(segmentation_fn)\n",
      "\n",
      "#     labellist = label_superpixels(cropped_labelmap, segmentation)\n",
      "\n",
      "\n",
      "#     segmentation_vis_fn = os.path.join(data_dir, stack, resol, slice, params_name+'_pipelineResults', \n",
      "#                  '_'.join([stack, resol, slice, params_name]) + '_segmentation.tif')\n",
      "#     segvis = cv2.imread(segmentation_vis_fn, 0)\n",
      "\n",
      "    \n",
      "# #     show_labelmap(labellist[segmentation], segvis)\n",
      "    \n",
      "#     f = os.path.join(data_dir, stack, resol, slice, params_name+'_pipelineResults', \n",
      "#                  '_'.join([stack, resol, slice, params_name]) + '_texHist.npy')\n",
      "#     print f\n",
      "    \n",
      "#     tex_hists = np.load(f)\n",
      "    \n",
      "#     models = generate_models(labellist, tex_hists)\n",
      "    \n",
      "#     return models"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def show_labelmap(lm, im):\n",
      "    \n",
      "#     hc_colors = np.loadtxt('../visualization/high_contrast_colors.txt', skiprows=1)\n",
      "\n",
      "#     labelmap_rgb = label2rgb(lm.astype(np.int), image=im, colors=hc_colors[1:]/255., alpha=0.1, \n",
      "#                              image_alpha=1, bg_color=hc_colors[0]/255.)\n",
      "\n",
      "#     labelmap_rgb = utilities.regulate_img(labelmap_rgb)\n",
      "#     plt.imshow(labelmap_rgb)\n",
      "#     plt.show()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# models = models_from_labeling(args.labeling_fn)\n",
      "# pickle.dump(models, open('/tmp/models.pkl', 'w'))\n",
      "\n",
      "models = pickle.load(open('/tmp/models.pkl', 'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== filter using Gabor filters ===\n",
        "num. of kernels: 66\n",
        "frequencies: [ 0.2         0.13333333  0.08888889  0.05925926  0.03950617  0.02633745]\n",
        "wavelength (pixels): [  5.        7.5      11.25     16.875    25.3125   37.96875]\n",
        "73\n",
        "/home/yuncong/BrainLocal/DavidData_v3/RS141/x5/0000/redNissl_pipelineResults/RS141_x5_0000_redNissl_segmentation.npy\n",
        "/home/yuncong/BrainLocal/DavidData_v3/RS141/x5/0000/redNissl_pipelineResults/RS141_x5_0000_redNissl_texHist.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0 [798 810 858 875 894 919 935 978]\n",
        "1 [1333 1354 1355 1357 1360 1376 1378 1392 1395 1413 1420 1424 1442 1450 1459\n",
        " 1479 1489]\n",
        "2 [1425 1435 1478 1481 1490 1540]\n",
        "3 [1030 1031 1062 1066 1070 1081 1122 1127 1136 1163 1172 1174 1175 1181 1185\n",
        " 1188 1192 1194 1213 1227 1237 1242 1243 1246 1247 1260 1266 1275 1297 1301\n",
        " 1336 1345]\n",
        "4 models\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "img = cv2.imread(os.path.join(data_dir, stack_name, resolution, slice_id, image_name + '.tif'), 0)\n",
      "cropImg = cv2.imread(full_object_name('cropImg', 'tif'), 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# models = pickle.load(open(args.models_fn, 'r'))\n",
      "n_models = len(models)\n",
      "\n",
      "texton_models = [model['texton_hist'] for model in models]\n",
      "\n",
      "mask = np.load(full_object_name('cropMask','npy'))\n",
      "fg_superpixels = np.load(full_object_name('fg','npy'))\n",
      "bg_superpixels = np.load(full_object_name('bg','npy'))\n",
      "neighbors = np.load(full_object_name('neighbors','npy'))\n",
      "\n",
      "sp_texton_hist_normalized = np.load(full_object_name('texHist', 'npy'))\n",
      "\n",
      "segmentation = np.load\n",
      "\n",
      "segmentation = np.load(full_object_name('segmentation', 'npy'))\n",
      "n_superpixels = len(np.unique(segmentation))\n",
      "\n",
      "D_texton_model = -1*np.ones((n_models, n_superpixels))\n",
      "D_texton_model[:, fg_superpixels] = cdist(sp_texton_hist_normalized[fg_superpixels], texton_models, chi2).T\n",
      "\n",
      "textonmap = np.load(full_object_name('texMap', 'npy'))\n",
      "overall_texton_hist = np.bincount(textonmap[mask].flat)\n",
      "\n",
      "overall_texton_hist_normalized = overall_texton_hist.astype(np.float) / overall_texton_hist.sum()\n",
      "\n",
      "D_texton_null = np.squeeze(cdist(sp_texton_hist_normalized, [overall_texton_hist_normalized], chi2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re_thresh_min = 0.01\n",
      "re_thresh_max = 0.8\n",
      "\n",
      "def grow_cluster_relative_entropy(seed, debug=False, \n",
      "                                  frontier_contrast_diff_thresh = 0.1,\n",
      "                                  max_cluster_size = 100):\n",
      "    '''\n",
      "    find the connected cluster of superpixels that have similar texture, starting from a superpixel as seed\n",
      "    '''\n",
      "    \n",
      "    bg_set = set(bg_superpixels.tolist())\n",
      "    \n",
      "    if seed in bg_set:\n",
      "        return [], -1\n",
      "\n",
      "    prev_frontier_contrast = np.inf\n",
      "    for re_thresh in np.arange(re_thresh_min, re_thresh_max, .01):\n",
      "    \n",
      "        curr_cluster = set([seed])\n",
      "        frontier = [seed]\n",
      "\n",
      "        while len(frontier) > 0:\n",
      "            u = frontier.pop(-1)\n",
      "            for v in neighbors[u]:\n",
      "                if v in bg_superpixels or v in curr_cluster: \n",
      "                    continue\n",
      "\n",
      "                if chi2(p[v], p[seed]) < re_thresh:\n",
      "                    curr_cluster.add(v)\n",
      "                    frontier.append(v)\n",
      "        \n",
      "        surround = set.union(*[neighbors[i] for i in curr_cluster]) - set.union(curr_cluster, bg_set)\n",
      "        if len(surround) == 0:\n",
      "            return curr_cluster, re_thresh\n",
      "\n",
      "        frontier_in_cluster = set.intersection(set.union(*[neighbors[i] for i in surround]), curr_cluster)\n",
      "        frontier_contrasts = [np.nanmax([chi2(p[i], p[j]) for j in neighbors[i] if j not in bg_set]) for i in frontier_in_cluster]\n",
      "        frontier_contrast = np.max(frontier_contrasts)\n",
      "        \n",
      "        if debug:\n",
      "            print 'frontier_contrast=', frontier_contrast, 'prev_frontier_contrast=', prev_frontier_contrast, 'diff=', frontier_contrast - prev_frontier_contrast\n",
      "        \n",
      "        if len(curr_cluster) > max_cluster_size or \\\n",
      "        frontier_contrast - prev_frontier_contrast > frontier_contrast_diff_thresh:\n",
      "            return curr_cluster, re_thresh\n",
      "        \n",
      "        prev_frontier_contrast = frontier_contrast\n",
      "        prev_cluster = curr_cluster\n",
      "        prev_re_thresh = re_thresh\n",
      "                                \n",
      "    return curr_cluster, re_thresh\n",
      "    \n",
      "\n",
      "def grow_cluster_likelihood_ratio(seed, texton_model, debug=False, lr_grow_thresh = 0.1):\n",
      "    '''\n",
      "    find the connected cluster of superpixels that are more likely to be explained by given model than by null, \n",
      "    starting from a superpixel as seed\n",
      "    '''\n",
      "    \n",
      "    if seed in bg_superpixels:\n",
      "        return [], -1\n",
      "\n",
      "    curr_cluster = set([seed])\n",
      "    frontier = [seed]\n",
      "        \n",
      "    while len(frontier) > 0:\n",
      "        u = frontier.pop(-1)\n",
      "        for v in neighbors[u]:\n",
      "            if v in bg_superpixels or v in curr_cluster: \n",
      "                continue\n",
      "            \n",
      "            ratio_v = D_texton_null[v] - chi2(p[v], texton_model)\n",
      "            if debug:  \n",
      "                print 'u=', u, 'v=',v, 'ratio_v = ', ratio_v\n",
      "                print D_texton_null[v],  chi2(p[v], texton_model)\n",
      "            \n",
      "            if ratio_v > lr_grow_thresh:\n",
      "                curr_cluster.add(v)\n",
      "                frontier.append(v)\n",
      "                                \n",
      "    return curr_cluster, lr_grow_thresh\n",
      "\n",
      "def grow_cluster_likelihood_ratio_precomputed(seed, D_texton_model, debug=False, lr_grow_thresh = 0.1):\n",
      "    '''\n",
      "    find the connected cluster of superpixels that are more likely to be explained by given model than by null, \n",
      "    starting from a superpixel as seed\n",
      "    using pre-computed distances between model and superpixels\n",
      "    '''\n",
      "\n",
      "    if seed in bg_superpixels:\n",
      "        return [], -1\n",
      "\n",
      "    curr_cluster = set([seed])\n",
      "    frontier = [seed]\n",
      "        \n",
      "    while len(frontier) > 0:\n",
      "        u = frontier.pop(-1)\n",
      "        for v in neighbors[u]:\n",
      "            if v in bg_superpixels or v in curr_cluster: \n",
      "                continue\n",
      "            \n",
      "            ratio_v = D_texton_null[v] - D_texton_model[v]\n",
      "            if debug:  \n",
      "                print 'u=', u, 'v=',v, 'ratio_v = ', ratio_v\n",
      "                print D_texton_null[v],  D_texton_model[v], \\\n",
      "            \n",
      "            if ratio_v > lr_grow_thresh:\n",
      "                curr_cluster.add(v)\n",
      "                frontier.append(v)\n",
      "                                \n",
      "    return curr_cluster, lr_grow_thresh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lr_decision_thresh = param['lr_decision_thresh']\n",
      "# lr_grow_thresh = param['lr_grow_thresh']\n",
      "\n",
      "lr_grow_thresh = .01\n",
      "lr_decision_thresh = .04\n",
      "\n",
      "def find_best_model(i):\n",
      "    model_score = np.empty((n_models, ))\n",
      "\n",
      "    if i in bg_superpixels:\n",
      "        return -1\n",
      "    else:\n",
      "        for m in range(n_models):\n",
      "            matched, _ = grow_cluster_likelihood_ratio_precomputed(i, D_texton_model[m], lr_grow_thresh=lr_grow_thresh)         \n",
      "            matched = list(matched)\n",
      "            model_score[m] = np.mean(D_texton_null[matched] - D_texton_model[m, matched])\n",
      "\n",
      "#             print matched, model_score[m]\n",
      "            \n",
      "        best_sig = model_score.max()\n",
      "        if best_sig > lr_decision_thresh: # sp whose sig is smaller than this is assigned null\n",
      "          return model_score.argmax()    \n",
      "    return -1\n",
      "\n",
      "\n",
      "def assign_models():\n",
      "\n",
      "    print lr_decision_thresh, lr_grow_thresh\n",
      "\n",
      "    r = Parallel(n_jobs=16)(delayed(find_best_model)(i) for i in range(n_superpixels))\n",
      "    labels = np.array(r, dtype=np.int)\n",
      "    \n",
      "    return labels\n",
      "    \n",
      "    \n",
      "# find_best_model(801)\n",
      "# find_best_model(1360)\n",
      "# find_best_model(1181)\n",
      "find_best_model(1435)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assigned_models = assign_models()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.04 0.01\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "labelmap = assigned_models[segmentation]\n",
      "\n",
      "hc_colors = np.loadtxt('../visualization/high_contrast_colors.txt', skiprows=1)\n",
      "\n",
      "labelmap_rgb = label2rgb(labelmap.astype(np.int), image=cropImg, colors=hc_colors[1:]/255., alpha=0.1, \n",
      "                         image_alpha=1, bg_color=hc_colors[0]/255.)\n",
      "\n",
      "# import datetime\n",
      "# dt = datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")\n",
      "\n",
      "# new_labeling = {\n",
      "# 'username': 'sigboost',\n",
      "# 'parent_labeling_name': None,\n",
      "# 'login_time': dt,\n",
      "# 'logout_time': dt,\n",
      "# 'init_labellist': None,\n",
      "# 'final_labellist': labels,\n",
      "# 'labelnames': None,\n",
      "# 'history': None\n",
      "# }\n",
      "\n",
      "labelmap_rgb = utilities.regulate_img(labelmap_rgb)\n",
      "# plt.imshow(labelmap_rgb)\n",
      "# plt.show()\n",
      "\n",
      "new_preview_fn = os.path.join('/home/yuncong/BrainLocal/sigboost_outputs', image_name + '_sigboost.tif')\n",
      "cv2.imwrite(new_preview_fn, labelmap_rgb[:,:,::-1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # set up sigboost parameters\n",
      "\n",
      "# # n_models = param['n_models']\n",
      "# n_models = None\n",
      "# frontier_contrast_diff_thresh = param['frontier_contrast_diff_thresh']\n",
      "# lr_grow_thresh = param['lr_grow_thresh']\n",
      "# beta = param['beta']\n",
      "# lr_decision_thresh = param['lr_decision_thresh']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # compute RE-clusters of every superpixel\n",
      "# r = Parallel(n_jobs=16)(delayed(grow_cluster_relative_entropy)(i, frontier_contrast_diff_thresh=frontier_contrast_diff_thresh) \n",
      "#                         for i in range(n_superpixels))\n",
      "# clusters = [list(c) for c, t in r]\n",
      "# print 'clusters computed'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # create output directory\n",
      "# stages_dir = os.path.join(results_dir, 'stages')\n",
      "# if not os.path.exists(stages_dir):\n",
      "#     os.makedirs(stages_dir)\n",
      "\n",
      "# # initialize models\n",
      "# texton_models = np.zeros((n_models, n_texton))\n",
      "# dir_models = np.zeros((n_models, n_angle))\n",
      "\n",
      "# seed_indices = np.zeros((n_models,))\n",
      "\n",
      "# weights = np.ones((n_superpixels, ))/n_superpixels\n",
      "# weights[bg_superpixels] = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "# # begin boosting loop; learn one model at each iteration\n",
      "# for t in range(n_models):\n",
      "    \n",
      "#     print 'model %d' % (t)\n",
      "    \n",
      "#     # Compute significance scores for every superpixel;\n",
      "#     # i.e. the significance of using the appearance of superpixel i as model\n",
      "#     # the significance score is defined as the average log likelihood ratio in a superpixel's RE-cluster\n",
      "#     sig_score = np.zeros((n_superpixels, ))\n",
      "#     for i in fg_superpixels:\n",
      "#         cluster = clusters[i]\n",
      "#         sig_score[i] = np.mean(weights[cluster] * \\\n",
      "#                                (D_texton_null[cluster] - np.array([chi2(p[j], p[i]) for j in cluster]) +\\\n",
      "#                                D_dir_null[cluster] - np.array([chi2(q[j], q[i]) for j in cluster])))\n",
      " \n",
      "#     # Pick the most significant superpixel\n",
      "#     seed_sp = sig_score.argsort()[-1]\n",
      "#     print \"most significant superpixel\", seed_sp\n",
      "    \n",
      "#     visualize_cluster(sig_score, 'all', title='significance score for each superpixel', filename='sigscore%d'%t)\n",
      "    \n",
      "#     curr_cluster = clusters[seed_sp]\n",
      "#     visualize_cluster(sig_score, curr_cluster, title='distance cluster', filename='curr_cluster%d'%t)\n",
      "\n",
      "#     # models are the average of the distributions in the chosen superpixel's RE-cluster\n",
      "#     model_texton = sp_texton_hist_normalized[curr_cluster].mean(axis=0)\n",
      "#     model_dir = sp_dir_hist_normalized[curr_cluster].mean(axis=0)\n",
      "    \n",
      "#     # Compute log likelihood ratio of this model against the null, for every superpixel\n",
      "    \n",
      "#     # RE(pj|pm)\n",
      "#     D_texton_model = np.empty((n_superpixels,))\n",
      "#     D_texton_model[fg_superpixels] = np.array([chi2(sp_texton_hist_normalized[i], model_texton) for i in fg_superpixels])\n",
      "#     D_texton_model[bg_superpixels] = np.nan\n",
      "    \n",
      "#     # RE(qj|qm)\n",
      "#     D_dir_model = np.empty((n_superpixels,)) \n",
      "#     D_dir_model[fg_superpixels] = np.array([chi2(sp_dir_hist_normalized[i], model_dir) for i in fg_superpixels])\n",
      "#     D_dir_model[bg_superpixels] = np.nan\n",
      "    \n",
      "#     # RE(pj|p0)-RE(pj|pm) + RE(qj|q0)-RE(qj|qm)\n",
      "#     match_scores = np.empty((n_superpixels,))\n",
      "#     match_scores[fg_superpixels] = D_texton_null[fg_superpixels] - D_texton_model[fg_superpixels] +\\\n",
      "#                                     D_dir_model[fg_superpixels] - D_dir_model[fg_superpixels]\n",
      "#     match_scores[bg_superpixels] = 0\n",
      "\n",
      "#     visualize_cluster(match_scores, 'all', title='match score', filename='grow%d'%t)\n",
      "\n",
      "#     # Find the cluster growed from seed based on log likelihood ratio. Refer to this cluster as the LR-cluster\n",
      "#     matched, _ = grow_cluster_likelihood_ratio(seed_sp, model_texton, model_dir)\n",
      "#     matched = list(matched)\n",
      "\n",
      "#     visualize_cluster(match_scores, matched, title='growed cluster', filename='grow%d'%t)\n",
      "\n",
      "#     # Reduce the weights of superpixels in LR-cluster\n",
      "#     weights[matched] = weights[matched] * np.exp(-5*(D_texton_null[matched] - D_texton_model[matched] +\\\n",
      "#                                                    D_dir_null[matched] - D_dir_model[matched])**beta)\n",
      "#     weights[bg_superpixels] = 0\n",
      "#     weights = weights/weights.sum()\n",
      "#     visualize_cluster((weights - weights.min())/(weights.max()-weights.min()), 'all', \n",
      "#                       title='updated superpixel weights', filename='weight%d'%t)\n",
      "    \n",
      "#     labels = -1*np.ones_like(segmentation)\n",
      "#     for i in matched:\n",
      "#         labels[segmentation == i] = 1\n",
      "#     real_image = label2rgb(labels, img)\n",
      "#     save_img(real_image, os.path.join('stage', 'real_image_model%d'%t))\n",
      "\n",
      "#     # record the model found at this round\n",
      "#     seed_indices[t] = seed_sp\n",
      "#     texton_models[t] = model_texton\n",
      "#     dir_models[t] = model_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    }
   ],
   "metadata": {}
  }
 ]
}