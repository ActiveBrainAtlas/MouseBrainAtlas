{
 "metadata": {
  "name": "",
  "signature": "sha256:2a386b691fef15ea3f745ff161c3daa743e3f138a7e8a20e36893bfbb370231e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "import sys\n",
      "sys.path.append('/home/yuncong/Brain/pipeline_scripts')\n",
      "from utilities2014 import *\n",
      "import os\n",
      "\n",
      "from scipy.spatial.distance import cdist, pdist, squareform\n",
      "from joblib import Parallel, delayed\n",
      "from skimage.color import gray2rgb\n",
      "\n",
      "import networkx\n",
      "from networkx.algorithms import node_connected_component\n",
      "\n",
      "%run grow_regions_common.ipynb\n",
      "\n",
      "os.environ['GORDON_DATA_DIR'] = '/home/yuncong/project/DavidData2014tif/'\n",
      "os.environ['GORDON_REPO_DIR'] = '/home/yuncong/Brain'\n",
      "os.environ['GORDON_RESULT_DIR'] = '/home/yuncong/project/DavidData2014results/'\n",
      "os.environ['GORDON_LABELING_DIR'] = '/home/yuncong/project/DavidData2014labelings/'\n",
      "\n",
      "dm = DataManager(data_dir=os.environ['GORDON_DATA_DIR'], \n",
      "  repo_dir=os.environ['GORDON_REPO_DIR'], \n",
      "  result_dir=os.environ['GORDON_RESULT_DIR'], \n",
      "  labeling_dir=os.environ['GORDON_LABELING_DIR'])\n",
      "\n",
      "dm.set_stack('RS141')\n",
      "dm.set_resol('x5')\n",
      "dm.set_gabor_params(gabor_params_id='blueNisslWide')\n",
      "dm.set_segmentation_params(segm_params_id='blueNisslRegular')\n",
      "dm.set_vq_params(vq_params_id='blueNissl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_boundaries(clusters, neighbors, mode=None, sp_properties=None):\n",
      "        \n",
      "    n_superpixels = len(clusters)\n",
      "    \n",
      "    border_sps = []\n",
      "\n",
      "    for j, cluster in enumerate(clusters):\n",
      "        surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
      "        surrounds = set([i for i in surrounds if any([(n not in cluster) and (n not in surrounds) for n in neighbors[i]])])\n",
      "        if len(surrounds) == 0:\n",
      "            continue\n",
      "\n",
      "        if mode=='frontiers':\n",
      "            frontiers = set.union(*[neighbors[c] for c in surrounds]) & set(cluster)\n",
      "            border = frontiers\n",
      "        elif mode=='surrounds':\n",
      "            border = surrounds\n",
      "        elif mode=='both':\n",
      "            border = surrounds | frontiers\n",
      "        \n",
      "        if sp_properties is not None:\n",
      "        \n",
      "            interior_center = sp_properties[list(cluster), :2].mean(axis=0)\n",
      "\n",
      "            border_sps_list = np.array(list(border))\n",
      "            vec_intctr2ext = sp_properties[border_sps_list, :2] - interior_center\n",
      "            angles = np.arctan2(vec_intctr2ext[:,1], vec_intctr2ext[:,0])\n",
      "            for i in range(len(angles)):\n",
      "                if angles[i] < 0:\n",
      "                    angles[i] += 2*np.pi\n",
      "\n",
      "            border_sps_list_sorted = border_sps_list[np.argsort(angles)] \n",
      "\n",
      "            border_positions = sp_properties[border_sps_list_sorted, :2]\n",
      "            \n",
      "            border_sps.append(border_sps_list_sorted)\n",
      "            border_positions_sps.append(border_positions)\n",
      "            \n",
      "        else:\n",
      "            border_sps.append(border)\n",
      "            \n",
      "    if sp_properties is not None:\n",
      "        return border_sps, border_positions\n",
      "    else:\n",
      "        return border_sps\n",
      "        \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_boundariness_vote(clusters, neighbors, cluster_scores=None, mode=None):\n",
      "    \n",
      "#     cluster_size_sps = np.array([len(c) for c in cluster_sps])\n",
      "#     cluster_score_sps = np.array(cluster_score_sps)\n",
      "    # cluster_bounding_boxes = Parallel(n_jobs=16)(delayed(compute_bounding_box)(c) for c in cluster_sps)\n",
      "    \n",
      "    if cluster_scores is None:\n",
      "        cluster_scores = np.ones((len(clusters),))\n",
      "    \n",
      "    n_superpixels = len(clusters)\n",
      "    \n",
      "    sp_votes = np.zeros((n_superpixels,), dtype=np.float)\n",
      "\n",
      "    border_sps = []\n",
      "\n",
      "    for j, (cluster, score) in enumerate(zip(clusters, cluster_scores)):\n",
      "        surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
      "        surrounds = set([i for i in surrounds if any([(n not in cluster) and (n not in surrounds) for n in neighbors[i]])])\n",
      "        if len(surrounds) == 0:\n",
      "            continue\n",
      "\n",
      "        if mode=='frontiers':\n",
      "            frontiers = set.union(*[neighbors[c] for c in surrounds]) & set(cluster)\n",
      "            border = frontiers\n",
      "        elif mode=='surrounds':\n",
      "            border = surrounds\n",
      "        elif mode=='both':\n",
      "            border = surrounds | frontiers\n",
      "        \n",
      "        border_sps.append(border)\n",
      "\n",
      "    #     weight = 1./np.sqrt(len(cluster))\n",
      "#         weight = 1.\n",
      "        weight = score\n",
      "        sp_votes[list(surrounds)] += weight\n",
      "    \n",
      "    return sp_votes, border_sps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_votemap(section_ind):\n",
      "\n",
      "    sp_votes = compute_boundariness_vote(section_ind)\n",
      "    \n",
      "    votemap = sp_votes[segmentation]\n",
      "    votemap[~dm.mask] = 0\n",
      "\n",
      "    votemap_normalized = (votemap-votemap.min())/(votemap.max()-votemap.min())\n",
      "    votemap_vis = plt.cm.Reds(votemap_normalized)[..., :3]\n",
      "\n",
      "    \n",
      "    vis = alpha_blending(votemap_vis, gray2rgb(dm.image), .5 * np.ones_like(dm.image),\n",
      "                          1. * np.ones_like(dm.image))\n",
      "    \n",
      "\n",
      "    dm.save_pipeline_result(votemap_vis, 'votemap', 'jpg')\n",
      "    dm.save_pipeline_result(vis, 'votemapOverlaid', 'jpg')\n",
      "    \n",
      "    return votemap_normalized, votemap_vis\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.spatial.distance import pdist, squareform\n",
      "from scipy.cluster.hierarchy import average, fcluster, leaders, complete, single, dendrogram\n",
      "\n",
      "def compute_overlap(c1, c2):\n",
      "    return float(len(c1 & c2))/min(len(c1),len(c2))\n",
      "\n",
      "def compute_overlap_partial(indices, sets):\n",
      "    n_sets = len(sets)\n",
      "    \n",
      "    overlap_matrix = np.zeros((len(indices), n_sets))\n",
      "        \n",
      "    for ii, i in enumerate(indices):\n",
      "        for j in range(n_sets):\n",
      "            c1 = set(sets[i])\n",
      "            c2 = set(sets[j])\n",
      "            overlap_matrix[ii, j] = compute_overlap(c1, c2)\n",
      "            \n",
      "    return overlap_matrix\n",
      "\n",
      "def set_pairwise_distances(sets):\n",
      "\n",
      "    partial_overlap_mat = Parallel(n_jobs=16, max_nbytes=1e6)(delayed(compute_overlap_partial)(s, sets) \n",
      "                                        for s in np.array_split(range(len(sets)), 16))\n",
      "    overlap_matrix = np.vstack(partial_overlap_mat)\n",
      "    distance_matrix = 1 - overlap_matrix\n",
      "    \n",
      "    return distance_matrix\n",
      "\n",
      "def group_clusters(clusters=None, dist_thresh = 0.1, distance_matrix=None):\n",
      "\n",
      "    if distance_matrix is None:\n",
      "        assert clusters is not None\n",
      "        distance_matrix = set_pairwise_distances(clusters)\n",
      "        \n",
      "    lk = average(squareform(distance_matrix))\n",
      "#     lk = single(squareform(distance_matrix))\n",
      "\n",
      "    # T = fcluster(lk, 1.15, criterion='inconsistent')\n",
      "    T = fcluster(lk, dist_thresh, criterion='distance')\n",
      "\n",
      "    n_groups = len(set(T))    \n",
      "    groups = [None] * n_groups\n",
      "\n",
      "    for group_id in range(n_groups):\n",
      "        groups[group_id] = where(T == group_id)[0]\n",
      "        \n",
      "    return [g for g in groups if len(g) > 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def find_boundaries(section_ind):\n",
      "    \n",
      "#     dm.set_slice(section_ind)\n",
      "#     dm._load_image()\n",
      "\n",
      "#     texton_hists = dm.load_pipeline_result('texHist', 'npy')\n",
      "# #     segmentation = dm.load_pipeline_result('segmentation', 'npy')\n",
      "# #     n_superpixels = len(unique(segmentation)) - 1\n",
      "# #     textonmap = dm.load_pipeline_result('texMap', 'npy')\n",
      "# #     n_texton = len(np.unique(textonmap)) - 1\n",
      "#     neighbors = dm.load_pipeline_result('neighbors', 'npy')\n",
      "# #     sp_properties = dm.load_pipeline_result('spProps', 'npy')\n",
      "# #     segmentation_vis = dm.load_pipeline_result('segmentationWithText', 'jpg')\n",
      "\n",
      "#     expansion_clusters_tuples = dm.load_pipeline_result('clusters', 'pkl')\n",
      "#     expansion_clusters, expansion_cluster_scores = zip(*expansion_clusters_tuples)\n",
      "    \n",
      "#     votemap, border_sps = compute_boundariness_vote(expansion_clusters, neighbors, mode='surrounds')\n",
      "    \n",
      "#     boundary_dist_matrix = set_pairwise_distances(border_sps)\n",
      "    \n",
      "#     border_clusters = group_clusters(border_sps, dist_thresh=.05, distance_matrix=boundary_dist_matrix)\n",
      "\n",
      "#     border_lengths = np.array([len(b) for b in border_sps])\n",
      "\n",
      "#     border_clusters = [b for b in border_clusters if len(b) > 0]\n",
      "\n",
      "#     border_cluster_size = np.array([len(b) for b in border_clusters])\n",
      "\n",
      "#     representative_borders = np.array([bc[np.argmax([border_lengths[i] for i in bc])] for bc in border_clusters])\n",
      "\n",
      "#     good_cluster_indices, good_border_indices, good_counts = zip(*[(i, b, s) \n",
      "#                                 for i, (b, s) in enumerate(zip(representative_borders, border_cluster_size)) if s > 8])\n",
      "\n",
      "#     good_borders = [border_sps[i] for i in good_border_indices]\n",
      "        \n",
      "#     interior_mean_distrs = []\n",
      "#     variances = []\n",
      "#     exteriors = []\n",
      "    \n",
      "#     for i in good_border_indices:\n",
      "    \n",
      "#         interior_textons = texton_hists[expansion_clusters[i]]\n",
      "#         interior_mean_distr = interior_textons.mean(axis=0)\n",
      "        \n",
      "#         interior_mean_distrs.append(interior_mean_distr)\n",
      "\n",
      "#         variance = 1./len(expansion_clusters[i]) * np.sum(cdist(interior_mean_distr[np.newaxis,:], interior_textons, metric=js)[0]**2)\n",
      "#         variances.append(variance)\n",
      "        \n",
      "#         exterior_sps = set.union(*[neighbors[b] for b in border_sps[i]]) - set(expansion_clusters[i]) - set(border_sps[i])\n",
      "\n",
      "#         exterior_textons = texton_hists[list(exterior_sps)]\n",
      "        \n",
      "#         exteriors.append(exterior_textons)\n",
      "        \n",
      "#     return good_borders, interior_mean_distrs, variances, exteriors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}