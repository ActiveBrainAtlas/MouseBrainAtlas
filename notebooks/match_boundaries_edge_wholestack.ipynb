{
 "metadata": {
  "name": "",
  "signature": "sha256:6811488c73bb2d0ec724b7a088c475378d324c0dbd49a62cb505a33bf6445c2b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('/home/yuncong/Brain/pipeline_scripts')\n",
      "from utilities2014 import *\n",
      "import os\n",
      "\n",
      "from scipy.spatial.distance import cdist, pdist, squareform\n",
      "from joblib import Parallel, delayed\n",
      "from skimage.color import gray2rgb\n",
      "from skimage.util import img_as_float\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "\n",
      "from collections import defaultdict, Counter\n",
      "from itertools import combinations, chain\n",
      "\n",
      "import networkx\n",
      "from networkx import from_dict_of_lists, dfs_postorder_nodes\n",
      "\n",
      "%run grow_region_boundary_based_common.ipynb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named joblib",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-d6f9583cbe93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquareform\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgray2rgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_as_float\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named joblib"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Order the exterior by depth-first search\n",
      "# The ordering is not well-defined in some corners\n",
      "\n",
      "def find_boundary_sps(clusters, neighbors, neighbor_graph):\n",
      "    '''\n",
      "    Identify superpixels that are at the boundary of regions: surround set and frontier set\n",
      "    '''\n",
      "        \n",
      "    n_superpixels = len(clusters)\n",
      "    \n",
      "    surrounds_sps = []\n",
      "    frontiers_sps = []\n",
      "    \n",
      "    for cluster_ind, cluster in enumerate(clusters):\n",
      "        \n",
      "        surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
      "#         surrounds = set([i for i in surrounds if any([(n not in cluster) and (n not in surrounds) for n in neighbors[i]])])\n",
      "        surrounds = set([i for i in surrounds if any([n not in cluster for n in neighbors[i]])])\n",
      "\n",
      "        if len(surrounds) == 0:\n",
      "            surrounds_sps.append([])\n",
      "            frontiers_sps.append([])\n",
      "\n",
      "        else:\n",
      "            frontiers = set.union(*[neighbors[c] for c in surrounds]) & set(cluster)\n",
      "\n",
      "            surrounds_subgraph = neighbor_graph.subgraph(surrounds)\n",
      "            surrounds_traversal = list(dfs_postorder_nodes(surrounds_subgraph))\n",
      "\n",
      "            frontiers_subgraph = neighbor_graph.subgraph(frontiers)\n",
      "            frontiers_traversal = list(dfs_postorder_nodes(frontiers_subgraph))\n",
      "\n",
      "            surrounds_sps.append(surrounds_traversal)\n",
      "            frontiers_sps.append(frontiers_traversal)\n",
      "        \n",
      "    return surrounds_sps, frontiers_sps\n",
      "\n",
      "\n",
      "def compute_overlap(c1, c2):\n",
      "    return float(len(c1 & c2)) / min(len(c1),len(c2))\n",
      "#     return float(len(c1 & c2)) / len(c1 | c2)\n",
      "\n",
      "def compute_overlap_partial(indices, sets):\n",
      "    n_sets = len(sets)\n",
      "    \n",
      "    overlap_matrix = np.zeros((len(indices), n_sets))\n",
      "        \n",
      "    for ii, i in enumerate(indices):\n",
      "        for j in range(n_sets):\n",
      "            c1 = set(sets[i])\n",
      "            c2 = set(sets[j])\n",
      "            if len(c1) == 0 or len(c2) == 0:\n",
      "                overlap_matrix[ii, j] = 0\n",
      "            else:\n",
      "                overlap_matrix[ii, j] = compute_overlap(c1, c2)\n",
      "            \n",
      "    return overlap_matrix\n",
      "\n",
      "def set_pairwise_distances(sets):\n",
      "\n",
      "    partial_overlap_mat = Parallel(n_jobs=16, max_nbytes=1e6)(delayed(compute_overlap_partial)(s, sets) \n",
      "                                        for s in np.array_split(range(len(sets)), 16))\n",
      "    overlap_matrix = np.vstack(partial_overlap_mat)\n",
      "    distance_matrix = 1 - overlap_matrix\n",
      "    \n",
      "    np.fill_diagonal(distance_matrix, 0)\n",
      "    \n",
      "    return distance_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_boundaries_section(sec_ind):\n",
      "    '''\n",
      "    Find good boundary landmarks (both closed and open) from a section image.\n",
      "    '''\n",
      "    \n",
      "    dm = DataManager(generate_hierarchy=False, stack='RS141', resol='x5')\n",
      "    \n",
      "    # Load image and relevant data\n",
      "    dm.set_slice(sec_ind)\n",
      "    im_height, im_width = imread(dm._get_image_filepath()).shape[:2]\n",
      "\n",
      "    texton_hists = dm.load_pipeline_result('texHist', 'npy')\n",
      "    segmentation = dm.load_pipeline_result('segmentation', 'npy')\n",
      "    n_superpixels = len(unique(segmentation)) - 1\n",
      "    textonmap = dm.load_pipeline_result('texMap', 'npy')\n",
      "    n_texton = len(np.unique(textonmap)) - 1\n",
      "    neighbors = dm.load_pipeline_result('neighbors', 'npy')\n",
      "    sp_properties = dm.load_pipeline_result('spProps', 'npy')\n",
      "    segmentation_vis = dm.load_pipeline_result('segmentationWithText', 'jpg')\n",
      "\n",
      "    # Load region proposals\n",
      "    expansion_clusters_tuples = dm.load_pipeline_result('clusters', 'pkl')\n",
      "    expansion_clusters, expansion_cluster_scores = zip(*expansion_clusters_tuples)\n",
      "    expansion_cluster_scores = np.array(expansion_cluster_scores)\n",
      "\n",
      "    neighbors_dict = dict(zip(np.arange(n_superpixels), [list(i) for i in neighbors]))\n",
      "    neighbor_graph = from_dict_of_lists(neighbors_dict)\n",
      "        \n",
      "    surrounds_sps, frontiers_sps = find_boundary_sps(expansion_clusters, neighbors=neighbors, neighbor_graph=neighbor_graph)\n",
      "    \n",
      "    \n",
      "    ############# OPEN BOUNDARY landmarks ##############\n",
      "    \n",
      "    # votes for directed edgelets\n",
      "    dEdge_votes = defaultdict(int)\n",
      "    \n",
      "    # Compute the supporter sets of every edgelet, based on region proposals\n",
      "    # supporter_all[(100,101)] is the set of superpixels that supports directed edgelet (100,101)\n",
      "    supporters_all = defaultdict(list)\n",
      "\n",
      "    for s in range(n_superpixels):\n",
      "    #     if s not in sps_in_good_regions: continue\n",
      "\n",
      "        c = list(expansion_clusters[s])\n",
      "        interior_texture = texton_hists[c].mean(axis=0)\n",
      "        b_sps = surrounds_sps[s]\n",
      "        b_contrasts = cdist(texton_hists[b_sps], interior_texture[np.newaxis, :], chi2)\n",
      "\n",
      "        for b_sp, b_contrast in zip(b_sps, b_contrasts):\n",
      "            int_sps = neighbors[b_sp] & set(c)\n",
      "            for int_sp in int_sps:\n",
      "                dEdge_votes[(b_sp, int_sp)] += b_contrast / max(len(c), 5)\n",
      "                dEdge_votes[(int_sp, b_sp)] += b_contrast / max(len(c), 5)\n",
      "                supporters_all[(b_sp, int_sp)].append(s) # (border_sp, interior_sp) or (out, in)\n",
      "\n",
      "    \n",
      "    # Identify all edgelets based on sp neighborhood information\n",
      "    try:\n",
      "        edge_coords = dm.load_pipeline_result('edgeCoords', 'pkl')\n",
      "    except:    \n",
      "        \n",
      "        # generate the set of all edgelets\n",
      "        edges = []\n",
      "        for i in range(n_superpixels):\n",
      "            for n in neighbors[i]:\n",
      "                edges.append(frozenset([i, n]))\n",
      "\n",
      "        # find the coordinates that belong to each edgelet\n",
      "        edge_map = gradient(segmentation.astype(np.uint8), disk(3)) > 0\n",
      "\n",
      "        edge_coords = defaultdict(list)\n",
      "\n",
      "        for y,x in zip(*np.nonzero(edge_map)):\n",
      "            if segmentation[y,x] != -1:\n",
      "                m = frozenset(segmentation[max(0, y-2):min(im_height, y+2),\n",
      "                                           max(0, x-2):min(im_width, x+2)].ravel())\n",
      "                if len(m) > 1:\n",
      "                    for q in set(combinations(m, 2)):\n",
      "                        edge_coords[frozenset(q)].append([y,x])\n",
      "\n",
      "        for edge, pts in edge_coords.iteritems():\n",
      "            if edge in edges:\n",
      "                edge_coords[edge] = np.array(pts)\n",
      "            \n",
      "        dm.save_pipeline_result(edge_coords, 'edgeCoords', 'pkl')\n",
      "    \n",
      "    # threshold the edgelet votes\n",
      "    dEdge_vote_thresh = 1.\n",
      "    winner_dEdges = [dedge for dedge, v in dEdge_votes.iteritems() if v > dEdge_vote_thresh]\n",
      "    \n",
      "    # group the winning edgelets, according to their supporter sets\n",
      "    winner_supporters = [supporters_all[dedge] for dedge in winner_dEdges]\n",
      "    supporter_D = set_pairwise_distances(winner_supporters)\n",
      "    supporter_sim_thresh = .2\n",
      "    winner_indices_grouped_by_supporters = group_clusters(winner_supporters, dist_thresh=supporter_sim_thresh, \n",
      "                                                          distance_matrix=supporter_D)\n",
      "    print len(winner_indices_grouped_by_supporters), 'alliances'\n",
      "    winners_grouped_by_supporters = [[winner_dEdges[i] for i in g] for g in winner_indices_grouped_by_supporters]\n",
      "    \n",
      "    alliance_sizes = np.array(map(len, winners_grouped_by_supporters))\n",
      "\n",
      "    # pick edgelet groups with more than 20 edgelets\n",
      "    big_alliance_indices = np.where(alliance_sizes > 20)[0]\n",
      "    print len(big_alliance_indices), 'big alliances'\n",
      "    big_alliance_dEdges = [winners_grouped_by_supporters[ba_ind] for ba_ind in big_alliance_indices]\n",
      "    big_alliance_winnerIndices = [winner_indices_grouped_by_supporters[ba_ind] for ba_ind in big_alliance_indices]\n",
      "    big_alliance_supporters = [set.union(*[set(winner_supporters[i]) for i in a]) for a in big_alliance_winnerIndices]\n",
      "    # define supporter set of an edgelet group as the union of each supporter set\n",
      "\n",
      "    # sort edgelet groups by total vote over all edgelets\n",
      "    good_edgeSet_tuple_sorted_by_score = sorted([(np.sum([dEdge_votes[e] for e in edges]), edges, alliance_ind)\n",
      "                                                      for alliance_ind, edges in enumerate(big_alliance_dEdges)\n",
      "                                                      if len(edges) > 10], reverse=True)\n",
      "    good_edgeSet_scores_sorted, good_edgeSets_sorted, good_edgeSet_indices_sorted = zip(*good_edgeSet_tuple_sorted_by_score)  \n",
      "    good_edgeSet_supporters = [ big_alliance_supporters[alliance_i] for alliance_i in good_edgeSet_indices_sorted]\n",
      "    # give this to Idan's web interface\n",
      "    \n",
      "    print len(good_edgeSet_tuple_sorted_by_score), 'good boundaries'\n",
      "    \n",
      "    # These edgelet groups constitute Open Boundary landmarks.\n",
      "    \n",
      "    \n",
      "    ############# CLOSED BOUNDARY landmarks ##############\n",
      "    \n",
      "    # Load region landmarks\n",
      "    representative_clusters = dm.load_pipeline_result('representativeClusters', 'pkl')\n",
      "\n",
      "    representative_cluster_scores_sorted, representative_clusters_sorted_by_score, \\\n",
      "    representative_cluster_indices_sorted_by_score, big_groups_sorted_by_score = zip(*representative_clusters)\n",
      "    \n",
      "    # Convert regions from sp set representation to edgelet set representation\n",
      "    \n",
      "    # Fill holes\n",
      "    representative_clusters_holefilled_sorted_by_score = []\n",
      "    \n",
      "    for ci, cluster in enumerate(representative_clusters_sorted_by_score):\n",
      "        holes = set([])\n",
      "        for t in [j for j in range(n_superpixels) if j not in cluster]:\n",
      "            if np.sum([i in cluster for i in neighbors[t]]) >= 3:\n",
      "                holes.add(t)\n",
      "#         print ci, holes\n",
      "        representative_clusters_holefilled_sorted_by_score.append(set(cluster) | holes)\n",
      "\n",
      "#     while True:\n",
      "#         representative_clusters_holefilled_sorted_by_score2 = []\n",
      "        \n",
      "#         for ci, cluster in enumerate(representative_clusters_sorted_by_score):\n",
      "            \n",
      "#             holes = set([])\n",
      "#             for t in [j for j in range(n_superpixels) if j not in cluster]:\n",
      "#                 if np.sum([i in cluster for i in neighbors[t]]) >= 3:\n",
      "#                     holes.add(t)\n",
      "# #             print ci, holes\n",
      "#             representative_clusters_holefilled_sorted_by_score2.append(set(cluster) | holes)\n",
      "#             representative_clusters_holefilled_sorted_by_score = representative_clusters_holefilled_sorted_by_score2[:]\n",
      "#         if len(holes) == 0:\n",
      "#             break    \n",
      "    \n",
      "    # Identify edgelets for the superpixel set\n",
      "    representative_cluster_edges = []\n",
      "\n",
      "    for cluster in representative_clusters_holefilled_sorted_by_score:\n",
      "        surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
      "    #     surrounds = set([i for i in surrounds if any([(n not in cluster) and (n not in surrounds) for n in neighbors[i]])])\n",
      "        surrounds = set([i for i in surrounds if any([n not in cluster for n in neighbors[i]])])\n",
      "\n",
      "        frontiers = set.union(*[neighbors[c] for c in surrounds]) & set(cluster)\n",
      "\n",
      "        region_edges = []\n",
      "        for s in surrounds:\n",
      "            for f in neighbors[s] & set(frontiers):\n",
      "                region_edges.append((s, f))\n",
      "\n",
      "        for i in cluster:\n",
      "            if -1 in neighbors[i]:\n",
      "                region_edges.append((-1, i))\n",
      "        representative_cluster_edges.append(region_edges)\n",
      "    \n",
      "    dm.save_pipeline_result(representative_cluster_edges, 'RepresentativeClusterEdges', 'pkl')\n",
      "    dm.save_pipeline_result(good_edgeSets_sorted, 'good_edgeSets_sorted', 'pkl')\n",
      "    \n",
      "    \n",
      "    # Select top 30 Closed Boundary landmarks and top 10 Open Boundary landmarks\n",
      "    all_edges = chain(*[representative_cluster_edges[:30], good_edgeSets_sorted[:10]])\n",
      "    \n",
      "    # Represent each landmark with a tuple\n",
      "    # (edgeSet, interior_texture, exterior_textures, points, center, supporter_set(?))\n",
      "    boundary_models = []\n",
      "    lm_supporters = []\n",
      "    \n",
      "    for s in representative_clusters_sorted_by_score[:30]:\n",
      "        lm_supporters.append(s)\n",
      "    \n",
      "    for s in good_edgeSet_supporters[:10]:\n",
      "        lm_supporters.append(s)\n",
      "    \n",
      "    dm.save_pipeline_result(lm_supporters, 'supporters', 'pkl')\n",
      "    \n",
      "#     for edgeSet_ind, edgeSet in zip(good_edgeSet_indices_sorted, good_edgeSets_sorted):\n",
      "\n",
      "    for edgeSet in all_edges:\n",
      "\n",
      "        # interior_texture = texton_hists[list(big_alliance_supporters[edgeSet_ind])].mean(axis=0)\n",
      "\n",
      "        surrounds = [e[0] for e in edgeSet]\n",
      "        frontiers = [e[1] for e in edgeSet]\n",
      "\n",
      "        interior_texture = texton_hists[frontiers].mean(axis=0)\n",
      "        exterior_textures = texton_hists[surrounds]\n",
      "\n",
      "        points = np.array([edge_coords[frozenset(e)].mean(axis=0) for e in edgeSet])\n",
      "\n",
      "        center = points.mean(axis=0)\n",
      "\n",
      "        boundary_models.append((edgeSet, interior_texture, exterior_textures, points, center))\n",
      "    \n",
      "    return boundary_models\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from munkres import Munkres\n",
      "\n",
      "sys.path.append('shape_context')\n",
      "import shape_context as sc\n",
      "\n",
      "\n",
      "def centering(ps):\n",
      "    return ps - ps.mean(axis=0)\n",
      "\n",
      "def hausdorff(ps1, ps2):\n",
      "    psc1 = centering(ps1)\n",
      "    psc2 = centering(ps2)\n",
      "    D = cdist(psc1, psc2)\n",
      "    return max(np.max(D.min(axis=0)), np.max(D.min(axis=1)))\n",
      "\n",
      "def hausdorff_histograms(h1s, h2s, metric):\n",
      "    Ds = cdist(h1s, h2s, metric)\n",
      "    return max(np.nanmax(np.nanmin(Ds, axis=1)), np.nanmax(np.nanmin(Ds, axis=0)))\n",
      "\n",
      "\n",
      "def shape_context_score_with_textures(pts1, pts2, exterior_textures1, exterior_textures2):\n",
      "    '''\n",
      "    Compute the shape context matching score between two point sets;\n",
      "    each point is associated a feature vector (a gabor texton histogram) \n",
      "    (not actually used in cost computation)\n",
      "    '''\n",
      "    \n",
      "    # NOTE THAT pts1 must contain more points than pts2 !!!\n",
      "\n",
      "    dists1, angles1 = sc.euclidean_dists_angles(pts1)\n",
      "    descriptors1 = sc.shape_context(dists1, angles1)\n",
      "    \n",
      "    dists2, angles2 = sc.euclidean_dists_angles(pts2)\n",
      "    descriptors2 = sc.shape_context(dists2, angles2)\n",
      "\n",
      "    n_rows = descriptors1.shape[0]\n",
      "    n_cols = descriptors2.shape[0]\n",
      "    \n",
      "    descriptors1 = descriptors1.reshape(n_rows, -1)\n",
      "    descriptors2 = descriptors2.reshape(n_cols, -1)\n",
      "        \n",
      "    Dm = np.zeros((n_rows, n_cols))\n",
      "    for i, di in enumerate(descriptors1):\n",
      "        for j, dj in enumerate(descriptors2):\n",
      "#             in addition to comparing shape context, also compare texture features\n",
      "#             Dm[i,j] = 0.01 * chi2(di, dj) + chi2(exterior_textures1[i], exterior_textures2[j])\n",
      "            \n",
      "        # only compare shape context\n",
      "        Dm[i,j] = chi2(di, dj)\n",
      "                \n",
      "    m = Munkres()\n",
      "    matches = m.compute(Dm.copy())\n",
      "    \n",
      "    # compute the total matching cost\n",
      "    total = 0\n",
      "    for row, column in matches:\n",
      "        value = Dm[row][column]\n",
      "        total += value\n",
      "        \n",
      "    # average the total cost by number of matches\n",
      "    if len(matches) > 0:\n",
      "        s = total / len(matches)\n",
      "\n",
      "    return s, matches\n",
      "\n",
      "\n",
      "def boundary_distance(b1, b2):\n",
      "    '''\n",
      "    Compute the distance between two boundaries\n",
      "    '''\n",
      "    \n",
      "    _, interior_texture1, exterior_textures1, points1, center1 = b1\n",
      "    _, interior_texture2, exterior_textures2, points2, center2 = b2\n",
      "\n",
      "    # compute interior texture difference\n",
      "    D_int = chi2(interior_texture1, interior_texture2)\n",
      "#     D_ext = hausdorff_histograms(exterior_textures1, exterior_textures2, metric=chi2)\n",
      "\n",
      "    # compute shape difference, exterior texture difference\n",
      "    if points1.shape[0] < points2.shape[0]:\n",
      "        d_shape, matches = shape_context_score_with_textures(points1, points2, exterior_textures1, exterior_textures2)\n",
      "        D_ext = np.mean([chi2(exterior_textures1[i], exterior_textures2[j]) for i, j in matches])\n",
      "    else:\n",
      "        d_shape, matches = shape_context_score_with_textures(points2, points1, exterior_textures2, exterior_textures1)\n",
      "        D_ext = np.mean([chi2(exterior_textures2[i], exterior_textures1[j]) for i, j in matches])\n",
      "    \n",
      "    # compute location difference\n",
      "    d_loc = np.linalg.norm(center1 - center2)\n",
      "    D_loc = np.maximum(0, d_loc - 500)\n",
      "                \n",
      "#     print D_int, D_ext, D_shape\n",
      "    \n",
      "    # weighted average of four terms\n",
      "    d = D_int + D_ext + .01 * D_shape + 0 * D_loc\n",
      "    \n",
      "    return d, D_int, D_ext, D_shape, D_loc\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import product\n",
      "\n",
      "def match_landmarks(sec1, sec2):\n",
      "    \n",
      "    '''\n",
      "    Match two sets of boundaries\n",
      "    '''\n",
      "    dm = DataManager(generate_hierarchy=False, stack='RS141', resol='x5')\n",
      "\n",
      "    boundaries1 = dm.load_pipeline_result('boundaryModels', 'pkl', section=sec1)\n",
      "    boundaries2 = dm.load_pipeline_result('boundaryModels', 'pkl', section=sec2)\n",
      "    \n",
      "    n_boundaries1 = len(boundaries1)\n",
      "    n_boundaries2 = len(boundaries2)\n",
      "\n",
      "    # compute all pairwise distances between two sets\n",
      "    Ds = Parallel(n_jobs=16)(delayed(boundary_distance)(boundaries1[i], boundaries2[j]) \n",
      "                             for i, j in product(range(n_boundaries1), range(n_boundaries2)))\n",
      "\n",
      "    D_boundaries, D_int, D_ext, D_shape, D_loc = zip(*Ds)\n",
      "\n",
      "    D_boundaries = np.reshape(D_boundaries, (n_boundaries1, n_boundaries2))\n",
      "    D1 = np.reshape(D_int, (n_boundaries1, n_boundaries2))\n",
      "    D2 = np.reshape(D_ext, (n_boundaries1, n_boundaries2))\n",
      "    D3 = np.reshape(D_shape, (n_boundaries1, n_boundaries2))\n",
      "    D4 = np.reshape(D_loc, (n_boundaries1, n_boundaries2))\n",
      "\n",
      "\n",
      "    # matching is all pairs in which either one is the closest to the other\n",
      "    # (score, landmark1_ind, landmark2_ind)\n",
      "    matchings = sorted([(D_boundaries[i,j], i, j) for j, i in enumerate(D_boundaries.argmin(axis=0)) \n",
      "                 if (i, j) in enumerate(D_boundaries.argmin(axis=1))])\n",
      "    \n",
      "#     if VERBOSE:\n",
      "#         print matchings\n",
      "    \n",
      "    dm.save_pipeline_result(vis, 'matchings%dWith%d'%(sec1,sec2), 'pkl', section=sec1)\n",
      "    \n",
      "    edge_sets1 = [b[0] for b in boundaries1]\n",
      "    edge_sets2 = [b[0] for b in boundaries2]\n",
      "\n",
      "    matched_boundaries1 = [boundaries1[i][0] for ind, (d,i,j) in enumerate(matchings)]\n",
      "    matched_boundaries2 = [boundaries2[j][0] for ind, (d,i,j) in enumerate(matchings)]\n",
      "\n",
      "    # Save output for section 1    \n",
      "    for i, b in enumerate(boundaries1):\n",
      "        dm.save_pipeline_result(dm.visualize_edges(b[0]), 'boundary%02d'%i, 'jpg', section=sec1)\n",
      "\n",
      "    dm.save_pipeline_result(dm.visualize_edge_sets(matched_boundaries1, text=True), \n",
      "                            'matchedBoundariesWithNext', 'jpg', section=sec1)\n",
      "\n",
      "    dm.save_pipeline_result(D_boundaries, 'DBoundariesWithNext', 'npy', section=sec1)\n",
      "    dm.save_pipeline_result(D1, 'D1WithNext', 'npy', section=sec1)\n",
      "    dm.save_pipeline_result(D2, 'D2WithNext', 'npy', section=sec1)\n",
      "    dm.save_pipeline_result(D3, 'D3WithNext', 'npy', section=sec1)\n",
      "    dm.save_pipeline_result(D4, 'D4WithNext', 'npy', section=sec1)\n",
      "\n",
      "    # Save output for section 2\n",
      "    for i, b in enumerate(boundaries2):\n",
      "        dm.save_pipeline_result(dm.visualize_edges(b[0]), 'boundary%02d'%i, 'jpg', section=sec2)\n",
      "\n",
      "    dm.save_pipeline_result(dm.visualize_edge_sets(matched_boundaries2, text=True), \n",
      "                            'matchedBoundariesWithPrev', 'jpg', section=sec2)\n",
      "\n",
      "    dm.save_pipeline_result(D_boundaries, 'DBoundariesWithPrev', 'npy', section=sec2)\n",
      "    dm.save_pipeline_result(D1, 'D1WithPrev', 'npy', section=sec2)\n",
      "    dm.save_pipeline_result(D2, 'D2WithPrev', 'npy', section=sec2)\n",
      "    dm.save_pipeline_result(D3, 'D3WithPrev', 'npy', section=sec2)\n",
      "    dm.save_pipeline_result(D4, 'D4WithPrev', 'npy', section=sec2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_landmarks(sec_ind):\n",
      "    \n",
      "    boundaries = find_boundaries_section(sec_ind)\n",
      "    dm.save_pipeline_result(boundaries, 'boundaryModels', 'pkl')\n",
      "    \n",
      "    edge_sets = [b[0] for b in boundaries]\n",
      "    dm.save_pipeline_result(dm.visualize_edge_sets(edge_sets, text=True), 'allBoundaries', 'jpg', section=sec_ind)\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "    \n",
      "    match_boundaries(boundaries1, boundaries2)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "Parallel(n_jobs=16)(delayed(find_landmarks)(sec_ind) for sec_ind in range(0,30))\n",
      "Parallel(n_jobs=16)(delayed(match_landmarks)(sec1, sec1+1) for sec1 in range(0,29))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}