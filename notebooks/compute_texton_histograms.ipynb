{
 "metadata": {
  "name": "",
  "signature": "sha256:131c232a33c63548bb831076b37fdce8d1b1ec7fc188381fe013833c782a0f1b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "from preamble import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num. of kernels: 99\n",
        "frequencies: [ 0.2         0.13333333  0.08888889  0.05925926  0.03950617  0.02633745\n",
        "  0.0175583   0.01170553  0.00780369]\n",
        "wavelength (pixels): [   5.            7.5          11.25         16.875        25.3125\n",
        "   37.96875      56.953125     85.4296875   128.14453125]\n",
        "max kernel matrix size: 243\n",
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "segmentation = dm.load_pipeline_result('segmentation', 'npy')\n",
      "n_superpixels = len(np.unique(segmentation)) - 1\n",
      "\n",
      "textonmap = dm.load_pipeline_result('texMap', 'npy')\n",
      "\n",
      "n_texton = len(np.unique(textonmap)) - 1\n",
      "\n",
      "# try:\n",
      "#     sp_texton_hist_normalized = dm.load_pipeline_result('texHist', 'npy')\n",
      "    \n",
      "# except:\n",
      "    \n",
      "def texton_histogram_worker(i):\n",
      "    return np.bincount(textonmap[(segmentation == i)&(textonmap != -1)], minlength=n_texton)\n",
      "\n",
      "r = Parallel(n_jobs=16)(delayed(texton_histogram_worker)(i) for i in range(n_superpixels))\n",
      "sp_texton_hist = np.array(r)\n",
      "sp_texton_hist_normalized = sp_texton_hist.astype(np.float) / sp_texton_hist.sum(axis=1)[:, np.newaxis] # denom might be invalid\n",
      "\n",
      "dm.save_pipeline_result(sp_texton_hist_normalized, 'texHist', 'npy')\n",
      "\n",
      "# compute the null texton histogram\n",
      "# overall_texton_hist = np.bincount(textonmap[dm.mask].flat)\n",
      "# overall_texton_hist_normalized = overall_texton_hist.astype(np.float) / overall_texton_hist.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/yuncong/DavidData/RS141/x5/0001/pipelineResults/RS141_x5_0001_segm-blueNissl_segmentation.npy\n",
        "loaded /home/yuncong/DavidData/RS141/x5/0001/pipelineResults/RS141_x5_0001_segm-blueNissl_segmentation.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/home/yuncong/DavidData/RS141/x5/0001/pipelineResults/RS141_x5_0001_gabor-blueNisslWide-vq-blueNissl_texMap.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loaded /home/yuncong/DavidData/RS141/x5/0001/pipelineResults/RS141_x5_0001_gabor-blueNisslWide-vq-blueNissl_texMap.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "saved /home/yuncong/DavidData/RS141/x5/0001/pipelineResults/RS141_x5_0001_gabor-blueNisslWide-segm-blueNissl-vq-blueNissl_texHist.npy\n"
       ]
      }
     ],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}