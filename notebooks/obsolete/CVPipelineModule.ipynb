{
 "metadata": {
  "name": "",
  "signature": "sha256:9ad973a0d6c570ddc57e50b21445a9bde18f64c030a6f8374996e3adb58e6fb4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import random, itertools, sys, os\n",
      "from multiprocessing import Pool\n",
      "import json\n",
      "\n",
      "from skimage.segmentation import slic, mark_boundaries\n",
      "from skimage.measure import regionprops\n",
      "from skimage.util import img_as_ubyte\n",
      "from skimage.color import hsv2rgb, label2rgb, gray2rgb\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "from skimage.filter import gabor_kernel\n",
      "from skimage.transform import rescale, resize\n",
      "\n",
      "from scipy.ndimage import gaussian_filter, measurements\n",
      "from scipy.sparse import coo_matrix\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "from scipy.signal import fftconvolve\n",
      "\n",
      "from IPython.display import FileLink, Image, FileLinks\n",
      "\n",
      "from utilities import *\n",
      "import manager_utilities\n",
      "\n",
      "from joblib import Parallel, delayed\n",
      "\n",
      "class CrossValidationPipeline(object):\n",
      "    def __init__(self, params=None):\n",
      "        self.kernels = None\n",
      "        if isinstance(params, str):\n",
      "            self.params = json.load(open(params))\n",
      "        elif isinstance(params, dict):\n",
      "            self.params = params\n",
      "            \n",
      "    def _generate_kernels(self):\n",
      "        theta_interval = self.params['theta_interval'] #10\n",
      "        n_angle = 180/theta_interval\n",
      "        # n_freq = params['n_freq']\n",
      "        freq_step = self.params['freq_step']\n",
      "        freq_max = 1./self.params['min_wavelen'] #1./5.\n",
      "        freq_min = 1./self.params['max_wavelen']\n",
      "        n_freq = int(np.log(freq_max/freq_min)/np.log(freq_step)) + 1\n",
      "        frequencies = freq_max/freq_step**np.arange(n_freq)\n",
      "\n",
      "        self.kernels = [gabor_kernel(f, theta=t, bandwidth=1.) for f in frequencies \n",
      "                  for t in np.arange(0, np.pi, np.deg2rad(theta_interval))]\n",
      "        self.kernels = map(np.real, self.kernels)\n",
      "        self.n_kernel = len(self.kernels)\n",
      "\n",
      "        print '=== filter using Gabor filters ==='\n",
      "        print 'num. of kernels: %d' % (self.n_kernel)\n",
      "        print 'frequencies:', frequencies\n",
      "        print 'wavelength (pixels):', 1/frequencies\n",
      "\n",
      "        max_kern_size = np.max([kern.shape[0] for kern in self.kernels])\n",
      "        print 'max kernel matrix size:', max_kern_size\n",
      "    \n",
      "#     def _convolve_per_proc(self, i):\n",
      "#         assert self.img.base is not None\n",
      "# #         return fftconvolve(self.img, self.kernels[i], 'same').astype(np.half)\n",
      "#         return fftconvolve(self.img, self.kernels[i], 'same').astype(np.float_)\n",
      "\n",
      "    def convolve_per_proc(self, i):\n",
      "        return fftconvolve(self.img, self.kernels[i], 'same').astype(np.half)\n",
      "\n",
      "    @timeit\n",
      "    def filter_image(self, img_path):\n",
      "        \n",
      "        img_dir, img_filename = os.path.split(img_path)\n",
      "        self.img = cv2.imread(img_path, 0)\n",
      "        self.im_height, self.im_width = self.img.shape[:2]\n",
      "        print 'read %s' % img_path\n",
      "        \n",
      "        print '=== finding foreground mask ==='\n",
      "        mask = foreground_mask(rescale(self.img, .5**3), min_size=100)\n",
      "        mask = resize(mask, self.img.shape) > .5\n",
      "        \n",
      "        if self.kernels is None:\n",
      "            self._generate_kernels()\n",
      "\n",
      "\n",
      "        filtered = Parallel(n_jobs=16)(delayed(self.convolve_per_proc)(i) \n",
      "                            for i in range(self.n_kernel))\n",
      "        print 'done'\n",
      "\n",
      "        self.features = np.empty((self.im_height, self.im_width, self.n_kernel), dtype=np.half)\n",
      "        for i in range(self.n_kernel):\n",
      "            self.features[...,i] = filtered[i]\n",
      "\n",
      "        del filtered\n",
      "        print 'done'\n",
      "\n",
      "        filtered = Parallel(n_jobs=16)(delayed(self.convolve_per_proc)(i) \n",
      "                            for i in range(self.n_kernel))\n",
      "        del filtered\n",
      "        \n",
      "        print 'done'\n",
      "        \n",
      "#         self.feature = np.load(feature_file)\n",
      "        \n",
      "#         import ctypes\n",
      "#         from multiprocessing import Array\n",
      "#         shared_array_base = Array(ctypes.c_float, np.load(feature_file).flat, lock=False)\n",
      "#         shared_array = np.frombuffer(shared_array_base, dtype=np.float_)\n",
      "#         self.feature = shared_array.reshape((self.im_height, self.im_width, 72))\n",
      "                \n",
      "#         pool = Pool(processes=8)\n",
      "#         filtered = pool.map(self._convolve_per_proc, range(self.n_kernel))\n",
      "\n",
      "#         pool.close()\n",
      "#         pool.join()\n",
      "#         del pool\n",
      "        \n",
      "#         print 'done'\n",
      "\n",
      "#         self.features = np.empty((self.im_height, self.im_width, self.n_kernel))\n",
      "#         for i in range(self.n_kernel):\n",
      "#             self.features[...,i] = filtered[i]\n",
      "\n",
      "#         np.save(feature_file, self.features)\n",
      "#         print 'features saved to %s' % feature_file\n",
      "                \n",
      "        self.n_feature = self.features.shape[-1]\n",
      "        \n",
      "        \n",
      "#         pool = Pool(processes=8)\n",
      "#         filtered = pool.map(self._convolve_per_proc, range(self.n_kernel))\n",
      "#         print 'done'\n",
      "                \n",
      "    @timeit\n",
      "    def generate_textonmap(self, textonmap_output=None):\n",
      "        self.n_texton = self.params['n_texton']\n",
      "        X = self.features.reshape(-1, self.n_feature)\n",
      "#         X = self.features[self.mask].reshape(-1, self.n_feature)\n",
      "        self.n_data = X.shape[0]\n",
      "\n",
      "        self.n_splits = 1000\n",
      "        n_sample = 10000\n",
      "        data = random.sample(X, n_sample)\n",
      "        centroids = data[:self.n_texton]\n",
      "\n",
      "        n_iter = 5\n",
      "        for iteration in range(n_iter):\n",
      "            print 'iteration', iteration\n",
      "            self.centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, self.n_freq), i)) \n",
      "                                        for c,i in itertools.product(centroids, range(self.n_angle))])\n",
      "\n",
      "            pool = Pool(processes=4)\n",
      "#             res = np.vstack(pool.map(self._compute_dist_per_proc, \n",
      "#                                      zip(np.array_split(data, n_splits, axis=0), \n",
      "#                                          itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "            res = np.vstack(pool.map(self._compute_dist_per_proc, range(self.n_splits)))\n",
      "            pool.close()\n",
      "            pool.join()\n",
      "            del pool\n",
      "            \n",
      "            labels = res[:,0]\n",
      "            rotations = res[:,1]\n",
      "\n",
      "            centroids_new = np.zeros((self.n_texton, self.n_feature))\n",
      "            for d, l, r in itertools.izip(data, labels, rotations):\n",
      "                rot = np.concatenate(np.roll(np.split(d, self.n_freq), i))\n",
      "                centroids_new[l] += rot\n",
      "\n",
      "            counts = np.bincount(labels)\n",
      "            centroids_new /= counts[:, np.newaxis]\n",
      "            print np.sqrt(np.sum((centroids - centroids_new)**2, axis=1)).mean()\n",
      "\n",
      "            centroids = centroids_new\n",
      "\n",
      "        print 'kmeans completes'\n",
      "        centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, self.n_freq), i)) \n",
      "                                    for c,i in itertools.product(centroids, range(self.n_angle))])\n",
      "\n",
      "        pool = Pool(processes=8)\n",
      "        res = np.vstack(pool.map(self._compute_dist_per_proc, \n",
      "                                 zip(np.array_split(X[self.mask], n_splits, axis=0), \n",
      "                                     itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "        pool.close()\n",
      "        pool.join()\n",
      "        del pool\n",
      "        \n",
      "        labels = res[:,0]\n",
      "        rotations = res[:,1]\n",
      "\n",
      "        textonmap = -1*np.ones((features.shape[:2]))\n",
      "        textonmap[self.mask] = labels.reshape(features.shape[:2])\n",
      "        \n",
      "        if textonmap_output is not None:\n",
      "            textonmap_rgb = label2rgb(textonmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "            cv2.imwrite(textonmap_output, img_as_ubyte(textonmap_rgb))\n",
      "    \n",
      "    @timeit\n",
      "    def compute_dirmap(self, dirmap_output=None):\n",
      "        f = np.reshape(self.features, (self.im_height, self.im_width, self.n_freq, self.n_angle))\n",
      "        dirmap = np.argmax(np.max(f, axis=2), axis=-1)\n",
      "        dirmap[~self.mask] = -1\n",
      "             \n",
      "        # colors = [(1,0,0),(0,1,0),(0,0,1),(.5,.5,.0),(0,.5,.5),(.5,0,.5)]\n",
      "\n",
      "        if dirmap_output is not None:\n",
      "            dirmap_rgb = label2rgb(dirmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "            cv2.imwrite(dirmap_output, img_as_ubyte(dirmap_rgb))\n",
      "        \n",
      "    @timeit\n",
      "    def segment_superpixels(self, compactness=5, sigma=10):\n",
      "        self.segmentation = slic(gray2rgb(self.img), compactness=compactness, sigma=sigma, enforce_connectivity=True)\n",
      "        self.n_superpixels = len(np.unique(self.segmentation))\n",
      "\n",
      "        sp_props = regionprops(self.segmentation+1, intensity_image=self.img, cache=True)\n",
      "        self.sp_centroids = np.array([s.centroid for s in sp_props])\n",
      "        sp_centroid_dist = pdist(self.sp_centroids)\n",
      "        self.sp_centroid_dist_matrix = squareform(sp_centroid_dist)\n",
      "\n",
      "        self.sp_mean_intensity = np.array([s.mean_intensity for s in sp_props])\n",
      "        sp_areas = np.array([s.area for s in sp_props])\n",
      "        \n",
      "        superpixels_bg_count = np.array([(~self.mask[self.segmentation==i]).sum() for i in range(self.n_superpixels)])\n",
      "        self.bg_superpixels = np.nonzero((superpixels_bg_count/sp_areas) > .8)[0]\n",
      "        \n",
      "#         superpixels_fg_count = np.array([self.mask[self.segmentation==i].sum() for i in range(self.n_superpixels)])\n",
      "#         self.bg_superpixels = np.nonzero((superpixels_fg_count/sp_areas) < 0.3)[0]\n",
      "    \n",
      "    def visualize_superpixels(self, output=None):\n",
      "        img_superpixelized = mark_boundaries(gray2rgb(self.img), self.segmentation)\n",
      "        img_superpixelized_text = img_as_ubyte(img_superpixelized)\n",
      "        for s in range(self.n_superpixels):\n",
      "            img_superpixelized_text = cv2.putText(img_superpixelized_text, str(s), \n",
      "                                                  tuple(np.floor(self.sp_centroids[s][::-1]).astype(np.int)), \n",
      "                                                  cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
      "                                                  0.8, ((255,0,255)), 1)\n",
      "        img_superpixelized_text = img_superpixelized_text/255.\n",
      "        \n",
      "        if output is not None:\n",
      "            cv2.imwrite(output, img_as_ubyte(img_superpixelized_text))\n",
      "\n",
      "    def visualize_textonmap_superpixels(self, output=None):\n",
      "        textonmap_rgb = label2rgb(self.textonmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "        \n",
      "        img_superpixelized = mark_boundaries(gray2rgb(self.img), self.segmentation)\n",
      "        img_superpixelized_text = img_as_ubyte(img_superpixelized)\n",
      "        for s in range(self.n_superpixels):\n",
      "            img_superpixelized_text = cv2.putText(img_superpixelized_text, str(s), \n",
      "                                                  tuple(np.floor(self.sp_centroids[s][::-1]).astype(np.int)), \n",
      "                                                  cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
      "                                                  0.8, ((255,0,255)), 1)\n",
      "        img_superpixelized_text = img_superpixelized_text/255.\n",
      "        \n",
      "        if output is not None:\n",
      "            cv2.imwrite(output, img_as_ubyte(.5*textonmap_rgb + .5*img_superpixelized_text))\n",
      "\n",
      "        \n",
      "    def _kl(self, u,v):\n",
      "        eps = 0.001\n",
      "        return np.sum((u+eps)*np.log((u+eps)/(v+eps)))\n",
      "\n",
      "    def _kl_no_eps(self, u,v):\n",
      "        return np.sum(u*np.log(u/v))\n",
      "    \n",
      "    def _chi2(self, u,v):\n",
      "        return np.sum(np.where(u+v!=0, (u-v)**2/(u+v), 0))\n",
      "\n",
      "    @timeit\n",
      "    def compute_distance_matrix(self):\n",
      "        sample_interval = 1\n",
      "        gridy, gridx = np.mgrid[:self.img.shape[0]:sample_interval, :self.img.shape[1]:sample_interval]\n",
      "\n",
      "        all_seg = self.segmentation[gridy.ravel(), gridx.ravel()]\n",
      "        all_texton = self.textonmap[gridy.ravel(), gridx.ravel()]\n",
      "        sp_texton_hist = np.array([np.bincount(all_texton[all_seg == s], minlength=self.num_textons) \n",
      "                         for s in range(self.n_superpixels)])\n",
      "\n",
      "        row_sums = sp_texton_hist.sum(axis=1)\n",
      "        self.sp_texton_hist_normalized = sp_texton_hist.astype(np.float) / row_sums[:, np.newaxis]\n",
      "        D = pdist(self.sp_texton_hist_normalized, self._kl)\n",
      "        self.hist_distance_matrix = squareform(D)\n",
      "\n",
      "    @timeit\n",
      "    def compute_connectivity(self):\n",
      "        edge_map = gradient(self.segmentation.astype(np.uint8), disk(3))\n",
      "        self.neighbors = [set() for i in range(self.n_superpixels)]\n",
      "        for y,x in zip(*np.nonzero(edge_map)):\n",
      "            self.neighbors[self.segmentation[y,x]] |= set(self.segmentation[y-2:y+2,x-2:x+2].ravel())\n",
      "\n",
      "        rows = np.hstack([s*np.ones((len(self.neighbors[s]),), dtype=np.int) for s in range(self.n_superpixels)])\n",
      "        cols = np.hstack([list(self.neighbors[s]) for s in range(self.n_superpixels)])\n",
      "        data = np.ones((cols.size, ), dtype=np.bool)\n",
      "        self.connectivity_matrix = coo_matrix((data, (rows, cols)), shape=(self.n_superpixels, self.n_superpixels))\n",
      "    \n",
      "    @timeit\n",
      "    def compute_saliency_map(self, neighbor_term_weight=1.):\n",
      "        overall_texton_hist = np.bincount(self.textonmap[self.mask].flat)\n",
      "        overall_texton_hist_normalized = overall_texton_hist.astype(np.float) / overall_texton_hist.sum()\n",
      "\n",
      "        individual_saliency_score = np.array([self._kl(sp_hist, overall_texton_hist_normalized) for sp_hist in self.sp_texton_hist_normalized])\n",
      "\n",
      "        self.saliency_score = np.zeros((self.n_superpixels,))\n",
      "        for i, sp_hist in enumerate(self.sp_texton_hist_normalized):\n",
      "            if i in self.bg_superpixels: continue\n",
      "            self.saliency_score[i] = individual_saliency_score[i]\n",
      "            neighbor_term = 0\n",
      "            for j in self.neighbors[i]:\n",
      "                if j!=i and j not in self.bg_superpixels:\n",
      "                    neighbor_term += np.exp(-self.hist_distance_matrix[i,j]) * individual_saliency_score[j]\n",
      "            self.saliency_score[i] += neighbor_term_weight*neighbor_term/(len(self.neighbors[i])-1)\n",
      "\n",
      "        self.saliency_map = self.saliency_score[self.segmentation]\n",
      "    \n",
      "    def visualize_saliency_map(self, output=None):\n",
      "        from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "        fig, ax = plt.subplots()\n",
      "        im = ax.matshow(self.saliency_map, cmap=plt.cm.Greys_r)\n",
      "        ax.axis('off')\n",
      "        divider = make_axes_locatable(ax)\n",
      "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
      "        plt.colorbar(im, cax=cax)\n",
      "        plt.close();\n",
      "        if output is not None:\n",
      "            fig.savefig(output, bbox_inches='tight')\n",
      "    \n",
      "    @timeit    \n",
      "    def find_salient_clusters(self, dist_thresh = 0.5, n_top_clusters=10):\n",
      "    \n",
      "        chosen_superpixels = set([])\n",
      "        self.clusters = []\n",
      "\n",
      "        for t in range(n_top_clusters):\n",
      "            for i in self.saliency_score.argsort()[::-1]:\n",
      "                if i not in chosen_superpixels and i not in self.bg_superpixels:\n",
      "                    break\n",
      "\n",
      "            curr_cluster = np.array([i], dtype=np.int)\n",
      "            frontier = [i]\n",
      "            while len(frontier) > 0:\n",
      "                i = frontier.pop(-1)\n",
      "                for j in self.neighbors[i]:\n",
      "                    if j != i and j not in curr_cluster and j not in chosen_superpixels\\\n",
      "                    and self.hist_distance_matrix[curr_cluster,j].mean() < dist_thresh\\\n",
      "                    and i not in self.bg_superpixels and j not in self.bg_superpixels:\n",
      "                        curr_cluster = np.append(curr_cluster, j)\n",
      "                        frontier.append(j)\n",
      "\n",
      "            self.clusters.append(curr_cluster)\n",
      "            chosen_superpixels |= set(curr_cluster)\n",
      "    \n",
      "    def visualize_salient_clusters(self, output=None):\n",
      "        segmentation_copy = np.zeros_like(self.segmentation)\n",
      "\n",
      "        for i, c in enumerate(self.clusters):\n",
      "            propagate_selection = np.equal.outer(self.segmentation, c).any(axis=2)\n",
      "            segmentation_copy[propagate_selection] = i + 1\n",
      "\n",
      "        selection_rgb = label2rgb(segmentation_copy, self.img, \n",
      "                                  bg_label=0, bg_color=(1,1,1), \n",
      "                                  colors=None)\n",
      "        if output is not None:\n",
      "            cv2.imwrite(output, img_as_ubyte(selection_rgb))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}