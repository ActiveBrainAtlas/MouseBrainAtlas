{
 "metadata": {
  "name": "",
  "signature": "sha256:13eb74d0f6afa4df481dcc4afa359500823bad58e09a10cc7c42d349417a637d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from joblib import Parallel, delayed\n",
      "import pathos\n",
      "from pathos.multiprocessing import ProcessingPool as Pool\n",
      "\n",
      "class ModelDetector(object):\n",
      "    def __init__(self, param, segmentation, p, q, output_dir, bg_superpixels, neighbors):\n",
      "        self.param = param\n",
      "        self.p = p\n",
      "        self.q = q\n",
      "        self.output_dir = output_dir\n",
      "        self.bg_superpixels = bg_superpixels\n",
      "        self.neighbors = neighbors\n",
      "        \n",
      "    def _grow_cluster_relative_entropy(self, seed, frontier_contrast_diff_thresh = 0.1,\n",
      "                                      max_cluster_size = 100):\n",
      "        '''\n",
      "        find the connected cluster of superpixels that have similar texture, starting from a superpixel as seed\n",
      "        '''\n",
      "        re_thresh_min = 0.01\n",
      "        re_thresh_max = 0.8\n",
      "        \n",
      "        bg_set = set(self.bg_superpixels.tolist())\n",
      "\n",
      "        if seed in bg_set:\n",
      "            return [], -1\n",
      "\n",
      "        prev_frontier_contrast = np.inf\n",
      "        for re_thresh in np.arange(re_thresh_min, re_thresh_max, .01):\n",
      "\n",
      "            curr_cluster = set([seed])\n",
      "            frontier = [seed]\n",
      "\n",
      "            while len(frontier) > 0:\n",
      "                u = frontier.pop(-1)\n",
      "                for v in neighbors[u]:\n",
      "                    if v in bg_superpixels or v in curr_cluster: \n",
      "                        continue\n",
      "\n",
      "                    if chi2(p[v], p[seed]) < re_thresh:\n",
      "                        curr_cluster.add(v)\n",
      "                        frontier.append(v)\n",
      "\n",
      "            surround = set.union(*[neighbors[i] for i in curr_cluster]) - set.union(curr_cluster, bg_set)\n",
      "            assert len(surround) != 0, seed\n",
      "\n",
      "            frontier_in_cluster = set.intersection(set.union(*[neighbors[i] for i in surround]), curr_cluster)\n",
      "            frontier_contrasts = [np.nanmax([chi2(p[i], p[j]) for j in neighbors[i] if j not in bg_set]) for i in frontier_in_cluster]\n",
      "            frontier_contrast = np.max(frontier_contrasts)\n",
      "\n",
      "            if len(curr_cluster) > max_cluster_size or \\\n",
      "            frontier_contrast - prev_frontier_contrast > frontier_contrast_diff_thresh:\n",
      "                return curr_cluster, re_thresh\n",
      "\n",
      "            prev_frontier_contrast = frontier_contrast\n",
      "            prev_cluster = curr_cluster\n",
      "            prev_re_thresh = re_thresh\n",
      "\n",
      "        return curr_cluster, re_thresh\n",
      "    \n",
      "\n",
      "    def _grow_cluster_likelihood_ratio(self, seed, texton_model, dir_model, lr_grow_thresh = 0.1, precompute=False):\n",
      "        '''\n",
      "        find the connected cluster of superpixels that are more likely to be explained by given model than by null, starting from a superpixel as seed\n",
      "        '''\n",
      "\n",
      "        if seed in bg_superpixels:\n",
      "            return [], -1\n",
      "\n",
      "        curr_cluster = set([seed])\n",
      "        frontier = [seed]\n",
      "\n",
      "        while len(frontier) > 0:\n",
      "            u = frontier.pop(-1)\n",
      "            for v in neighbors[u]:\n",
      "                if v in bg_superpixels or v in curr_cluster: \n",
      "                    continue\n",
      "\n",
      "                if precompute:\n",
      "                    ratio_v = D_texton_null[v] - D_texton_model[v] +\\\n",
      "                        D_dir_null[v] - D_dir_model[v]\n",
      "                else:\n",
      "                    ratio_v = D_texton_null[v] - chi2(p[v], texton_model) +\\\n",
      "                            D_dir_null[v] - chi2(q[v], dir_model)\n",
      "\n",
      "                if ratio_v > lr_grow_thresh:\n",
      "                    curr_cluster.add(v)\n",
      "                    frontier.append(v)\n",
      "\n",
      "        return curr_cluster, lr_grow_thresh\n",
      "\n",
      "    def _visualize_cluster(self, scores, cluster='all', title='', filename=None):\n",
      "        vis = scores[segmentation]\n",
      "        if cluster != 'all':\n",
      "            cluster_selection = np.equal.outer(segmentation, cluster).any(axis=2)\n",
      "            vis[~cluster_selection] = 0\n",
      "\n",
      "        plt.matshow(vis, cmap=plt.cm.Greys_r);\n",
      "        plt.axis('off');\n",
      "        plt.title(title)\n",
      "        if filename is not None:\n",
      "            plt.savefig(os.path.join(args.output_dir, 'stages', filename + '.png'), bbox_inches='tight')\n",
      "    #     plt.show()\n",
      "        plt.close();\n",
      "    \n",
      "    def _paint_cluster_on_img(self, cluster, title, filename=None):\n",
      "        cluster_map = -1*np.ones_like(segmentation)\n",
      "        for s in cluster:\n",
      "            cluster_map[segmentation==s] = 1\n",
      "        vis = label2rgb(cluster_map, image=img)\n",
      "        plt.imshow(vis, cmap=plt.cm.Greys_r);\n",
      "        plt.axis('off');\n",
      "        plt.title(title)\n",
      "        if filename is not None:\n",
      "            plt.savefig(os.path.join(args.output_dir, 'stages', filename + '.png'), bbox_inches='tight')\n",
      "    #     plt.show()\n",
      "        plt.close();\n",
      "\n",
      "    def _paint_clusters_on_img(self, clusters, title, filename=None):\n",
      "        cluster_map = -1*np.ones_like(segmentation)\n",
      "        for i, cluster in enumerate(clusters):\n",
      "            for j in cluster:\n",
      "                cluster_map[segmentation==j] = i\n",
      "        vis = label2rgb(cluster_map, image=img)\n",
      "        plt.imshow(vis, cmap=plt.cm.Greys_r);\n",
      "        plt.axis('off');\n",
      "        plt.title(title)\n",
      "        if filename is not None:\n",
      "            plt.savefig(os.path.join(args.output_dir, 'stages', filename + '.png'), bbox_inches='tight')\n",
      "    #     plt.show()\n",
      "        plt.close();\n",
      "    \n",
      "    \n",
      "    def compute_all_clusters(self):\n",
      "        '''\n",
      "        compute clusters for each superpixel, using each superpixel as seed\n",
      "        '''\n",
      "        n_superpixels = int(self.param['n_superpixels'])\n",
      "        frontier_contrast_diff_thresh = self.param['frontier_contrast_diff_thresh']\n",
      "        \n",
      "        pool = Pool()\n",
      "        r = pool.map(self._grow_cluster_relative_entropy, range(n_superpixels))\n",
      "        \n",
      "#         r = Parallel(n_jobs=16)(delayed(self._grow_cluster_relative_entropy)(i, frontier_contrast_diff_thresh=frontier_contrast_diff_thresh) \n",
      "#                                 for i in range(n_superpixels))\n",
      "#         r = Parallel(n_jobs=16)(delayed(self._grow_cluster_relative_entropy)(i, frontier_contrast_diff_thresh=frontier_contrast_diff_thresh) \n",
      "#                                 for i in range(n_superpixels))\n",
      "        self.clusters = [list(c) for c, t in r]\n",
      "        print 'clusters computed'\n",
      "    \n",
      "    def sigboost(self):\n",
      "        '''\n",
      "        perform sigboost\n",
      "        '''\n",
      "\n",
      "        n_models = self.param['n_models']\n",
      "        frontier_contrast_diff_thresh = self.param['frontier_contrast_diff_thresh']\n",
      "        lr_grow_thresh = self.param['lr_grow_thresh']\n",
      "        beta = self.param['beta']\n",
      "            \n",
      "    \n",
      "        f = os.path.join(self.output_dir, 'stages')\n",
      "        if not os.path.exists(f):\n",
      "            os.makedirs(f)\n",
      "\n",
      "        texton_models = np.zeros((n_models, n_texton))\n",
      "        dir_models = np.zeros((n_models, n_angle))\n",
      "\n",
      "        seed_indices = np.zeros((n_models,))\n",
      "\n",
      "        weights = np.ones((n_superpixels, ))/n_superpixels\n",
      "        weights[bg_superpixels] = 0\n",
      "\n",
      "        for t in range(n_models):\n",
      "\n",
      "            print 'model %d' % (t)\n",
      "\n",
      "            sig_score = np.zeros((n_superpixels, ))\n",
      "            for i in fg_superpixels:\n",
      "                cluster = clusters[i]\n",
      "                sig_score[i] = np.mean(weights[cluster] * \\\n",
      "                                       (D_texton_null[cluster] - np.array([chi2(p[j], p[i]) for j in cluster]) +\\\n",
      "                                       D_dir_null[cluster] - np.array([chi2(q[j], q[i]) for j in cluster])))\n",
      "\n",
      "            seed_sp = sig_score.argsort()[-1]\n",
      "            print \"most significant superpixel\", seed_sp\n",
      "\n",
      "            self.visualize_cluster(sig_score, 'all', title='significance score for each superpixel', filename='sigscore%d'%t)\n",
      "\n",
      "            curr_cluster = clusters[seed_sp]\n",
      "            self.visualize_cluster(sig_score, curr_cluster, title='distance cluster', filename='curr_cluster%d'%t)\n",
      "\n",
      "            model_texton = sp_texton_hist_normalized[curr_cluster].mean(axis=0)\n",
      "            model_dir = sp_dir_hist_normalized[curr_cluster].mean(axis=0)\n",
      "\n",
      "            # RE(pj|pm)\n",
      "            D_texton_model = np.empty((n_superpixels,))\n",
      "            D_texton_model[fg_superpixels] = np.array([chi2(sp_texton_hist_normalized[i], model_texton) for i in fg_superpixels])\n",
      "            D_texton_model[bg_superpixels] = np.nan\n",
      "\n",
      "            # RE(qj|qm)\n",
      "            D_dir_model = np.empty((n_superpixels,)) \n",
      "            D_dir_model[fg_superpixels] = np.array([chi2(sp_dir_hist_normalized[i], model_dir) for i in fg_superpixels])\n",
      "            D_dir_model[bg_superpixels] = np.nan\n",
      "\n",
      "            # RE(pj|p0)-RE(pj|pm) + RE(qj|q0)-RE(qj|qm)\n",
      "            match_scores = np.empty((n_superpixels,))\n",
      "            match_scores[fg_superpixels] = D_texton_null[fg_superpixels] - D_texton_model[fg_superpixels] +\\\n",
      "                                            D_dir_model[fg_superpixels] - D_dir_model[fg_superpixels]\n",
      "            match_scores[bg_superpixels] = 0\n",
      "\n",
      "            self.visualize_cluster(match_scores, 'all', title='match score', filename='grow%d'%t)\n",
      "\n",
      "\n",
      "            matched, _ = self._grow_cluster_likelihood_ratio(seed_sp, model_texton, model_dir)\n",
      "            matched = list(matched)\n",
      "\n",
      "            self.visualize_cluster(match_scores, matched, title='growed cluster', filename='grow%d'%t)\n",
      "\n",
      "            weights[matched] = weights[matched] * np.exp(-5*(D_texton_null[matched] - D_texton_model[matched] +\\\n",
      "                                                           D_dir_null[matched] - D_dir_model[matched])**beta)\n",
      "            weights[bg_superpixels] = 0\n",
      "            weights = weights/weights.sum()\n",
      "            self.visualize_cluster((weights - weights.min())/(weights.max()-weights.min()), 'all', \n",
      "                              title='updated superpixel weights', filename='weight%d'%t)\n",
      "\n",
      "            labels = -1*np.ones_like(segmentation)\n",
      "            for i in matched:\n",
      "                labels[segmentation == i] = 1\n",
      "            real_image = label2rgb(labels, img)\n",
      "            save_img(real_image, 'real_image_model%d'%t)\n",
      "\n",
      "            seed_indices[t] = seed_sp\n",
      "            texton_models[t] = model_texton\n",
      "            dir_models[t] = model_dir\n",
      "\n",
      "            \n",
      "    def _compute_model_score_per_proc(i):\n",
      "        model_score = np.empty((n_models, ))\n",
      "\n",
      "        if i in bg_superpixels:\n",
      "            return -1\n",
      "        else:\n",
      "            for m in range(n_models):\n",
      "                matched, _ = grow_cluster_likelihood_ratio_precomputed(i, D_texton_model[m], D_dir_model[m], \n",
      "                                                                       lr_grow_thresh=lr_grow_thresh)\n",
      "                matched = list(matched)\n",
      "                model_score[m] = np.mean(D_texton_null[matched] - D_texton_model[m, matched] +\\\n",
      "                                         D_dir_null[matched] - D_dir_model[m, matched])\n",
      "\n",
      "            best_sig = model_score.max()\n",
      "            if best_sig > lr_decision_thresh: # sp whose sig is smaller than this is assigned null\n",
      "              return model_score.argmax()\n",
      "        return -1\n",
      "\n",
      "            \n",
      "    def apply_image(self):\n",
      "        \n",
      "        D_texton_model = -1*np.ones((n_models, n_superpixels))\n",
      "        D_dir_model = -1*np.ones((n_models, n_superpixels))\n",
      "        D_texton_model[:, fg_superpixels] = cdist(sp_texton_hist_normalized[fg_superpixels], texton_models, chi2).T\n",
      "        D_dir_model[:, fg_superpixels] = cdist(sp_dir_hist_normalized[fg_superpixels], dir_models, chi2).T\n",
      "\n",
      "        lr_decision_thresh = self.param['lr_decision_thresh']\n",
      "\n",
      "        r = Parallel(n_jobs=16)(delayed(self._compute_model_score_per_proc)(i) for i in range(n_superpixels))\n",
      "        labels = np.array(r, dtype=np.int)\n",
      "        save_array(labels, 'labels')\n",
      "\n",
      "        labelmap = labels[segmentation]\n",
      "        save_array(labelmap, 'labelmap')\n",
      "\n",
      "        labelmap_rgb = label2rgb(labelmap.astype(np.int), image=img)\n",
      "        save_img(labelmap_rgb, 'labelmap')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-3-c27db9734cc3>, line 3)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-c27db9734cc3>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    def f(self, seed, frontier_contrast_diff_thresh = 0.1, max_cluster_size = 100:\u001b[0m\n\u001b[1;37m                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}