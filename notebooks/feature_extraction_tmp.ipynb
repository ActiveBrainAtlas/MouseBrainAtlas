{
 "metadata": {
  "name": "",
  "signature": "sha256:4228d6e6e4259cd1003ae22a46bf8736d5dff01d1f29f429c3b7b7792d0d1f0f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "import matplotlib\n",
      "# Force matplotlib to not use any Xwindows backend.\n",
      "matplotlib.use('Agg')\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import random, itertools, sys, os\n",
      "from multiprocessing import Pool\n",
      "import json\n",
      "\n",
      "from skimage.segmentation import slic, mark_boundaries\n",
      "from skimage.measure import regionprops\n",
      "from skimage.util import img_as_ubyte\n",
      "from skimage.color import hsv2rgb, label2rgb, gray2rgb\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "from skimage.filter import gabor_kernel\n",
      "from skimage.transform import rescale, resize\n",
      "\n",
      "from scipy.ndimage import gaussian_filter, measurements\n",
      "from scipy.sparse import coo_matrix\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "from scipy.signal import fftconvolve\n",
      "\n",
      "from IPython.display import FileLink, Image, FileLinks\n",
      "\n",
      "import utilities\n",
      "\n",
      "from joblib import Parallel, delayed\n",
      "\n",
      "import glob, re, os, sys, subprocess, argparse\n",
      "import pprint\n",
      "\n",
      "import logging\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class FeatureExtractor(object):\n",
      "     def __init__(self, img, param):\n",
      "        self.img = img\n",
      "        self.param = param\n",
      "\n",
      "# read in parameters\n",
      "params_dir = os.path.realpath(args.params_dir)\n",
      "param_file = os.path.join(params_dir, 'param_%s.json'%args.param_id)\n",
      "param_default_file = os.path.join(params_dir, 'param_default.json')\n",
      "param = json.load(open(param_file, 'r'))\n",
      "param_default = json.load(open(param_default_file, 'r'))\n",
      "\n",
      "for k, v in param_default.iteritems():\n",
      "    if not isinstance(param[k], basestring):\n",
      "        if np.isnan(param[k]):\n",
      "            param[k] = v\n",
      "\n",
      "pprint.pprint(param)\n",
      "\n",
      "# set image path\n",
      "img_file = os.path.realpath(args.img_file)\n",
      "img_path, ext = os.path.splitext(img_file)\n",
      "img_dir, img_name = os.path.split(img_path)\n",
      "\n",
      "logging.info('read %s', img_file)\n",
      "img = cv2.imread(img_file, 0)\n",
      "im_height, im_width = img.shape[:2]\n",
      "\n",
      "# set output path\n",
      "output_dir = os.path.realpath(args.output_dir)\n",
      "\n",
      "result_name = img_name + '_param_' + str(param['param_id'])\n",
      "result_dir = os.path.join(output_dir, result_name)\n",
      "if not os.path.exists(result_dir):\n",
      "    os.makedirs(result_dir)\n",
      "    \n",
      "\n",
      "# Find foreground mask\n",
      "logger.info('=== finding foreground mask ===')\n",
      "mask = utilities.foreground_mask(img, min_size=2500)\n",
      "mask = mask > .5\n",
      "# plt.imshow(mask, cmap=plt.cm.Greys_r);\n",
      "\n",
      "# Generate Gabor filter kernels\n",
      "theta_interval = param['theta_interval']\n",
      "n_angle = int(180/theta_interval)\n",
      "freq_step = param['freq_step']\n",
      "freq_max = 1./param['min_wavelen']\n",
      "freq_min = 1./param['max_wavelen']\n",
      "bandwidth = param['bandwidth']\n",
      "n_freq = int(np.log(freq_max/freq_min)/np.log(freq_step)) + 1\n",
      "frequencies = freq_max/freq_step**np.arange(n_freq)\n",
      "\n",
      "kernels = [gabor_kernel(f, theta=t, bandwidth=bandwidth) for f in frequencies \n",
      "          for t in np.arange(0, n_angle)*np.deg2rad(theta_interval)]\n",
      "kernels = map(np.real, kernels)\n",
      "\n",
      "n_kernel = len(kernels)\n",
      "\n",
      "\n",
      "# filter using Gabor filters\n",
      "logger.info('=== filter using Gabor filters ===')\n",
      "logger.info('num. of kernels: %d', n_kernel)\n",
      "logger.info('frequencies: %s', ', '.join(map(str, frequencies)))\n",
      "logger.info('wavelength (pixels): %s', ', '.join(map(str, 1/frequencies)))\n",
      "\n",
      "max_kern_size = np.max([kern.shape[0] for kern in kernels])\n",
      "print 'max kernel matrix size:', max_kern_size\n",
      "\n",
      "try:\n",
      "    raise IOError\n",
      "    features = load_array('features')\n",
      "except IOError:\n",
      "    def convolve_per_proc(i):\n",
      "        return fftconvolve(img, kernels[i], 'same').astype(np.half)\n",
      "    \n",
      "    filtered = Parallel(n_jobs=16)(delayed(convolve_per_proc)(i) \n",
      "                            for i in range(n_kernel))\n",
      "\n",
      "    features = np.empty((im_height, im_width, n_kernel), dtype=np.half)\n",
      "    for i in range(n_kernel):\n",
      "        features[...,i] = filtered[i]\n",
      "\n",
      "    del filtered\n",
      "    \n",
      "    save_array(features, 'features')\n",
      "\n",
      "n_feature = features.shape[-1]\n",
      "\n",
      "\n",
      "# crop border where filters show border effects\n",
      "logger.info('crop border where filters show border effects')\n",
      "features = features[max_kern_size/2:-max_kern_size/2, max_kern_size/2:-max_kern_size/2, :]\n",
      "img = img[max_kern_size/2:-max_kern_size/2, max_kern_size/2:-max_kern_size/2]\n",
      "mask = mask[max_kern_size/2:-max_kern_size/2, max_kern_size/2:-max_kern_size/2]\n",
      "im_height, im_width = img.shape[:2]\n",
      "save_img(img, 'img_cropped')\n",
      "\n",
      "\n",
      "# compute rotation-invariant texton map using K-Means\n",
      "logger.info('=== compute rotation-invariant texton map using K-Means ===')\n",
      "\n",
      "n_texton = int(param['n_texton'])\n",
      "try: \n",
      "    raise IOError\n",
      "    textonmap = load_array('textonmap')\n",
      "except IOError:\n",
      "    \n",
      "    X = features.reshape(-1, n_feature)\n",
      "    n_data = X.shape[0]\n",
      "    n_splits = 1000\n",
      "    n_sample = int(param['n_sample'])\n",
      "    centroids = np.array(random.sample(X, n_texton))\n",
      "    \n",
      "    n_iter = int(param['n_iter'])\n",
      "\n",
      "    def compute_dist_per_proc(X_partial, c_all_rot):\n",
      "        D = cdist(X_partial, c_all_rot, 'sqeuclidean')\n",
      "        ci, ri = np.unravel_index(D.argmin(axis=1), (n_texton, n_angle))\n",
      "        return np.c_[ci, ri]\n",
      "\n",
      "    for iteration in range(n_iter):\n",
      "        \n",
      "        data = random.sample(X, n_sample)\n",
      "        \n",
      "        print 'iteration', iteration\n",
      "        centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, n_freq), i)) \n",
      "                                for c,i in itertools.product(centroids, range(n_angle))])\n",
      "\n",
      "        r = Parallel(n_jobs=16)(delayed(compute_dist_per_proc)(x,c) \n",
      "                        for x, c in zip(np.array_split(data, n_splits, axis=0), \n",
      "                                        itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "        res = np.vstack(r)        \n",
      "\n",
      "        labels = res[:,0]\n",
      "        rotations = res[:,1]\n",
      "\n",
      "        centroids_new = np.zeros((n_texton, n_feature))\n",
      "        for d, l, r in itertools.izip(data, labels, rotations):\n",
      "            rot = np.concatenate(np.roll(np.split(d, n_freq), i))\n",
      "            centroids_new[l] += rot\n",
      "\n",
      "        counts = np.bincount(labels, minlength=n_texton)\n",
      "        centroids_new /= counts[:, np.newaxis]\n",
      "        centroids_new[counts==0] = centroids[counts==0]\n",
      "        print np.sqrt(np.sum((centroids - centroids_new)**2, axis=1)).mean()\n",
      "\n",
      "        centroids = centroids_new\n",
      "\n",
      "    logger.info('kmeans completes')\n",
      "    centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, n_freq), i)) \n",
      "                                for c,i in itertools.product(centroids, range(n_angle))])\n",
      "\n",
      "    r = Parallel(n_jobs=16)(delayed(compute_dist_per_proc)(x,c) \n",
      "                            for x, c in zip(np.array_split(X, n_splits, axis=0), \n",
      "                                            itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "    res = np.vstack(r)\n",
      "    \n",
      "    labels = res[:,0]\n",
      "    rotations = res[:,1]\n",
      "\n",
      "    textonmap = labels.reshape(features.shape[:2])\n",
      "    textonmap[~mask] = -1\n",
      "    \n",
      "    save_array(textonmap, 'textonmap')\n",
      "    \n",
      "textonmap_rgb = label2rgb(textonmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "save_img(textonmap_rgb, 'textonmap')\n",
      "\n",
      "print '=== over-segment the image into superpixels based on color information ==='\n",
      "\n",
      "img_rgb = gray2rgb(img)\n",
      "\n",
      "try:\n",
      "    raise IOError\n",
      "    segmentation = load_array('segmentation')\n",
      "    \n",
      "except IOError:\n",
      "    segmentation = slic(img_rgb, n_segments=int(param['n_superpixels']), \n",
      "                        max_iter=10, \n",
      "                        compactness=float(param['slic_compactness']), \n",
      "                        sigma=float(param['slic_sigma']), \n",
      "                        enforce_connectivity=True)\n",
      "    print 'segmentation computed'\n",
      "    \n",
      "    save_array(segmentation, 'segmentation')\n",
      "    \n",
      "sp_props = regionprops(segmentation+1, intensity_image=img, cache=True)\n",
      "\n",
      "def foo2(i):\n",
      "    return sp_props[i].centroid, sp_props[i].area, sp_props[i].mean_intensity\n",
      "\n",
      "r = Parallel(n_jobs=16)(delayed(foo2)(i) for i in range(len(sp_props)))\n",
      "sp_centroids = np.array([s[0] for s in r])\n",
      "sp_areas = np.array([s[1] for s in r])\n",
      "sp_mean_intensity = np.array([s[2] for s in r])\n",
      "\n",
      "n_superpixels = len(np.unique(segmentation))\n",
      "\n",
      "img_superpixelized = mark_boundaries(img_rgb, segmentation)\n",
      "sptext = img_as_ubyte(img_superpixelized)\n",
      "for s in range(n_superpixels):\n",
      "    sptext = cv2.putText(sptext, str(s), \n",
      "                      tuple(np.floor(sp_centroids[s][::-1]).astype(np.int)), \n",
      "                      cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
      "                      .5, ((255,0,255)), 1)\n",
      "save_img(sptext, 'segmentation')\n",
      "\n",
      "\n",
      "def foo(i):\n",
      "    return np.count_nonzero(mask[segmentation==i])\n",
      "\n",
      "r = Parallel(n_jobs=16)(delayed(foo)(i) for i in range(n_superpixels))\n",
      "superpixels_fg_count = np.array(r)\n",
      "bg_superpixels = np.nonzero((superpixels_fg_count/sp_areas) < 0.3)[0]\n",
      "fg_superpixels = np.array([i for i in range(n_superpixels) if i not in bg_superpixels])\n",
      "print '%d background superpixels'%len(bg_superpixels)\n",
      "\n",
      "# a = np.zeros((n_superpixels,), dtype=np.bool)\n",
      "# a[fg_superpixels] = True\n",
      "# plt.imshow(a[segmentation], cmap=plt.cm.Greys_r)\n",
      "# plt.show()\n",
      "\n",
      "\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "from scipy.sparse import coo_matrix\n",
      "\n",
      "edge_map = gradient(segmentation.astype(np.uint8), disk(3))\n",
      "neighbors = [set() for i in range(n_superpixels)]\n",
      "\n",
      "for y,x in zip(*np.nonzero(edge_map)):\n",
      "    neighbors[segmentation[y,x]] |= set(segmentation[y-2:y+2,x-2:x+2].ravel())\n",
      "\n",
      "for i in range(n_superpixels):\n",
      "    neighbors[i] -= set([i])\n",
      "    \n",
      "rows = np.hstack([s*np.ones((len(neighbors[s]),), dtype=np.int) for s in range(n_superpixels)])\n",
      "cols = np.hstack([list(neighbors[s]) for s in range(n_superpixels)])\n",
      "data = np.ones((cols.size, ), dtype=np.bool)\n",
      "connectivity_matrix = coo_matrix((data, (rows, cols)), shape=(n_superpixels,n_superpixels))\n",
      "connectivity_matrix = connectivity_matrix.transpose() * connectivity_matrix\n",
      "\n",
      "print '=== compute texton histogram of each superpixel ==='\n",
      "\n",
      "try:\n",
      "    raise IOError\n",
      "    sp_texton_hist_normalized = load_array('sp_texton_hist_normalized')\n",
      "except IOError:\n",
      "    def bar(i):\n",
      "        return np.bincount(textonmap[(segmentation == i)&(textonmap != -1)], minlength=n_texton)\n",
      "\n",
      "    r = Parallel(n_jobs=16)(delayed(bar)(i) for i in range(n_superpixels))\n",
      "    sp_texton_hist = np.array(r)\n",
      "    sp_texton_hist_normalized = sp_texton_hist.astype(np.float) / sp_texton_hist.sum(axis=1)[:, np.newaxis]\n",
      "    save_array(sp_texton_hist_normalized, 'sp_texton_hist_normalized')\n",
      "    \n",
      "overall_texton_hist = np.bincount(textonmap[mask].flat)\n",
      "overall_texton_hist_normalized = overall_texton_hist.astype(np.float) / overall_texton_hist.sum()\n",
      "\n",
      "print '=== compute directionality histogram of each superpixel ==='\n",
      "\n",
      "try:\n",
      "    raise IOError\n",
      "    sp_dir_hist_normalized = load_array('sp_dir_hist_normalized')\n",
      "except IOError:\n",
      "    f = np.reshape(features, (features.shape[0], features.shape[1], n_freq, n_angle))\n",
      "    dir_energy = np.sum(abs(f), axis=2)\n",
      "\n",
      "    def bar2(i):\n",
      "        segment_dir_energies = dir_energy[segmentation == i].astype(np.float_).mean(axis=0)\n",
      "        return segment_dir_energies    \n",
      "\n",
      "    r = Parallel(n_jobs=16)(delayed(bar2)(i) for i in range(n_superpixels))\n",
      "    \n",
      "    sp_dir_hist = np.vstack(r)\n",
      "    sp_dir_hist_normalized = sp_dir_hist/sp_dir_hist.sum(axis=1)[:,np.newaxis]\n",
      "    save_array(sp_dir_hist_normalized, 'sp_dir_hist_normalized')\n",
      "    \n",
      "overall_dir_hist = sp_dir_hist_normalized[fg_superpixels].mean(axis=0)\n",
      "overall_dir_hist_normalized = overall_dir_hist.astype(np.float) / overall_dir_hist.sum()\n",
      "\n",
      "\n",
      "# if __name__ == '__main__'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}