{
 "metadata": {
  "name": "",
  "signature": "sha256:393ec9a56dca9a0e278765774a4d3b827d24b87aa1de06fb09f743817be15862"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import random, itertools, sys, os\n",
      "from multiprocessing import Pool\n",
      "import json\n",
      "\n",
      "from skimage.segmentation import slic, mark_boundaries\n",
      "from skimage.measure import regionprops\n",
      "from skimage.util import img_as_ubyte\n",
      "from skimage.color import hsv2rgb, label2rgb, gray2rgb\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "from skimage.filter import gabor_kernel\n",
      "from skimage.transform import rescale, resize\n",
      "\n",
      "from scipy.ndimage import gaussian_filter, measurements\n",
      "from scipy.sparse import coo_matrix\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "from scipy.signal import fftconvolve\n",
      "\n",
      "from IPython.display import FileLink, Image, FileLinks\n",
      "\n",
      "from utilities import *\n",
      "import manager_utilities\n",
      "\n",
      "from joblib import Parallel, delayed\n",
      "\n",
      "import glob, re, os, sys, subprocess, argparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parser = argparse.ArgumentParser()\n",
      "# parser.add_argument(\"param_file\", type=str, help=\"parameter file name\")\n",
      "# parser.add_argument(\"img_file\", type=str, help=\"path to image file\")\n",
      "# # parser.add_argument(\"-of\", \"--output_feature\", action='store_true', help=\"whether to output feature array\")\n",
      "# # parser.add_argument(\"-ot\", \"--output_textonmap\", action='store_true', help=\"whether to output textonmap file\")\n",
      "# # parser.add_argument(\"-od\", \"--output_dirmap\", action='store_true', help=\"whether to output dirmap file\")\n",
      "# # parser.add_argument(\"-os\", \"--output_segmentation\", action='store_true', help=\"whether to output superpixel segmentation file\")\n",
      "# parser.add_argument(\"-c\", \"--cache_dir\", default='scratch', help=\"directory to store outputs\")\n",
      "# args = parser.parse_args()\n",
      "\n",
      "img_dir = '../data/PMD1305_reduce0_region0'\n",
      "img_name_full = 'PMD1305_244_reduce0_region0.tif'\n",
      "\n",
      "img_path = os.path.join(img_dir, img_name_full)\n",
      "img_name, ext = os.path.splitext(img_name_full)\n",
      "\n",
      "param_id = 91\n",
      "param_file = '../params/param%d.json'%param_id\n",
      "\n",
      "class args:\n",
      "    param_file = param_file\n",
      "    img_file = img_path\n",
      "    output_feature = True\n",
      "    output_textonmap = True\n",
      "    output_dirmap = True\n",
      "    output_segmentation = True\n",
      "    cache_dir = '../scratch'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_array(suffix):\n",
      "    return manager_utilities.load_array(suffix, img_name, \n",
      "                                 params['param_id'], args.cache_dir)\n",
      "\n",
      "def save_array(arr, suffix):\n",
      "    manager_utilities.save_array(arr, suffix, img_name, \n",
      "                                 params['param_id'], args.cache_dir)\n",
      "        \n",
      "def save_img(img, suffix, ext='tif'):\n",
      "    manager_utilities.save_img(img, suffix, img_name, params['param_id'], \n",
      "                               args.cache_dir, ext)\n",
      "\n",
      "def get_img_filename(suffix, ext='tif'):\n",
      "    return manager_utilities.get_img_filename(suffix, img_name, params['param_id'], args.cache_dir, ext=ext)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "params = json.load(open(args.param_file))\n",
      "p, ext = os.path.splitext(args.img_file)\n",
      "img_dir, img_name = os.path.split(p)\n",
      "img = cv2.imread(os.path.join(args.img_file), 0)\n",
      "im_height, im_width = img.shape[:2]\n",
      "\n",
      "output_dir = os.path.join(args.cache_dir, img_name)\n",
      "if not os.path.exists(output_dir):\n",
      "    os.makedirs(output_dir)\n",
      "\n",
      "print '=== finding foreground mask ==='\n",
      "mask = foreground_mask(rescale(img, .5**3), min_size=100)\n",
      "mask = resize(mask, img.shape) > .5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== finding foreground mask ===\n",
        "CPU times: user 5.82 s, sys: 1.18 s, total: 7 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 8.44 s\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta_interval = params['theta_interval'] #10\n",
      "n_angle = 180/theta_interval\n",
      "n_freq = params['n_freq']\n",
      "freq_max = params['max_freq'] #1./5.\n",
      "frequencies = freq_max/2**np.arange(n_freq)\n",
      "\n",
      "kernels = [gabor_kernel(f, theta=t, bandwidth=1.) for f in frequencies \n",
      "          for t in np.arange(0, np.pi, np.deg2rad(theta_interval))]\n",
      "kernels = map(np.real, kernels)\n",
      "n_kernel = len(kernels)\n",
      "\n",
      "print '=== filter using Gabor filters ==='\n",
      "print 'num. of kernels: %d' % (n_kernel)\n",
      "print 'frequencies:', frequencies\n",
      "print 'wavelength (pixels):', 1/frequencies\n",
      "\n",
      "max_kern_size = np.max([kern.shape[0] for kern in kernels])\n",
      "print 'max kernel matrix size:', max_kern_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== filter using Gabor filters ===\n",
        "num. of kernels: 72\n",
        "frequencies: [ 0.2    0.1    0.05   0.025]\n",
        "wavelength (pixels): [  5.  10.  20.  40.]\n",
        "max kernel matrix size: 137\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "try:\n",
      "    features = load_array('features')\n",
      "except IOError:\n",
      "    def convolve_per_proc(i):\n",
      "        return fftconvolve(img, kernels[i], 'same').astype(np.half)\n",
      "\n",
      "    pool = Pool(processes=8)\n",
      "    filtered = pool.map(convolve_per_proc, range(n_kernel))\n",
      "\n",
      "    features = np.empty((im_height, im_width, n_kernel), dtype=np.half)\n",
      "    for i in range(n_kernel):\n",
      "        features[...,i] = filtered[i]\n",
      "\n",
      "    del filtered\n",
      "    \n",
      "    save_array(features, 'features')\n",
      "\n",
      "n_feature = features.shape[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "load ../scratch/PMD1305_244_reduce0_region0/PMD1305_244_reduce0_region0_param91_features.npy\n",
        "CPU times: user 39 ms, sys: 10.7 s, total: 10.7 s\n",
        "Wall time: 11 s\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'crop border where filters show border effects'\n",
      "features = features[max_kern_size:-max_kern_size, max_kern_size:-max_kern_size, :]\n",
      "img = img[max_kern_size:-max_kern_size, max_kern_size:-max_kern_size]\n",
      "mask = mask[max_kern_size:-max_kern_size, max_kern_size:-max_kern_size]\n",
      "im_height, im_width = img.shape[:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crop border where filters show border effects\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "print '=== compute rotation-invariant texton map using K-Means ==='\n",
      "\n",
      "n_texton = params['n_texton']\n",
      "\n",
      "try: \n",
      "    textonmap = load_array('textonmap')\n",
      "except IOError:\n",
      "    \n",
      "    X = features.reshape(-1, n_feature)\n",
      "    n_data = X.shape[0]\n",
      "    n_splits = 1000\n",
      "    n_sample = 10000\n",
      "    data = random.sample(X, n_sample)\n",
      "    centroids = data[:n_texton]\n",
      "    \n",
      "    n_iter = 5\n",
      "\n",
      "    def compute_dist_per_proc((X_partial, c_all_rot)):\n",
      "        D = cdist(X_partial, c_all_rot, 'sqeuclidean')\n",
      "        ci, ri = np.unravel_index(D.argmin(axis=1), (n_texton, n_angle))\n",
      "        return np.c_[ci, ri]\n",
      "    \n",
      "    pool = Pool(processes=16)\n",
      "\n",
      "    for iteration in range(n_iter):\n",
      "        print 'iteration', iteration\n",
      "        centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, n_freq), i)) \n",
      "                                for c,i in itertools.product(centroids, range(n_angle))])\n",
      "\n",
      "        res = np.vstack(pool.map(compute_dist_per_proc, \n",
      "                                 zip(np.array_split(data, n_splits, axis=0), \n",
      "                                     itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "\n",
      "        labels = res[:,0]\n",
      "        rotations = res[:,1]\n",
      "\n",
      "        centroids_new = np.zeros((n_texton, n_feature))\n",
      "        for d, l, r in itertools.izip(data, labels, rotations):\n",
      "            rot = np.concatenate(np.roll(np.split(d, n_freq), i))\n",
      "            centroids_new[l] += rot\n",
      "\n",
      "        counts = np.bincount(labels)\n",
      "        centroids_new /= counts[:, np.newaxis]\n",
      "        print np.sqrt(np.sum((centroids - centroids_new)**2, axis=1)).mean()\n",
      "\n",
      "        centroids = centroids_new\n",
      "\n",
      "    print 'kmeans completes'\n",
      "    centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, n_freq), i)) \n",
      "                                for c,i in itertools.product(centroids, range(n_angle))])\n",
      "\n",
      "    res = np.vstack(pool.map(compute_dist_per_proc, \n",
      "                             zip(np.array_split(X, n_splits, axis=0), itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "    labels = res[:,0]\n",
      "    rotations = res[:,1]\n",
      "\n",
      "    pool.close()\n",
      "    pool.join()\n",
      "    del pool\n",
      "\n",
      "    textonmap = labels.reshape(features.shape[:2])\n",
      "    textonmap[~mask] = -1\n",
      "    \n",
      "    save_array(textonmap, 'textonmap')\n",
      "    \n",
      "    textonmap_rgb = label2rgb(textonmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "    save_img(textonmap_rgb, 'textonmap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute rotation-invariant texton map using K-Means ===\n",
        "load ../scratch/PMD1305_244_reduce0_region0/PMD1305_244_reduce0_region0_param91_textonmap.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 5 ms, sys: 536 ms, total: 541 ms\n",
        "Wall time: 924 ms\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "print '=== compute directionality map ==='\n",
      "\n",
      "try:\n",
      "    dirmap = load_array('dirmap')\n",
      "except IOError:\n",
      "    f = np.reshape(features, (features.shape[0], features.shape[1], n_freq, n_angle))\n",
      "    dir_energy = np.sum(abs(f), axis=2)\n",
      "#     total_energy = np.mean(dir_energy, axis=-1)\n",
      "#     dirmap = dir_energy/total_energy[:, :, np.newaxis]\n",
      "#     dirmap[~mask] = -1\n",
      "#     save_array(dirmap, 'dirmap')\n",
      "#     print 'dirmap computed'\n",
      "\n",
      "# dirmap_rgb = label2rgb(dirmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "# save_img(dirmap_rgb, 'dirmap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute directionality map ===\n",
        "../scratch/PMD1305_244_reduce0_region0/PMD1305_244_reduce0_region0_param91_dirmap.npy already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dirmap computed\n",
        "CPU times: user 2min 5s, sys: 4.37 s, total: 2min 9s\n",
        "Wall time: 2min 9s\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "print '=== over-segment the image into superpixels based on color information ==='\n",
      "\n",
      "img_rgb = gray2rgb(img)\n",
      "\n",
      "try:\n",
      "    segmentation = load_array('segmentation')\n",
      "    \n",
      "except IOError:\n",
      "    segmentation = slic(img_rgb, n_segments=params['n_superpixels'], max_iter=10, \n",
      "                        compactness=params['slic_compactness'], \n",
      "                        sigma=params['slic_sigma'], enforce_connectivity=True)\n",
      "    print 'segmentation computed'\n",
      "    \n",
      "    save_array(segmentation, 'segmentation')\n",
      "\n",
      "n_superpixels = len(np.unique(segmentation))\n",
      "\n",
      "sp_props = regionprops(segmentation+1, intensity_image=img, cache=True)\n",
      "sp_centroids = np.array([s.centroid for s in sp_props])\n",
      "sp_areas = np.array([s.area for s in sp_props])\n",
      "# sp_wcentroids = np.array([s.weighted_centroid for s in sp_props])\n",
      "# sp_centroid_dist = pdist(sp_centroids)\n",
      "# sp_centroid_dist_matrix = squareform(sp_centroid_dist)\n",
      "sp_mean_intensity = np.array([s.mean_intensity for s in sp_props])\n",
      "\n",
      "img_superpixelized = mark_boundaries(img_rgb, segmentation)\n",
      "# sptext = img_as_ubyte(img_superpixelized)\n",
      "# for s in range(n_superpixels):\n",
      "#     sptext = cv2.putText(sptext, str(s), \n",
      "#                       tuple(np.floor(sp_centroids[s][::-1]).astype(np.int)), \n",
      "#                       cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
      "#                       .5, ((255,0,255)), 1)\n",
      "save_img(img_superpixelized, 'segmentation')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== over-segment the image into superpixels based on color information ===\n",
        "load ../scratch/PMD1305_244_reduce0_region0/PMD1305_244_reduce0_region0_param91_segmentation.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 2min 28s, sys: 2.48 s, total: 2min 31s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 2min 32s\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "# superpixels_fg_count = [np.count_nonzero(mask[segmentation==i]) for i in range(n_superpixels)]\n",
      "\n",
      "def foo(i):\n",
      "    return np.count_nonzero(mask[segmentation==i])\n",
      "\n",
      "r = Parallel(n_jobs=16)(delayed(foo)(i) for i in range(n_superpixels))\n",
      "superpixels_fg_count = np.array(r)\n",
      "bg_superpixels = np.nonzero((superpixels_fg_count/sp_areas) < 0.3)[0]\n",
      "print '%d background superpixels'%len(bg_superpixels)\n",
      "\n",
      "# pool = Pool(16)\n",
      "# superpixels_fg_count = np.array(pool.map(foo, range(n_superpixels)))\n",
      "# pool.close()\n",
      "# pool.join()\n",
      "# del pool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "26 background superpixels\n",
        "CPU times: user 84 ms, sys: 531 ms, total: 615 ms\n",
        "Wall time: 3.13 s\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "print '=== compute texton and directionality histogram of each superpixel ==='\n",
      "\n",
      "# sample_interval = 1\n",
      "# gridy, gridx = np.mgrid[:img.shape[0]:sample_interval, :img.shape[1]:sample_interval]\n",
      "\n",
      "# all_seg = segmentation[gridy.ravel(), gridx.ravel()]\n",
      "\n",
      "# try:\n",
      "#     sp_texton_hist_normalized = load_array('sp_texton_hist_normalized')\n",
      "# except IOError:\n",
      "#     all_texton = textonmap[gridy.ravel(), gridx.ravel()]\n",
      "\n",
      "def bar(i):\n",
      "    return np.bincount(textonmap[(segmentation == i)&(textonmap != -1)], minlength=n_texton)\n",
      "    \n",
      "r = Parallel(n_jobs=16)(delayed(bar)(i) for i in range(n_superpixels))\n",
      "sp_texton_hist = np.array(r)\n",
      "# sp_texton_hist = np.array([np.bincount(textonmap[(segmentation == s)&(textonmap != -1)], minlength=n_texton) \n",
      "#                  for s in range(n_superpixels)])\n",
      "sp_texton_hist_normalized = sp_texton_hist.astype(np.float) / sp_texton_hist.sum(axis=1)[:, np.newaxis]\n",
      "# save_array(sp_texton_hist_normalized, 'sp_texton_hist_normalized')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute texton and directionality histogram of each superpixel ===\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 104 ms, sys: 535 ms, total: 639 ms\n",
        "Wall time: 5.52 s\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:21: RuntimeWarning: invalid value encountered in divide\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "def bar2(i):\n",
      "    segment_dir_energies = dir_energy[segmentation == i].astype(np.float_).sum(axis=0)\n",
      "    return segment_dir_energies/segment_dir_energies.sum()    \n",
      "\n",
      "r = Parallel(n_jobs=16)(delayed(bar2)(i) for i in range(n_superpixels))\n",
      "sp_dir_hist_normalized = np.vstack(r)\n",
      "\n",
      "# pool = Pool(16)\n",
      "# r = pool.map(bar2, range(n_superpixels))\n",
      "# try:\n",
      "#     sp_dir_hist_normalized = load_array('sp_dir_hist_normalized')\n",
      "# except IOError:\n",
      "# sp_dir_hist_normalized = np.empty((n_superpixels, n_angle))\n",
      "# for i in range(n_superpixels):\n",
      "#     segment_dir_energies = dir_energy[segmentation == i].astype(np.float_).sum(axis=0)\n",
      "#     sp_dir_hist_normalized[i,:] = segment_dir_energies/segment_dir_energies.sum()    \n",
      "# save_array(sp_dir_hist_normalized, 'sp_dir_hist_normalized')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 110 ms, sys: 496 ms, total: 606 ms\n",
        "Wall time: 5.38 s\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "def chi2(u,v):\n",
      "    return np.sum(np.where(u+v!=0, (u-v)**2/(u+v), 0))\n",
      "\n",
      "print '=== compute significance of each superpixel ==='\n",
      "\n",
      "overall_texton_hist = np.bincount(textonmap[mask].flat)\n",
      "overall_texton_hist_normalized = overall_texton_hist.astype(np.float) / overall_texton_hist.sum()\n",
      "\n",
      "# individual_texton_saliency_score = np.zeros((n_superpixels, ))\n",
      "# for i, sp_hist in enumerate(sp_texton_hist_normalized):\n",
      "#     individual_texton_saliency_score[i] = chi2(sp_hist, overall_texton_hist_normalized)\n",
      "\n",
      "# individual_texton_saliency_score = cdist(sp_texton_hist_normalized, overall_texton_hist_normalized[np.newaxis,:], chi2)\n",
      "# individual_texton_saliency_score[bg_superpixels] = 0\n",
      "individual_texton_saliency_score = np.array([chi2(sp_hist, overall_texton_hist_normalized) if sp_hist not in bg_superpixels else 0 \n",
      "                                             for sp_hist in sp_texton_hist_normalized])\n",
      "\n",
      "# texton_saliency_score = individual_texton_saliency_score\n",
      "\n",
      "texton_saliency_score = np.zeros((n_superpixels,))\n",
      "for i, sp_hist in enumerate(sp_texton_hist_normalized):\n",
      "    if i not in bg_superpixels:\n",
      "        texton_saliency_score[i] = individual_texton_saliency_score[i]\n",
      "        \n",
      "texton_saliency_map = texton_saliency_score[segmentation]\n",
      "\n",
      "save_img(gray2rgb(texton_saliency_map), 'texton_saliencymap', ext='png')\n",
      "# Image(get_img_filename('texton_saliencymap', 'png'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute significance of each superpixel ===\n",
        "../scratch/PMD1305_244_reduce0_region0/PMD1305_244_reduce0_region0_param91_texton_saliencymap.png already exists"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 4.57 s, sys: 1.34 s, total: 5.91 s\n",
        "Wall time: 5.94 s\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "overall_dir_hist = np.bincount(dirmap[mask].flat)\n",
      "overall_dir_hist_normalized = overall_dir_hist.astype(np.float) / overall_dir_hist.sum()\n",
      "individual_dir_saliency_score = cdist(sp_dir_hist_normalized, overall_dir_hist_normalized[np.newaxis,:], chi2)\n",
      "# individual_dir_saliency_score = np.array([chi2(sp_hist, overall_dir_hist_normalized) for sp_hist in sp_dir_hist_normalized])\n",
      "\n",
      "dir_saliency_score = np.zeros((n_superpixels,))\n",
      "for i, sp_hist in enumerate(sp_dir_hist_normalized):\n",
      "    if i not in bg_superpixels:\n",
      "        dir_saliency_score[i] = individual_dir_saliency_score[i]\n",
      "dir_saliency_score\n",
      "\n",
      "dir_saliency_map = dir_saliency_score[segmentation]\n",
      "save_img(gray2rgb(dir_saliency_map), 'dir_saliencymap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "../scratch/PMD1305_244_reduce0_region0/PMD1305_244_reduce0_region0_param91_dir_saliencymap.tif already exists\n",
        "CPU times: user 4.58 s, sys: 1.33 s, total: 5.91 s\n",
        "Wall time: 5.92 s\n"
       ]
      }
     ],
     "prompt_number": 60
    }
   ],
   "metadata": {}
  }
 ]
}