{
 "metadata": {
  "name": "",
  "signature": "sha256:a28fea7f21bbe6452687cd5677c55c35244f8bd244d12dab16fa6bd833882ffd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reload_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('/home/yuncong/Brain/pipeline_scripts')\n",
      "from utilities2014 import *\n",
      "import os\n",
      "\n",
      "from scipy.spatial.distance import cdist, pdist, squareform\n",
      "from joblib import Parallel, delayed\n",
      "from skimage.color import gray2rgb\n",
      "\n",
      "from skimage.measure import find_contours\n",
      "import cv2\n",
      "from skimage.util import img_as_float\n",
      "\n",
      "from networkx import from_dict_of_lists, Graph, adjacency_matrix, dfs_postorder_nodes\n",
      "from networkx.algorithms import node_connected_component\n",
      "\n",
      "from scipy.stats import chisquare, chisqprob\n",
      "\n",
      "%run grow_regions_common.ipynb\n",
      "\n",
      "os.environ['GORDON_DATA_DIR'] = '/home/yuncong/project/DavidData2014tif/'\n",
      "os.environ['GORDON_REPO_DIR'] = '/home/yuncong/Brain'\n",
      "os.environ['GORDON_RESULT_DIR'] = '/home/yuncong/project/DavidData2014results/'\n",
      "os.environ['GORDON_LABELING_DIR'] = '/home/yuncong/project/DavidData2014labelings/'\n",
      "\n",
      "dm = DataManager(data_dir=os.environ['GORDON_DATA_DIR'], \n",
      "  repo_dir=os.environ['GORDON_REPO_DIR'], \n",
      "  result_dir=os.environ['GORDON_RESULT_DIR'], \n",
      "  labeling_dir=os.environ['GORDON_LABELING_DIR'])\n",
      "\n",
      "dm.set_stack('RS141')\n",
      "dm.set_resol('x5')\n",
      "dm.set_gabor_params(gabor_params_id='blueNisslWide')\n",
      "dm.set_segmentation_params(segm_params_id='blueNisslRegular')\n",
      "dm.set_vq_params(vq_params_id='blueNissl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_cluster_score(cluster, texton_hists, neighbors, output=False):\n",
      "    \n",
      "    cluster_list = list(cluster)    \n",
      "    cluster_avg = texton_hists[cluster_list].mean(axis=0)\n",
      "    \n",
      "    surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
      "    surrounds_list = list(surrounds)\n",
      "\n",
      "#     f_avg = texton_freqs[cluster_list].sum(axis=0)\n",
      "    \n",
      "#     interior_pvals = [chi2pval(f_avg, texton_freqs[i])[0] for i in cluster_list]\n",
      "#     interior_pval = np.mean(interior_pvals)\n",
      "    interior_pval = 0\n",
      "\n",
      "#     surround_pvals = [chi2pval(f_avg, texton_freqs[i])[0] for i in surrounds_list]\n",
      "#     surround_pval = np.max(surround_pvals)\n",
      "    surround_pval = 0\n",
      "#     assert not np.isnan(surround_pval), (cluster_list, surrounds_list[where(np.isnan(surround_pvals))[0]])\n",
      "\n",
      "    interior_dist = np.squeeze(cdist([cluster_avg], texton_hists[cluster_list], chi2)).mean()\n",
      "    surround_dist = np.squeeze(cdist([cluster_avg], texton_hists[surrounds_list], chi2)).min()\n",
      "\n",
      "#     surround_stats = [chi2pval(f_avg, texton_freqs[i])[1] for i in surrounds_list]\n",
      "#     surround_stat = np.min(surround_stats) \n",
      "    \n",
      "#     compactness = 0\n",
      "    compactness = len(find_boundaries([cluster], neighbors=neighbors)[0])**2/float(len(cluster))\n",
      "    compactness = .001 * np.maximum(compactness-40,0)**2\n",
      "    \n",
      "    size_prior = .1 * (1-np.exp(-.8*len(cluster)))\n",
      "    \n",
      "#     score = 0. * surround_dist - interior_dist -  .0001 * np.maximum(compactness-50,0)**2\n",
      "#     score = - surround_pval + interior_pval\n",
      "#     score = - interior_dist\n",
      "#     score = surround_dist - 0 * interior_dist - compactness\n",
      "#     score = - surround_pval - 0 * interior_dist - compactness + size_prior\n",
      "    score = surround_dist - 0 * interior_dist - compactness + size_prior\n",
      "    \n",
      "#     return score, surround_pval, interior_pval, compactness\n",
      "    return score, surround_dist, interior_dist, compactness, surround_pval, interior_pval, size_prior\n",
      "#     return score, surround_stat, interior_dist, compactness\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grow_cluster3(seed, neighbors, texton_hists, output=False, all_history=False):\n",
      "            \n",
      "    visited = set([])\n",
      "    curr_cluster = set([])\n",
      "        \n",
      "    candidate_scores = [0]\n",
      "    candidate_sps = [seed]\n",
      "\n",
      "    score_tuples = []\n",
      "    added_sps = []\n",
      "    \n",
      "    iter_ind = 0\n",
      "        \n",
      "    while len(candidate_sps) > 0:\n",
      "\n",
      "        best_ind = np.argmax(candidate_scores)\n",
      "        \n",
      "        heuristic = candidate_scores[best_ind]\n",
      "        sp = candidate_sps[best_ind]\n",
      "        \n",
      "        del candidate_scores[best_ind]\n",
      "        del candidate_sps[best_ind]\n",
      "        \n",
      "        if sp in curr_cluster:\n",
      "            continue\n",
      "                \n",
      "        iter_ind += 1\n",
      "        curr_cluster.add(sp)\n",
      "        added_sps.append(sp)\n",
      "        \n",
      "        tt = compute_cluster_score(curr_cluster, texton_hists=texton_hists, neighbors=neighbors)\n",
      "        tot, exterior, interior, compactness, surround_pval, interior_pval, size_prior = tt\n",
      "        score_tuples.append(np.r_[heuristic, tt])\n",
      "        \n",
      "        if output:\n",
      "            print 'iter', iter_ind, 'add', sp\n",
      "\n",
      "        visited.add(sp)\n",
      "        \n",
      "        candidate_sps = (set(candidate_sps) | (neighbors[sp] - set([-1])) | (visited - curr_cluster)) - curr_cluster\n",
      "        candidate_sps = list(candidate_sps)\n",
      "        \n",
      "#         f_avg = texton_freqs[list(curr_cluster)].sum(axis=0)\n",
      "#         candidate_scores = [chi2pval(f_avg, texton_freqs[i])[0] for i in candidate_sps]\n",
      "\n",
      "        h_avg = texton_hists[list(curr_cluster)].mean(axis=0)\n",
      "        candidate_scores = [-chi2(h_avg, texton_hists[i]) for i in candidate_sps]\n",
      "\n",
      "#         candidate_scores = [compute_cluster_score(curr_cluster | set([s])) for s in candidate_sps]\n",
      "                \n",
      "        if len(visited) > int(n_superpixels * 0.03):\n",
      "            break\n",
      "\n",
      "    score_tuples = np.array(score_tuples)\n",
      "    \n",
      "    min_size = 2\n",
      "    scores = score_tuples[:,1]\n",
      "    cutoff = np.argmax(scores[min_size:]) + min_size\n",
      "    \n",
      "    if output:\n",
      "        print 'cutoff', cutoff\n",
      "\n",
      "    final_cluster = added_sps[:cutoff]\n",
      "    final_score = scores[cutoff]\n",
      "    \n",
      "    if all_history:\n",
      "        return list(final_cluster), final_score, added_sps, score_tuples\n",
      "    else:\n",
      "        return list(final_cluster), final_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def visualize_cluster(cluster, segmentation, segmentation_vis, text=False, highlight_seed=False):\n",
      "\n",
      "    a = -1*np.ones_like(segmentation)        \n",
      "\n",
      "    for i, c in enumerate(cluster):\n",
      "        if highlight_seed:\n",
      "            if i == 0:\n",
      "                a[segmentation == c] = 1       \n",
      "            else:\n",
      "                a[segmentation == c] = 0\n",
      "        else:\n",
      "            a[segmentation == c] = 0\n",
      "\n",
      "    vis = label2rgb(a, image=segmentation_vis)\n",
      "\n",
      "    vis = img_as_ubyte(vis[...,::-1])\n",
      "\n",
      "    if text:\n",
      "\n",
      "        import cv2\n",
      "\n",
      "        for i, sp in enumerate(cluster):\n",
      "            vis = cv2.putText(vis, str(i), \n",
      "                              tuple(np.floor(sp_properties[sp, [1,0]] - np.array([10,-10])).astype(np.int)), \n",
      "                              cv2.FONT_HERSHEY_DUPLEX,\n",
      "                              1., ((0,255,255)), 1)\n",
      "\n",
      "    return vis.copy()\n",
      "\n",
      "def visualize_multiple_clusters(clusters, segmentation, segmentation_vis, alpha_blend=True):\n",
      "    \n",
      "    \n",
      "    if len(clusters) == 0:\n",
      "        return segmentation_vis\n",
      "    \n",
      "    colors = np.loadtxt('../visualization/100colors.txt')\n",
      "    n_superpixels = segmentation.max() + 1\n",
      "    \n",
      "    mask_alpha = .4\n",
      "    \n",
      "    if alpha_blend:\n",
      "        \n",
      "        for ci, c in enumerate(clusters):\n",
      "            m =  np.zeros((n_superpixels,), dtype=np.float)\n",
      "            m[list(c)] = mask_alpha\n",
      "            alpha = m[segmentation]\n",
      "            alpha[~dm.mask] = 0\n",
      "            \n",
      "            mm = np.zeros((n_superpixels,3), dtype=np.float)\n",
      "            mm[list(c)] = colors[ci]\n",
      "            blob = mm[segmentation]\n",
      "            \n",
      "            if ci == 0:\n",
      "                vis = alpha_blending(blob, gray2rgb(dm.image), alpha, 1.*np.ones((dm.image_height, dm.image_width)))\n",
      "            else:\n",
      "                vis = alpha_blending(blob, vis[..., :-1], alpha, vis[..., -1])\n",
      "                \n",
      "    else:\n",
      "    \n",
      "        n_superpixels = segmentation.max() + 1\n",
      "\n",
      "        n = len(clusters)\n",
      "        m = -1*np.ones((n_superpixels,), dtype=np.int)\n",
      "\n",
      "        for ci, c in enumerate(clusters):\n",
      "            m[list(c)] = ci\n",
      "\n",
      "        a = m[segmentation]\n",
      "        a[~dm.mask] = -1\n",
      "    #     a = -1*np.ones_like(segmentation)\n",
      "    #     for ci, c in enumerate(clusters):\n",
      "    #         for i in c:\n",
      "    #             a[segmentation == i] = ci\n",
      "\n",
      "        vis = label2rgb(a, image=segmentation_vis)\n",
      "\n",
      "        vis = img_as_ubyte(vis[...,::-1])\n",
      "\n",
      "    #     for ci, c in enumerate(clusters):\n",
      "    #         for i, sp in enumerate(c):\n",
      "    #             vis = cv2.putText(vis, str(i), \n",
      "    #                               tuple(np.floor(sp_properties[sp, [1,0]] - np.array([10,-10])).astype(np.int)), \n",
      "    #                               cv2.FONT_HERSHEY_DUPLEX,\n",
      "    #                               1., ((0,255,255)), 1)\n",
      "    \n",
      "    return vis.copy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_boundaries(clusters, neighbors, neighbor_graph=None):\n",
      "        \n",
      "    n_superpixels = len(clusters)\n",
      "    \n",
      "    surrounds_sps = []\n",
      "    frontiers_sps = []\n",
      "    \n",
      "    for cluster_ind, cluster in enumerate(clusters):\n",
      "        \n",
      "        surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
      "#         surrounds = set([i for i in surrounds if any([(n not in cluster) and (n not in surrounds) for n in neighbors[i]])])\n",
      "        surrounds = set([i for i in surrounds if any([n not in cluster for n in neighbors[i]])])\n",
      "\n",
      "        if len(surrounds) == 0:\n",
      "            continue\n",
      "\n",
      "        frontiers = set.union(*[neighbors[c] for c in surrounds]) & set(cluster)\n",
      "\n",
      "        if neighbor_graph is not None:\n",
      "        \n",
      "            surrounds_subgraph = neighbor_graph.subgraph(surrounds)\n",
      "            surrounds_traversal = list(dfs_postorder_nodes(surrounds_subgraph))\n",
      "\n",
      "            frontiers_subgraph = neighbor_graph.subgraph(frontiers)\n",
      "            frontiers_traversal = list(dfs_postorder_nodes(frontiers_subgraph))\n",
      "\n",
      "            surrounds_sps.append(surrounds_traversal)\n",
      "            frontiers_sps.append(frontiers_traversal)\n",
      "        \n",
      "        else:\n",
      "            surrounds_sps.append(list(surrounds))\n",
      "            frontiers_sps.append(list(frontiers))\n",
      "        \n",
      "    return surrounds_sps, frontiers_sps\n",
      "\n",
      "\n",
      "def chi2pval(O1, O2):\n",
      "    n1 = O1.sum()\n",
      "    n2 = O2.sum()\n",
      "    n = n1 + n2\n",
      "    nc = O1 + O2\n",
      "    E1 = n1/n*nc\n",
      "    E2 = n2/n*nc\n",
      "    v = np.nonzero((O1 > 0) &  (O2 > 0) & (vars > 0))[0]\n",
      "    if len(v) == 0:\n",
      "        return 1e-6, np.nan\n",
      "    dof = max(len(v)-1, 1)\n",
      "    q = np.sum((O1[v]-E1[v])**2/vars[v]+(O2[v]-E2[v])**2/vars[v])\n",
      "    return chisqprob(q, dof), q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_overlap(c1, c2):\n",
      "#     return float(len(c1 & c2))/min(len(c1),len(c2))\n",
      "    return float(len(c1 & c2))/len(c1 | c2)\n",
      "\n",
      "def compute_overlap_partial(indices, sets):\n",
      "    n_sets = len(sets)\n",
      "    \n",
      "    overlap_matrix = np.zeros((len(indices), n_sets))\n",
      "        \n",
      "    for ii, i in enumerate(indices):\n",
      "        for j in range(n_sets):\n",
      "            c1 = set(sets[i])\n",
      "            c2 = set(sets[j])\n",
      "            overlap_matrix[ii, j] = compute_overlap(c1, c2)\n",
      "            \n",
      "    return overlap_matrix\n",
      "\n",
      "def set_pairwise_distances(sets):\n",
      "\n",
      "    partial_overlap_mat = Parallel(n_jobs=16, max_nbytes=1e6)(delayed(compute_overlap_partial)(s, sets) \n",
      "                                        for s in np.array_split(range(len(sets)), 16))\n",
      "    overlap_matrix = np.vstack(partial_overlap_mat)\n",
      "    distance_matrix = 1 - overlap_matrix\n",
      "    \n",
      "    return distance_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.spatial.distance import pdist, squareform\n",
      "from scipy.cluster.hierarchy import average, fcluster, leaders, complete, single, dendrogram\n",
      "\n",
      "def group_clusters(clusters=None, dist_thresh = 0.01, distance_matrix=None):\n",
      "\n",
      "    if distance_matrix is None:\n",
      "        assert clusters is not None\n",
      "        distance_matrix = set_pairwise_distances(clusters)\n",
      "\n",
      "    lk = complete(squareform(distance_matrix))\n",
      "#     lk = average(squareform(distance_matrix))\n",
      "#     lk = single(squareform(distance_matrix))\n",
      "\n",
      "    # T = fcluster(lk, 1.15, criterion='inconsistent')\n",
      "    T = fcluster(lk, dist_thresh, criterion='distance')\n",
      "\n",
      "    n_groups = len(set(T))    \n",
      "    groups = [None] * n_groups\n",
      "\n",
      "    for group_id in range(n_groups):\n",
      "        groups[group_id] = where(T == group_id)[0]\n",
      "        \n",
      "    return [g for g in groups if len(g) > 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for section_id in range(1, 30):\n",
      "for section_id in range(16,17):\n",
      "\n",
      "    dm.set_slice(section_id)\n",
      "    dm._load_image()\n",
      "\n",
      "    texton_hists = dm.load_pipeline_result('texHist', 'npy')\n",
      "    segmentation = dm.load_pipeline_result('segmentation', 'npy')\n",
      "    n_superpixels = len(unique(segmentation)) - 1\n",
      "    textonmap = dm.load_pipeline_result('texMap', 'npy')\n",
      "    n_texton = len(np.unique(textonmap)) - 1\n",
      "    neighbors = dm.load_pipeline_result('neighbors', 'npy')\n",
      "    sp_properties = dm.load_pipeline_result('spProps', 'npy')\n",
      "    segmentation_vis = dm.load_pipeline_result('segmentationWithoutText', 'jpg')\n",
      "    texton_freqs = texton_hists * sp_properties[:,2][:, np.newaxis]\n",
      "    vars = np.var(texton_freqs, axis=0)\n",
      "\n",
      "    try:\n",
      "        sp_sp_dists = dm.load_pipeline_result('texHistPairwiseDist', 'npy')\n",
      "#         raise\n",
      "    except:\n",
      "        def f(a):\n",
      "            sp_dists = cdist(a, texton_hists, metric=chi2)\n",
      "    #         sp_dists = cdist(a, texton_hists, metric=js)\n",
      "            return sp_dists\n",
      "\n",
      "        sp_dists = Parallel(n_jobs=16)(delayed(f)(s) for s in np.array_split(texton_hists, 16))\n",
      "#         sp_sp_dists = np.vstack(sp_dists)\n",
      "\n",
      "        dm.save_pipeline_result(sp_sp_dists, 'texHistPairwiseDist', 'npy')\n",
      "\n",
      "    center_dists = pdist(sp_properties[:, :2])\n",
      "    center_dist_matrix = squareform(center_dists)\n",
      "\n",
      "    neighbors_dict = dict(zip(np.arange(n_superpixels), [list(i) for i in neighbors]))\n",
      "    neighbor_graph = from_dict_of_lists(neighbors_dict)\n",
      "\n",
      "\n",
      "    try:\n",
      "        expansion_clusters_tuples = dm.load_pipeline_result('clusters', 'pkl')\n",
      "#         raise\n",
      "    except Exception as e:\n",
      "\n",
      "        import time\n",
      "        b = time.time()\n",
      "\n",
      "        expansion_clusters_tuples = Parallel(n_jobs=16)(delayed(grow_cluster3)(s, neighbors, texton_hists) for s in range(n_superpixels))\n",
      "\n",
      "        print time.time() - b\n",
      "\n",
      "        dm.save_pipeline_result(expansion_clusters_tuples, 'clusters', 'pkl')\n",
      "\n",
      "    # expansion_clusters_tuples = dm.load_pipeline_result('clusters', 'pkl')\n",
      "    expansion_clusters, expansion_cluster_scores = zip(*expansion_clusters_tuples)\n",
      "    expansion_cluster_scores = np.array(expansion_cluster_scores)\n",
      "\n",
      "\n",
      "    try:\n",
      "        D = dm.load_pipeline_result('clusterPairwiseDist', 'npy')\n",
      "#         raise\n",
      "    except:\n",
      "        D = set_pairwise_distances(expansion_clusters)\n",
      "        dm.save_pipeline_result(D, 'clusterPairwiseDist', 'npy')\n",
      "\n",
      "    try:\n",
      "        expansion_cluster_groups = dm.load_pipeline_result('clusterGroups', 'pkl')\n",
      "#         raise\n",
      "    except:\n",
      "        import time\n",
      "        t = time.time()\n",
      "        expansion_cluster_groups = group_clusters(expansion_clusters, dist_thresh=.8, distance_matrix=D)\n",
      "        dm.save_pipeline_result(expansion_cluster_groups, 'clusterGroups', 'pkl')\n",
      "\n",
      "        print time.time() - t\n",
      "\n",
      "    print len(expansion_cluster_groups), 'expansion cluster groups'\n",
      "    expansion_cluster_group_sizes = np.array(map(len, expansion_cluster_groups))\n",
      "\n",
      "\n",
      "    big_group_indices = np.where(expansion_cluster_group_sizes > 5)[0]\n",
      "    n_big_groups = len(big_group_indices)\n",
      "    print n_big_groups, 'big cluster groups'\n",
      "    big_groups = [expansion_cluster_groups[i] for i in big_group_indices]\n",
      "\n",
      "    from collections import Counter\n",
      "\n",
      "    representative_clusters = []\n",
      "    representative_cluster_scores = []\n",
      "    representative_cluster_indices = []\n",
      "\n",
      "    big_groups_valid = []\n",
      "\n",
      "    for g in big_groups:\n",
      "        for i in np.argsort(expansion_cluster_scores[g])[::-1]:\n",
      "            c = expansion_clusters[g[i]]\n",
      "            sc = expansion_cluster_scores[g[i]]\n",
      "            if len(c) > n_superpixels * .004:\n",
      "                representative_clusters.append(c)\n",
      "                representative_cluster_indices.append(g[i])\n",
      "                representative_cluster_scores.append(sc)\n",
      "                big_groups_valid.append(g)\n",
      "                break\n",
      "\n",
      "    print len(representative_clusters), 'representative clusters'\n",
      "\n",
      "    representative_cluster_scores_sorted, representative_clusters_sorted_by_score, \\\n",
      "    representative_cluster_indices_sorted_by_score, \\\n",
      "    big_groups_sorted_by_score = map(list, zip(*sorted(zip(representative_cluster_scores, \n",
      "                                                            representative_clusters,\n",
      "                                                            representative_cluster_indices,\n",
      "                                                            big_groups_valid), reverse=True)))\n",
      "\n",
      "    representative_clusters = zip(representative_cluster_scores_sorted, representative_clusters_sorted_by_score, \n",
      "                   representative_cluster_indices_sorted_by_score, \n",
      "                   big_groups_sorted_by_score)\n",
      "\n",
      "    dm.save_pipeline_result(representative_clusters, 'representativeClusters', 'pkl')\n",
      "    \n",
      "\n",
      "    final_clusters_sorted_by_score = representative_clusters_sorted_by_score[:50]\n",
      "    final_cluster_scores_sorted = representative_cluster_scores_sorted[:50]\n",
      "    final_cluster_indices_sorted_by_score = representative_cluster_indices_sorted_by_score[:50]\n",
      "\n",
      "\n",
      "    ###################\n",
      "\n",
      "\n",
      "    vis = visualize_multiple_clusters(final_clusters_sorted_by_score[:10], segmentation=segmentation, \n",
      "                                      segmentation_vis=dm.image)\n",
      "    dm.save_pipeline_result( vis, 'regionsTop10' , 'jpg')\n",
      "\n",
      "    vis = visualize_multiple_clusters(final_clusters_sorted_by_score[:20], segmentation=segmentation, \n",
      "                                      segmentation_vis=dm.image)\n",
      "    dm.save_pipeline_result( vis, 'regionsTop20' , 'jpg')\n",
      "\n",
      "    vis = visualize_multiple_clusters(final_clusters_sorted_by_score[:30], segmentation=segmentation, \n",
      "                                      segmentation_vis=dm.image)\n",
      "    dm.save_pipeline_result( vis, 'regionsTop30' , 'jpg')\n",
      "\n",
      "#     vis = visualize_multiple_clusters(final_clusters_sorted_by_score[:40], segmentation=segmentation, \n",
      "#                                       segmentation_vis=dm.image)\n",
      "#     dm.save_pipeline_result( vis, 'regionsTop40' , 'jpg')\n",
      "\n",
      "#     vis = visualize_multiple_clusters(final_clusters_sorted_by_score[10:20], segmentation=segmentation, \n",
      "#                                       segmentation_vis=dm.image)\n",
      "#     dm.save_pipeline_result( vis, 'regionsTop10to20' , 'jpg')\n",
      "\n",
      "#     vis = visualize_multiple_clusters(final_clusters_sorted_by_score[20:30], segmentation=segmentation, \n",
      "#                                       segmentation_vis=dm.image)\n",
      "#     dm.save_pipeline_result( vis, 'regionsTop20to30' , 'jpg')\n",
      "\n",
      "#     vis = visualize_multiple_clusters(final_clusters_sorted_by_score[30:40], segmentation=segmentation, \n",
      "#                                       segmentation_vis=dm.image)\n",
      "#     dm.save_pipeline_result( vis, 'regionsTop30to40' , 'jpg')\n",
      "\n",
      "#     vis = visualize_multiple_clusters(final_clusters_sorted_by_score[40:], segmentation=segmentation, \n",
      "#                                       segmentation_vis=dm.image)\n",
      "#     dm.save_pipeline_result( vis, 'regionsTop40toX' , 'jpg')\n",
      "\n",
      "\n",
      "    #     fig = plt.figure(figsize=(10,10))\n",
      "    #     plt.imshow(vis)\n",
      "    #     plt.title('sorted group ' + str(i) + ', score ' + str(s))\n",
      "    #     plt.axis('off')\n",
      "    #     plt.show()\n",
      "\n",
      "\n",
      "    # colors = (np.loadtxt('../visualization/100colors.txt') * 255).astype(np.int)\n",
      "\n",
      "    # def visualize_contours(clusters):\n",
      "\n",
      "    #     vis = dm.image_rgb.copy()\n",
      "\n",
      "    #     for ci, c in enumerate(clusters):\n",
      "    #         q = np.zeros((n_superpixels,))\n",
      "    #         q[c] = 1.\n",
      "    #         v = q[segmentation]\n",
      "    #         contours = find_contours(img_as_float(v), 0.8)\n",
      "    #         contour = contours[np.argmax(map(len, contours))]\n",
      "    #         contour = np.round(contour[:,::-1].reshape((-1,1,2))).astype(np.int)\n",
      "    #         cv2.polylines(vis, [contour], isClosed=True, color=colors[ci%len(colors)], thickness=10) \n",
      "    #     #     cv2.polylines(vis, [contour], isClosed=True, color=[237,194,136], thickness=5) \n",
      "\n",
      "    #     #     ax.plot(contour[:,1], contour[:,0])\n",
      "\n",
      "    #     # fig.savefig('tmp.png', bbox_inches='tight', pad_inches=0)\n",
      "    #     # fig.savefig('tmp.png', pad_inches=0)\n",
      "    #     # SaveFigureAsImage('tmp.png', fig, orig_size=dm.image.shape[:2])\n",
      "    #     # FileLink('tmp.png')\n",
      "    #     return vis\n",
      "\n",
      "    # # dm.save_pipeline_result(representative_clusters_sorted_by_score, 'goodRegions', 'pkl')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score[:10])\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTop10', 'jpg')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score[10:20])\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTop10to20', 'jpg')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score[20:30])\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTop20to30', 'jpg')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score[30:])\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTop30to40', 'jpg')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score)\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTopAll', 'jpg')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score[:20])\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTop20', 'jpg')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score[:30])\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTop30', 'jpg')\n",
      "\n",
      "    # vis = visualize_contours(final_clusters_sorted_by_score[:40])\n",
      "    # dm.save_pipeline_result(np.uint8(vis), 'contoursTop40', 'jpg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_texHist.npy\n",
        "loaded /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_segm-blueNisslRegular_segmentation.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loaded /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-vq-blueNissl_texMap.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loaded /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_segm-blueNisslRegular_neighbors.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loaded /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_segm-blueNisslRegular_spProps.npy\n",
        "loaded /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_segm-blueNisslRegular_segmentationWithoutText.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "saved /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_texHistPairwiseDist.npy\n",
        "loaded /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_clusters.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n",
        "/home/yuncong/Brain/pipeline_scripts/utilities2014.py:758: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "saved /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_clusterPairwiseDist.npy\n",
        "saved /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_clusterGroups.pkl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12.6623940468\n",
        "1637 expansion cluster groups\n",
        "49 big cluster groups\n",
        "43 representative clusters\n",
        "saved /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_representativeClusters.pkl\n",
        "saved /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_regionsTop10.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "saved /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_regionsTop20.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "saved /home/yuncong/project/DavidData2014results/RS141/0016/RS141_x5_0016_gabor-blueNisslWide-segm-blueNisslRegular-vq-blueNissl_regionsTop30.jpg"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}