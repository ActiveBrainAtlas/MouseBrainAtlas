{
 "metadata": {
  "name": "",
  "signature": "sha256:6b7172da3574af0e715a7b4d5fd4d17502161c9850dd44d7c02399bbae84e481"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "import matplotlib\n",
      "# Force matplotlib to not use any Xwindows backend.\n",
      "matplotlib.use('Agg')\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import random\n",
      "import itertools\n",
      "import sys\n",
      "import os\n",
      "from multiprocessing import Pool\n",
      "import json\n",
      "import glob\n",
      "import re\n",
      "import subprocess\n",
      "import argparse\n",
      "import pprint\n",
      "\n",
      "from skimage.segmentation import slic, mark_boundaries\n",
      "from skimage.measure import regionprops\n",
      "from skimage.util import img_as_ubyte\n",
      "from skimage.color import hsv2rgb, label2rgb, gray2rgb\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "from skimage.filter import gabor_kernel\n",
      "from skimage.transform import rescale, resize\n",
      "\n",
      "from scipy.ndimage import gaussian_filter, measurements\n",
      "from scipy.sparse import coo_matrix\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "from scipy.signal import fftconvolve\n",
      "\n",
      "from IPython.display import FileLink, Image, FileLinks\n",
      "\n",
      "import utilities\n",
      "\n",
      "from joblib import Parallel, delayed\n",
      "\n",
      "\n",
      "class sigboost(object):\n",
      "\n",
      "    def __init__(self, img, segmentation, sp_texton_hist_normalized, sp_dir_hist_normalized,\n",
      "                 overall_texton_hist_normalized, overall_dir_hist_normalized,\n",
      "                 labeling, bg_superpixels, neighbors, result_dir, param):\n",
      "        self.img = img\n",
      "        self.segmentation = segmentation\n",
      "        self.labeling = labeling\n",
      "\n",
      "        self.D_texton_null = np.squeeze(\n",
      "                cdist(sp_texton_hist_normalized, [overall_texton_hist_normalized], chi2))\n",
      "        self.D_dir_null = np.squeeze(\n",
      "               cdist(sp_dir_hist_normalized, [overall_dir_hist_normalized], chi2))\n",
      "        self.p = sp_texton_hist_normalized\n",
      "        self.q = sp_dir_hist_normalized\n",
      "\n",
      "        self.bg_superpixels = bg_superpixels\n",
      "\n",
      "        self.result_dir = result_dir\n",
      "\n",
      "        self.n_models = param['n_models']\n",
      "        self.frontier_contrast_diff_thresh = param['frontier_contrast_diff_thresh']\n",
      "        self.lr_grow_thresh = param['lr_grow_thresh']\n",
      "        self.beta = param['beta']\n",
      "        self.lr_decision_thresh = param['lr_decision_thresh']\n",
      "\n",
      "        self.re_thresh_min = 0.01\n",
      "        self.re_thresh_max = 0.8\n",
      "\n",
      "        self.n_superpixels = segmentation.max() + 1\n",
      "\n",
      "    def chi2(u, v):\n",
      "        r = np.nansum((u - v) ** 2 / (u + v))\n",
      "        return r\n",
      "\n",
      "    def grow_cluster_relative_entropy(seed, debug=False,\n",
      "                                      frontier_contrast_diff_thresh=0.1,\n",
      "                                      max_cluster_size=100):\n",
      "        '''\n",
      "        find the connected cluster of superpixels that have similar texture, starting from a superpixel as seed\n",
      "        '''\n",
      "\n",
      "        bg_set = set(self.bg_superpixels.tolist())\n",
      "\n",
      "        if seed in bg_set:\n",
      "            return [], -1\n",
      "\n",
      "        prev_frontier_contrast = np.inf\n",
      "        for re_thresh in np.arange(self.re_thresh_min, self.re_thresh_max, .01):\n",
      "\n",
      "            curr_cluster = set([seed])\n",
      "            frontier = [seed]\n",
      "\n",
      "            while len(frontier) > 0:\n",
      "                u = frontier.pop(-1)\n",
      "                for v in neighbors[u]:\n",
      "                    if v in self.bg_superpixels or v in curr_cluster:\n",
      "                        continue\n",
      "\n",
      "                    if chi2(self.p[v], self.p[seed]) < re_thresh:\n",
      "                        curr_cluster.add(v)\n",
      "                        frontier.append(v)\n",
      "\n",
      "            surround = set.union(\n",
      "                *[self.neighbors[i] for i in curr_cluster]) - set.union(curr_cluster, bg_set)\n",
      "            if len(surround) == 0:\n",
      "                return curr_cluster, re_thresh\n",
      "\n",
      "            frontier_in_cluster = set.intersection(\n",
      "                set.union(*[self.neighbors[i] for i in surround]), curr_cluster)\n",
      "            frontier_contrasts = [np.nanmax([chi2(p[i], p[j]) for j in self.neighbors[i] if j not in bg_set]) for i in frontier_in_cluster]\n",
      "            frontier_contrast = np.max(frontier_contrasts)\n",
      "\n",
      "            if len(curr_cluster) > max_cluster_size or \\\n",
      "                    frontier_contrast - prev_frontier_contrast > frontier_contrast_diff_thresh:\n",
      "                return curr_cluster, re_thresh\n",
      "\n",
      "            prev_frontier_contrast = frontier_contrast\n",
      "            prev_cluster = curr_cluster\n",
      "            prev_re_thresh = re_thresh\n",
      "\n",
      "        return curr_cluster, re_thresh\n",
      "\n",
      "    def grow_cluster_likelihood_ratio(seed, texton_model, dir_model, debug=False, lr_grow_thresh=0.1):\n",
      "        '''\n",
      "        find the connected cluster of superpixels that are more likely to be explained by given model than by null, starting from a superpixel as seed\n",
      "        '''\n",
      "\n",
      "        if seed in self.bg_superpixels:\n",
      "            return [], -1\n",
      "\n",
      "        curr_cluster = set([seed])\n",
      "        frontier = [seed]\n",
      "\n",
      "        while len(frontier) > 0:\n",
      "            u = frontier.pop(-1)\n",
      "            for v in self.neighbors[u]:\n",
      "                if v in self.bg_superpixels or v in curr_cluster:\n",
      "                    continue\n",
      "\n",
      "                ratio_v = self.D_texton_null[v] - chi2(p[v], texton_model) +\\\n",
      "                    \t\tself.D_dir_null[v] - chi2(q[v], dir_model)\n",
      "\n",
      "                if ratio_v > lr_grow_thresh:\n",
      "                    curr_cluster.add(v)\n",
      "                    frontier.append(v)\n",
      "\n",
      "        return curr_cluster, lr_grow_thresh\n",
      "\n",
      "    def grow_cluster_likelihood_ratio_precomputed(seed, D_texton_model, D_dir_model, debug=False, lr_grow_thresh=0.1):\n",
      "        '''\n",
      "        find the connected cluster of superpixels that are more likely to be explained by given model than by null, starting from a superpixel as seed\n",
      "        using pre-computed distances between model and superpixels\n",
      "        '''\n",
      "\n",
      "        if seed in self.bg_superpixels:\n",
      "            return [], -1\n",
      "\n",
      "        curr_cluster = set([seed])\n",
      "        frontier = [seed]\n",
      "\n",
      "        while len(frontier) > 0:\n",
      "            u = frontier.pop(-1)\n",
      "            for v in self.neighbors[u]:\n",
      "                if v in self.bg_superpixels or v in curr_cluster:\n",
      "                    continue\n",
      "\n",
      "                ratio_v = self.D_texton_null[v] - D_texton_model[v] +\\\n",
      "                    self.D_dir_null[v] - D_dir_model[v]\n",
      "\n",
      "                if ratio_v > lr_grow_thresh:\n",
      "                    curr_cluster.add(v)\n",
      "                    frontier.append(v)\n",
      "\n",
      "        return curr_cluster, lr_grow_thresh\n",
      "\n",
      "    def visualize_cluster(self, scores, cluster='all', title='', filename=None):\n",
      "        '''\n",
      "        Generate black and white image with the cluster of superpixels highlighted\n",
      "        '''\n",
      "        vis = scores[self.segmentation]\n",
      "        if cluster != 'all':\n",
      "            cluster_selection = np.equal.outer(\n",
      "                self.segmentation, cluster).any(axis=2)\n",
      "            vis[~cluster_selection] = 0\n",
      "\n",
      "        plt.matshow(vis, cmap=plt.cm.Greys_r)\n",
      "        plt.axis('off')\n",
      "        plt.title(title)\n",
      "        if filename is not None:\n",
      "            plt.savefig(\n",
      "                os.path.join(self.result_dir, 'stages', filename + '.png'), bbox_inches='tight')\n",
      "    #     plt.show()\n",
      "        plt.close()\n",
      "\n",
      "    def paint_cluster_on_img(self, cluster, title, filename=None):\n",
      "        '''\n",
      "        Highlight the cluster of superpixels on the real image\n",
      "        '''\n",
      "        cluster_map = -1 * np.ones_like(self.segmentation)\n",
      "        for s in cluster:\n",
      "            cluster_map[self.segmentation == s] = 1\n",
      "        vis = label2rgb(cluster_map, image=img)\n",
      "        plt.imshow(vis, cmap=plt.cm.Greys_r)\n",
      "        plt.axis('off')\n",
      "        plt.title(title)\n",
      "        if filename is not None:\n",
      "            plt.savefig(\n",
      "                os.path.join(self.result_dir, 'stages', filename + '.png'), bbox_inches='tight')\n",
      "        #     plt.show()\n",
      "        plt.close()\n",
      "\n",
      "    def paint_clusters_on_img(self, clusters, title, filename=None):\n",
      "        '''\n",
      "        Highlight the clusters of superpixels on the real image\n",
      "        '''\n",
      "        cluster_map = -1 * np.ones_like(self.segmentation)\n",
      "        for i, cluster in enumerate(clusters):\n",
      "            for j in cluster:\n",
      "                cluster_map[segmentation == j] = i\n",
      "        vis = label2rgb(cluster_map, image=img)\n",
      "        plt.imshow(vis, cmap=plt.cm.Greys_r)\n",
      "        plt.axis('off')\n",
      "        plt.title(title)\n",
      "        if filename is not None:\n",
      "            plt.savefig(\n",
      "                os.path.join(self.result_dir, 'stages', filename + '.png'), bbox_inches='tight')\n",
      "        #     plt.show()\n",
      "        plt.close()\n",
      "\n",
      "\n",
      "    def boost(self):\n",
      "        '''\n",
      "        perform sigboost\n",
      "        '''\n",
      "\n",
      "        # create output directory\n",
      "        f = os.path.join(self.result_dir, 'stages')\n",
      "        if not os.path.exists(f):\n",
      "            os.makedirs(f)\n",
      "\n",
      "\n",
      "        # compute RE-clusters of every superpixel\n",
      "        r = Parallel(n_jobs=16)(delayed(grow_cluster_relative_entropy)(i, frontier_contrast_diff_thresh=self.frontier_contrast_diff_thresh)\n",
      "                                for i in range(self.n_superpixels))\n",
      "        clusters = [list(c) for c, t in r]\n",
      "        print 'RE-clusters computed'\n",
      "\n",
      "\n",
      "        # initialize models\n",
      "        self.texton_models = np.zeros((n_models, n_texton))\n",
      "        self.dir_models = np.zeros((n_models, n_angle))\n",
      "\n",
      "        self.seed_indices = np.zeros((n_models,))\n",
      "\n",
      "        weights = np.ones((self.n_superpixels, )) / self.n_superpixels\n",
      "        weights[self.bg_superpixels] = 0\n",
      "\n",
      "        # begin boosting loop; learn one model at each iteration\n",
      "        for t in range(n_models):\n",
      "\n",
      "            print 'model %d' % (t)\n",
      "\n",
      "            # Compute significance scores for every superpixel;\n",
      "            # the significance score is defined as the average log likelihood ratio in a superpixel's RE-cluster\n",
      "            sig_score = np.zeros((self.n_superpixels, ))\n",
      "            for i in self.fg_superpixels:\n",
      "                cluster = clusters[i]\n",
      "                sig_score[i] = np.mean(weights[cluster] *\n",
      "                                       (D_texton_null[cluster] - np.array([chi2(p[j], p[i]) for j in cluster]) +\n",
      "                                        D_dir_null[cluster] - np.array([chi2(q[j], q[i]) for j in cluster])))\n",
      "\n",
      "\n",
      "            # Pick the most significant superpixel\n",
      "            seed_sp = sig_score.argsort()[-1]\n",
      "            print \"most significant superpixel\", seed_sp\n",
      "\n",
      "            visualize_cluster(\n",
      "                sig_score, 'all', title='significance score for each superpixel', filename='sigscore%d' % t)\n",
      "\n",
      "            curr_cluster = clusters[seed_sp]\n",
      "            visualize_cluster(\n",
      "                sig_score, curr_cluster, title='cluster growed based on relative entropy', filename='re_cluster%d' % t)\n",
      "\n",
      "            # models are the average of the distributions in the chosen superpixel's RE-cluster\n",
      "            model_texton = sp_texton_hist_normalized[curr_cluster].mean(axis=0)\n",
      "            model_dir = sp_dir_hist_normalized[curr_cluster].mean(axis=0)\n",
      "\n",
      "            # Compute log likelihood ratio of this model against the null, for every superpixel\n",
      "\n",
      "            # RE(pj|pm)\n",
      "            D_texton_model = np.empty((self.n_superpixels,))\n",
      "            D_texton_model[self.fg_superpixels] = np.array(\n",
      "                [chi2(sp_texton_hist_normalized[i], model_texton) for i in self.fg_superpixels])\n",
      "            D_texton_model[self.bg_superpixels] = np.nan\n",
      "\n",
      "            # RE(qj|qm)\n",
      "            D_dir_model = np.empty((self.n_superpixels,))\n",
      "            D_dir_model[self.fg_superpixels] = np.array(\n",
      "                [chi2(sp_dir_hist_normalized[i], model_dir) for i in self.fg_superpixels])\n",
      "            D_dir_model[self.bg_superpixels] = np.nan\n",
      "\n",
      "            # RE(pj|p0)-RE(pj|pm) + RE(qj|q0)-RE(qj|qm)\n",
      "            match_scores = np.empty((self.n_superpixels,))\n",
      "            match_scores[self.fg_superpixels] = D_texton_null[self.fg_superpixels] - D_texton_model[self.fg_superpixels] +\\\n",
      "                D_dir_model[self.fg_superpixels] - D_dir_model[self.fg_superpixels]\n",
      "            match_scores[self.bg_superpixels] = 0\n",
      "\n",
      "            visualize_cluster(\n",
      "                match_scores, 'all', title='match score', filename='match_score%d' % t)\n",
      "\n",
      "            # Find the cluster growed from seed based on log likelihood ratio. Refer to this cluster as the LR-cluster\n",
      "            matched, _ = grow_cluster_likelihood_ratio_precomputed(seed_sp, D_texton_model, D_dir_model, lr_grow_thresh=self.lr_grow_thresh)\n",
      "            matched = list(matched)\n",
      "\n",
      "            visualize_cluster(\n",
      "                match_scores, matched, title='cluster growed based on likelihood ratio', filename='lr_cluster%d' % t)\n",
      "\n",
      "            # Reduce the weights of superpixels in LR-cluster\n",
      "            weights[matched] = weights[matched] * np.exp(-5 * (D_texton_null[matched] - D_texton_model[matched] +\n",
      "                                                               D_dir_null[matched] - D_dir_model[matched]) ** self.beta)\n",
      "            weights[self.bg_superpixels] = 0\n",
      "            weights = weights / weights.sum()\n",
      "            visualize_cluster((weights - weights.min()) / (weights.max() - weights.min()), 'all',\n",
      "                              title='updated superpixel weights', filename='weight%d' % t)\n",
      "\n",
      "            labels = -1 * np.ones_like(self.segmentation)\n",
      "            for i in matched:\n",
      "                labels[self.segmentation == i] = 1\n",
      "            real_image = label2rgb(labels, img)\n",
      "            save_img(real_image, os.path.join('stage', 'real_image_model%d' % t))\n",
      "\n",
      "            # record the model found at this round\n",
      "            self.seed_indices[t] = seed_sp\n",
      "            self.texton_models[t] = model_texton\n",
      "            self.dir_models[t] = model_dir\n",
      "\n",
      "\n",
      "    def find_best_model_per_proc(self, i):\n",
      "        '''\n",
      "        Worker function for finding the best models for every superpixel on the current image.\n",
      "        Best model is the one with the highest likelihood ratio against the null distribution.\n",
      "        '''\n",
      "        model_score = np.empty((n_models, ))\n",
      "\n",
      "        if i in self.bg_superpixels:\n",
      "            return -1\n",
      "        else:\n",
      "            for m in range(n_models):\n",
      "                matched, _ = grow_cluster_likelihood_ratio_precomputed(i, D_texton_model[m], D_dir_model[m],\n",
      "                                                                       lr_grow_thresh=self.lr_grow_thresh)\n",
      "                matched = list(matched)\n",
      "                model_score[m] = np.mean(D_texton_null[matched] - D_texton_model[m, matched] +\n",
      "                                         D_dir_null[matched] - D_dir_model[m, matched])\n",
      "\n",
      "            best_sig = model_score.max()\n",
      "            # sp whose sig is smaller than this is assigned null\n",
      "            if best_sig > self.lr_decision_thresh:\n",
      "                return model_score.argmax()\n",
      "        return -1\n",
      "\n",
      "    def apply_models_curr_img(self):\n",
      "        '''\n",
      "        Find the best models for every superpixel on the current image.\n",
      "        Best model is the one with the highest likelihood ratio against the null distribution.\n",
      "        '''\n",
      "\n",
      "        # Compute the distances between every model and every superpixel\n",
      "        D_texton_model = -1 * np.ones((n_models, self.n_superpixels))\n",
      "        D_dir_model = -1 * np.ones((n_models, self.n_superpixels))\n",
      "        D_texton_model[:, self.fg_superpixels] = cdist(\n",
      "            sp_texton_hist_normalized[self.fg_superpixels], self.texton_models, chi2).T\n",
      "        D_dir_model[:, self.fg_superpixels] = cdist(\n",
      "            sp_dir_hist_normalized[self.fg_superpixels], self.dir_models, chi2).T\n",
      "\n",
      "        # Compute the likelihood ratio for every model on every superpixel, and return the model with the highest ratio\n",
      "        best_model = Parallel(n_jobs=16)(delayed(self.find_best_model_per_proc)(i) for i in range(self.n_superpixels))\n",
      "        labels = np.array(best_model, dtype=np.int)\n",
      "        save_array(labels, 'labels')\n",
      "\n",
      "        labelmap = labels[self.segmentation]\n",
      "        save_array(labelmap, 'labelmap')\n",
      "\n",
      "        labelmap_rgb = label2rgb(labelmap.astype(np.int), image=img)\n",
      "        save_img(labelmap_rgb, 'labelmap')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}