{
 "metadata": {
  "name": "",
  "signature": "sha256:059fa3516f5a8928db4b2329e64f7163fb5aac6c2825d86ca883691f82c3e591"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %load_ext autoreload\n",
      "# %autoreload 2\n",
      "\n",
      "import sigboost\n",
      "import numpy as np\n",
      "import cv2\n",
      "import argparse, os, json, pprint\n",
      "import random\n",
      "import itertools\n",
      "from skimage.filter import gabor_kernel\n",
      "\n",
      "# from multiprocessing import Pool\n",
      "from pathos.multiprocessing import ProcessingPool as Pool\n",
      "\n",
      "from scipy.signal import fftconvolve\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "\n",
      "\n",
      "def load_stuff(args):\n",
      "    params_dir = os.path.realpath(args.params_dir)\n",
      "    param_file = os.path.join(params_dir, 'param_%s.json'%args.param_id)\n",
      "    param_default_file = os.path.join(params_dir, 'param_default.json')\n",
      "    param = json.load(open(param_file, 'r'))\n",
      "    param_default = json.load(open(param_default_file, 'r'))\n",
      "\n",
      "    for k, v in param_default.iteritems():\n",
      "        if not isinstance(param[k], basestring):\n",
      "            if np.isnan(param[k]):\n",
      "                param[k] = v\n",
      "\n",
      "    img_file = os.path.realpath(args.img_file)\n",
      "    img_path, ext = os.path.splitext(img_file)\n",
      "    img_dir, img_name = os.path.split(img_path)\n",
      "\n",
      "    print img_file\n",
      "    img = cv2.imread(img_file, 0)\n",
      "    im_height, im_width = img.shape[:2]\n",
      "    \n",
      "    output_dir = os.path.realpath(args.output_dir)\n",
      "\n",
      "    result_name = img_name + '_param_' + str(param['param_id'])\n",
      "    result_dir = os.path.join(output_dir, result_name)\n",
      "    if not os.path.exists(result_dir):\n",
      "        os.makedirs(result_dir)\n",
      "\n",
      "    return img, param, result_name, output_dir\n",
      "\n",
      "\n",
      "class FeatureExtractor(object):\n",
      "\n",
      "    def __init__(self, img, param):\n",
      "        self.img = img\n",
      "        self.param = param\n",
      "        \n",
      "    def get_kernels(self):\n",
      "\n",
      "        theta_interval = self.param['theta_interval']\n",
      "        self.n_angle = int(180/theta_interval)\n",
      "        freq_step = self.param['freq_step']\n",
      "        freq_max = 1./self.param['min_wavelen']\n",
      "        freq_min = 1./self.param['max_wavelen']\n",
      "        bandwidth = self.param['bandwidth']\n",
      "        self.n_freq = int(np.log(freq_max/freq_min)/np.log(freq_step)) + 1\n",
      "        frequencies = freq_max/freq_step**np.arange(self.n_freq)\n",
      "\n",
      "        kernels = [gabor_kernel(f, theta=t, bandwidth=bandwidth) for f in frequencies \n",
      "                  for t in np.arange(0, np.pi, np.deg2rad(theta_interval))]\n",
      "        self.kernels = map(np.real, kernels)\n",
      "        self.n_kernel = len(kernels)\n",
      "\n",
      "        print '=== filter using Gabor filters ==='\n",
      "        print 'num. of kernels: %d' % (self.n_kernel)\n",
      "        print 'frequencies:', frequencies\n",
      "        print 'wavelength (pixels):', 1/frequencies\n",
      "\n",
      "        max_kern_size = np.max([kern.shape[0] for kern in kernels])\n",
      "        print 'max kernel matrix size:', max_kern_size\n",
      "\n",
      "    def compute_features(self):\n",
      "        self.get_kernels()\n",
      "\n",
      "        def convolve_per_proc(i):\n",
      "            return fftconvolve(self.img, self.kernels[i], 'same').astype(np.half)\n",
      "\n",
      "        filtered = Pool().map(convolve_per_proc, range(self.n_kernel))\n",
      "        \n",
      "        self.features = np.empty((self.img.shape[0], self.img.shape[1], self.n_kernel), dtype=np.half)\n",
      "        for i in range(self.n_kernel):\n",
      "            self.features[...,i] = filtered[i]\n",
      "\n",
      "        del filtered\n",
      "\n",
      "#         save_array(features, 'features')\n",
      "\n",
      "        self.n_feature = self.features.shape[-1]\n",
      "\n",
      "    \n",
      "    def compute_texton(self):\n",
      "        print '=== compute rotation-invariant texton map using K-Means ==='\n",
      "\n",
      "        n_texton = int(self.param['n_texton'])\n",
      "\n",
      "        X = self.features.reshape(-1, self.n_feature)\n",
      "        n_data = X.shape[0]\n",
      "        n_splits = 1000\n",
      "        n_sample = int(self.param['n_sample'])\n",
      "        centroids = np.array(random.sample(X, n_texton))\n",
      "\n",
      "        n_iter = int(self.param['n_iter'])\n",
      "\n",
      "        def compute_dist_per_proc(x):\n",
      "            X_partial, c_all_rot = x\n",
      "            D = cdist(X_partial, c_all_rot, 'sqeuclidean')\n",
      "            ci, ri = np.unravel_index(D.argmin(axis=1), (n_texton, self.n_angle))\n",
      "            return np.c_[ci, ri]\n",
      "\n",
      "        for iteration in range(n_iter):\n",
      "\n",
      "            data = random.sample(X, n_sample)\n",
      "\n",
      "            print 'iteration', iteration\n",
      "            centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, self.n_freq), i)) \n",
      "                                    for c,i in itertools.product(centroids, range(self.n_angle))])\n",
      "\n",
      "            r = Pool().map(compute_dist_per_proc, zip(np.array_split(data, n_splits, axis=0), \n",
      "                                                itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "            \n",
      "#             r = Parallel(n_jobs=16)(delayed(compute_dist_per_proc)(x,c) \n",
      "#                             for x, c in zip(np.array_split(data, n_splits, axis=0), \n",
      "#                                             itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "            res = np.vstack(r)  \n",
      "\n",
      "            labels = res[:,0]\n",
      "            rotations = res[:,1]\n",
      "\n",
      "            centroids_new = np.zeros((n_texton, self.n_feature))\n",
      "            for d, l, r in itertools.izip(data, labels, rotations):\n",
      "                rot = np.concatenate(np.roll(np.split(d, self.n_freq), i))\n",
      "                centroids_new[l] += rot\n",
      "\n",
      "            counts = np.bincount(labels, minlength=n_texton)\n",
      "            centroids_new /= counts[:, np.newaxis]\n",
      "            centroids_new[counts==0] = centroids[counts==0]\n",
      "            print np.sqrt(np.sum((centroids - centroids_new)**2, axis=1)).mean()\n",
      "\n",
      "            centroids = centroids_new\n",
      "\n",
      "        print 'kmeans completes'\n",
      "        centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, self.n_freq), i)) \n",
      "                                    for c,i in itertools.product(centroids, range(self.n_angle))])\n",
      "\n",
      "        r = Pool().map(compute_dist_per_proc, zip(np.array_split(X, n_splits, axis=0), \n",
      "                                                itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "        \n",
      "#         r = Parallel(n_jobs=16)(delayed(compute_dist_per_proc)(x,c) \n",
      "#                                 for x, c in zip(np.array_split(X, n_splits, axis=0), \n",
      "#                                                 itertools.repeat(centroid_all_rotations, n_splits)))\n",
      "        res = np.vstack(r)\n",
      "\n",
      "        labels = res[:,0]\n",
      "        rotations = res[:,1]\n",
      "\n",
      "        textonmap = labels.reshape(self.features.shape[:2])\n",
      "        textonmap[~mask] = -1\n",
      "    \n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    class args:\n",
      "        param_id = 'nissl324'\n",
      "        img_file = '../DavidData/RS155_x5/RS155_x5_0004.tif'\n",
      "        output_dir = '/oasis/scratch/csd181/yuncong/output'\n",
      "        params_dir = '/oasis/projects/nsf/csd181/yuncong/Brain/params'\n",
      "        \n",
      "    img, param, result_name, output_dir = load_stuff(args)\n",
      "    feature_extractor = FeatureExtractor(img, param)\n",
      "    feature_extractor.compute_features()\n",
      "    feature_extractor.compute_texton()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}