{
 "metadata": {
  "name": "",
  "signature": "sha256:6389b72384491c80fdda6c04fed9d213d0ed315ac432d9119929d4de351c9452"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import *\n",
      "\n",
      "if 'SSH_CONNECTION' in os.environ:\n",
      "    DATA_DIR = '/home/yuncong/DavidData'\n",
      "    REPO_DIR = '/home/yuncong/Brain'\n",
      "else:\n",
      "    DATA_DIR = '/home/yuncong/BrainLocal/DavidData_v4'\n",
      "    REPO_DIR = '/home/yuncong/BrainSaliencyDetection'\n",
      "\n",
      "dm = DataManager(DATA_DIR, REPO_DIR)\n",
      "\n",
      "class args:\n",
      "    stack_name = 'RS141'\n",
      "    resolution = 'x5'\n",
      "    slice_ind = 2\n",
      "    gabor_params_id = 'blueNisslWide'\n",
      "    segm_params_id = 'blueNissl'\n",
      "    vq_params_id = 'blueNissl'\n",
      "\n",
      "dm.set_image(args.stack_name, args.resolution, args.slice_ind)\n",
      "dm.set_gabor_params(gabor_params_id=args.gabor_params_id)\n",
      "dm.set_segmentation_params(segm_params_id=args.segm_params_id)\n",
      "dm.set_vq_params(vq_params_id=args.vq_params_id)\n",
      "\n",
      "from joblib import Parallel, delayed\n",
      "\n",
      "n_texton = int(dm.vq_params['n_texton'])\n",
      "\n",
      "texton_hists = dm.load_pipeline_result('texHist', 'npy')\n",
      "\n",
      "cropped_segmentation = dm.load_pipeline_result('cropSegmentation', 'npy')\n",
      "n_superpixels = len(unique(cropped_segmentation)) - 1\n",
      "cropped_mask = dm.load_pipeline_result('cropMask', 'npy')\n",
      "\n",
      "textonmap = dm.load_pipeline_result('texMap', 'npy')\n",
      "neighbors = dm.load_pipeline_result('neighbors', 'npy')\n",
      "\n",
      "cropped_image = dm.load_pipeline_result('cropImg', 'tif')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded /home/yuncong/DavidData/RS141/x5/0002/histResults/RS141_x5_0002_gabor-blueNisslWide-segm-blueNissl-vq-blueNissl_texHist.npy\n",
        "loaded /home/yuncong/DavidData/RS141/x5/0002/segmResults/RS141_x5_0002_gabor-blueNisslWide-segm-blueNissl_cropSegmentation.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loaded /home/yuncong/DavidData/RS141/x5/0002/filterResults/RS141_x5_0002_gabor-blueNisslWide_cropMask.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loaded /home/yuncong/DavidData/RS141/x5/0002/vqResults/RS141_x5_0002_gabor-blueNisslWide-vq-blueNissl_texMap.npy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loaded /home/yuncong/DavidData/RS141/x5/0002/segmResults/RS141_x5_0002_gabor-blueNisslWide-segm-blueNissl_neighbors.npy\n",
        "loaded /home/yuncong/DavidData/RS141/x5/0002/filterResults/RS141_x5_0002_gabor-blueNisslWide_cropImg.tif"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from grow_regions import grow_cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.spatial.distance import cdist\n",
      "\n",
      "overall_texton_hist = np.bincount(textonmap[cropped_mask].flat)\n",
      "overall_texton_hist_normalized = overall_texton_hist.astype(np.float) / overall_texton_hist.sum()\n",
      "D_sp_null = np.squeeze(cdist(texton_hists, [overall_texton_hist_normalized], chi2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "utilities.py:418: RuntimeWarning: invalid value encountered in divide\n",
        "  r = np.nansum((u-v)**2/(u+v))\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def circle_list_to_labeling_field(self, circle_list):\n",
      "#     label_circles = []\n",
      "#     for c in circle_list:\n",
      "#         label = np.where(np.all(self.colors == c.get_facecolor()[:3], axis=1))[0][0] - 1\n",
      "#         label_circles.append((int(c.center[0]), int(c.center[1]), c.radius, label))\n",
      "#     return label_circles\n",
      "\n",
      "def labeling_field_to_labelmap(labeling_field, size):\n",
      "    \n",
      "    labelmap = -1*np.ones(size, dtype=np.int)\n",
      "\n",
      "    for cx,cy,cradius,label in labeling_field:\n",
      "        for x in np.arange(cx-cradius, cx+cradius):\n",
      "            for y in np.arange(cy-cradius, cy+cradius):\n",
      "                if (cx-x)**2+(cy-y)**2 <= cradius**2:\n",
      "                    labelmap[int(y),int(x)] = label\n",
      "    return labelmap\n",
      "\n",
      "@timeit\n",
      "def label_superpixels(labelmap, segmentation):\n",
      "    labellist = -1*np.ones((n_superpixels,), dtype=np.int)\n",
      "    for sp in range(n_superpixels):\n",
      "        in_sp_labels = labelmap[segmentation==sp]\n",
      "        \n",
      "        counts = np.bincount(in_sp_labels+1)\n",
      "        dominant_label = counts.argmax() - 1\n",
      "        if dominant_label != -1:\n",
      "            labellist[sp] = dominant_label\n",
      "    return labellist\n",
      "        \n",
      "\n",
      "@timeit\n",
      "def generate_models(labellist, sp_texton_hist_normalized):\n",
      "    \n",
      "    models = []\n",
      "    for i in range(np.max(labellist)+1):\n",
      "        sps = np.where(labellist == i)[0]\n",
      "        print i, sps\n",
      "        model = {}\n",
      "        if len(sps) > 0:\n",
      "            texton_model = sp_texton_hist_normalized[sps, :].mean(axis=0)\n",
      "            model['texton_hist'] = texton_model\n",
      "            models.append(model)\n",
      "\n",
      "    n_models = len(models)\n",
      "    print n_models, 'models'\n",
      "    \n",
      "    return models\n",
      "\n",
      "\n",
      "def models_from_labeling(labeling):\n",
      "    \n",
      "    labelmap = labeling_field_to_labelmap(labeling['final_label_circles'], size=dm.image.shape)\n",
      "    \n",
      "    kernels = dm.load_pipeline_result('kernels', 'pkl')\n",
      "    max_kern_size = max([k.shape[0] for k in kernels])    \n",
      "    \n",
      "    cropped_labelmap = labelmap[max_kern_size/2:-max_kern_size/2, max_kern_size/2:-max_kern_size/2]\n",
      "    \n",
      "    labellist = label_superpixels(cropped_labelmap, cropped_segmentation)\n",
      "    models = generate_models(labellist, texton_hists)\n",
      "    \n",
      "    return models"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    models = dm.load_pipeline_result('models', 'pkl')\n",
      "except:\n",
      "    labeling = dm.load_labeling('anon_11032014025541')\n",
      "    models = models_from_labeling(labeling)\n",
      "    dm.save_pipeline_result(models, 'models', 'pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded /home/yuncong/DavidData/RS141/x5/RS141_x5_gabor-blueNisslWide-segm-blueNissl-vq-blueNissl_models.pkl\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights = np.ones((n_superpixels, ))/n_superpixels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_sp = Parallel(n_jobs=16)(delayed(grow_cluster)(s, neighbors, texton_hists, D_sp_null) for s in range(n_superpixels))\n",
      "model_sp = [texton_hists[c].mean(axis=0) for c in clusters]\n",
      "# clusters = [grow_cluster(s, neighbors, texton_hists, D_sp_null) for s in range(n_superpixels)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for t in range(n_models):\n",
      "    \n",
      "    print 'model %d' % (t)\n",
      " \n",
      "    sig_score = np.zeros((n_superpixels, ))\n",
      "    for i in range(n_superpixels):\n",
      "        cluster = cluster_sp[i]\n",
      "        model = model_sp[i]\n",
      "        D_diff_cluster = D_sp_null[cluster] - np.squeeze(cdist([model], texton_hists[cluster], chi2))\n",
      "        sig_score[i] = np.mean(weights[cluster] * D_diff_cluster)\n",
      " \n",
      "    # Pick the most significant superpixel\n",
      "    most_sig_sp = sig_score.argsort()[-1]\n",
      "    print \"most significant superpixel\", most_sig_sp\n",
      "\n",
      "    # models are the average of the distributions in the chosen superpixel's RE-cluster\n",
      "    curr_cluster =  cluster_sp[most_sig_sp]\n",
      "    D_sp_model = np.squeeze(cdist(model_sp[most_sig_sp], texton_hists, chi2))\n",
      "    D_sp_diff = D_sp_null - D_sp_model\n",
      "    \n",
      "#     matched, _ = grow_cluster_likelihood_ratio(seed_sp, model_texton, model_dir)\n",
      "#     matched = list(matched)\n",
      "\n",
      "    # Reduce the weights of superpixels in LR-cluster\n",
      "    weights[curr_cluster] = weights[curr_cluster] * np.exp(-5*(D_sp_null[curr_cluster] - D_sp_model[curr_cluster])**beta)\n",
      "    \n",
      "    weights = weights/weights.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}