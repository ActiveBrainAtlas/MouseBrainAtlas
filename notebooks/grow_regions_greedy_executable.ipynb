{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yuncong/Brain/pipeline_scripts')\n",
    "# import utilities2014\n",
    "# reload(utilities2014)\n",
    "from utilities2014 import *\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.cluster.hierarchy import average, fcluster, single, complete\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.measure import find_contours\n",
    "from skimage.util import img_as_float\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/home/yuncong/project/opencv-2.4.9/release/lib/python2.7/site-packages')\n",
    "import cv2\n",
    "\n",
    "from networkx import from_dict_of_lists, Graph, adjacency_matrix, dfs_postorder_nodes\n",
    "from networkx.algorithms import node_connected_component\n",
    "\n",
    "os.environ['GORDON_DATA_DIR'] = '/home/yuncong/project/DavidData2014tif/'\n",
    "os.environ['GORDON_REPO_DIR'] = '/home/yuncong/Brain'\n",
    "os.environ['GORDON_RESULT_DIR'] = '/home/yuncong/project/DavidData2014results/'\n",
    "os.environ['GORDON_LABELING_DIR'] = '/home/yuncong/project/DavidData2014labelings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_boundary_sps(clusters, neighbors, neighbor_graph, mode=None):\n",
    "    '''\n",
    "    Identify superpixels that are at the boundary of regions: surround set and frontier set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clusters : list of integer lists\n",
    "    neighbors : neighbor_list\n",
    "    neighbor_graph : \n",
    "    '''\n",
    "        \n",
    "    n_superpixels = len(clusters)\n",
    "    \n",
    "    surrounds_sps = []\n",
    "    frontiers_sps = []\n",
    "    \n",
    "    for cluster_ind, cluster in enumerate(clusters):\n",
    "        \n",
    "        surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
    "        surrounds = set([i for i in surrounds if any([n not in cluster for n in neighbors[i]])])\n",
    "\n",
    "        if len(surrounds) == 0:\n",
    "            surrounds_sps.append([])\n",
    "            frontiers_sps.append([])\n",
    "\n",
    "        else:\n",
    "            \n",
    "            if mode == 'surrounds' or mode == 'both':\n",
    "                surrounds_subgraph = neighbor_graph.subgraph(surrounds)\n",
    "                surrounds_traversal = list(dfs_postorder_nodes(surrounds_subgraph))\n",
    "                surrounds_sps.append(surrounds_traversal)\n",
    "            \n",
    "            if mode == 'frontiers' or mode == 'both':\n",
    "                frontiers = set.union(*[neighbors[c] for c in surrounds]) & set(cluster)\n",
    "                frontiers_subgraph = neighbor_graph.subgraph(frontiers)\n",
    "                frontiers_traversal = list(dfs_postorder_nodes(frontiers_subgraph))\n",
    "                frontiers_sps.append(frontiers_traversal)            \n",
    "    \n",
    "    if mode == 'surrounds':\n",
    "        return surrounds_sps\n",
    "    elif mode == 'frontiers':\n",
    "        return frontiers_sps\n",
    "    else:\n",
    "        return surrounds_sps, frontiers_sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cluster_score(cluster, texton_hists, neighbors):\n",
    "    \n",
    "    cluster_list = list(cluster)\n",
    "    cluster_avg = texton_hists[cluster_list].mean(axis=0)\n",
    "    \n",
    "    surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
    "    if len(surrounds) == 0: # single sp on background\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    surrounds_list = list(surrounds)\n",
    "    surround_dist = np.squeeze(cdist([cluster_avg], texton_hists[surrounds_list], chi2)).min()\n",
    "\n",
    "    surds = find_boundary_sps([cluster], neighbors=neighbors, neighbor_graph=neighbor_graph,\n",
    "                                    mode='surrounds')\n",
    "    \n",
    "    compactness = len(surds[0])**2/float(len(cluster))\n",
    "    compactness = .001 * np.maximum(compactness-40,0)**2\n",
    "    \n",
    "    size_prior = .1 * (1-np.exp(-.8*len(cluster)))\n",
    "    \n",
    "    score = surround_dist - compactness + size_prior\n",
    "    \n",
    "    interior_dist = np.nan\n",
    "    interior_pval = np.nan\n",
    "    surround_pval = np.nan\n",
    "    \n",
    "    return score, surround_dist, interior_dist, compactness, surround_pval, interior_pval, size_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbors_global = None\n",
    "\n",
    "def grow_cluster3(seed, texton_hists, neighbors=None, output=False, all_history=False):\n",
    "    \n",
    "    if neighbors is None:\n",
    "        neighbors = neighbors_global\n",
    "    \n",
    "    visited = set([])\n",
    "    curr_cluster = set([])\n",
    "        \n",
    "    candidate_scores = [0]\n",
    "    candidate_sps = [seed]\n",
    "\n",
    "    score_tuples = []\n",
    "    added_sps = []\n",
    "    \n",
    "    iter_ind = 0\n",
    "        \n",
    "    while len(candidate_sps) > 0:\n",
    "\n",
    "        best_ind = np.argmax(candidate_scores)\n",
    "        \n",
    "        heuristic = candidate_scores[best_ind]\n",
    "        sp = candidate_sps[best_ind]\n",
    "        \n",
    "        del candidate_scores[best_ind]\n",
    "        del candidate_sps[best_ind]\n",
    "        \n",
    "        if sp in curr_cluster:\n",
    "            continue\n",
    "                \n",
    "        iter_ind += 1\n",
    "        curr_cluster.add(sp)\n",
    "        added_sps.append(sp)\n",
    "        \n",
    "        tt = compute_cluster_score(curr_cluster, texton_hists=texton_hists, neighbors=neighbors)\n",
    "        tot, exterior, interior, compactness, surround_pval, interior_pval, size_prior = tt\n",
    "        if np.isnan(tot):\n",
    "            return [seed], -np.inf\n",
    "        score_tuples.append(np.r_[heuristic, tt])\n",
    "        \n",
    "        if output:\n",
    "            print 'iter', iter_ind, 'add', sp\n",
    "\n",
    "        visited.add(sp)\n",
    "        \n",
    "        candidate_sps = (set(candidate_sps) | (neighbors[sp] - set([-1])) | (visited - curr_cluster)) - curr_cluster\n",
    "        candidate_sps = list(candidate_sps)\n",
    "        \n",
    "#         f_avg = texton_freqs[list(curr_cluster)].sum(axis=0)\n",
    "#         candidate_scores = [chi2pval(f_avg, texton_freqs[i])[0] for i in candidate_sps]\n",
    "\n",
    "        h_avg = texton_hists[list(curr_cluster)].mean(axis=0)\n",
    "        candidate_scores = [-chi2(h_avg, texton_hists[i]) for i in candidate_sps]\n",
    "\n",
    "#         candidate_scores = [compute_cluster_score(curr_cluster | set([s])) for s in candidate_sps]\n",
    "                \n",
    "        if len(visited) > int(n_superpixels * 0.03):\n",
    "            break\n",
    "\n",
    "    score_tuples = np.array(score_tuples)\n",
    "    \n",
    "    min_size = 2\n",
    "    scores = score_tuples[:,1]\n",
    "    cutoff = np.argmax(scores[min_size:]) + min_size\n",
    "    \n",
    "    if output:\n",
    "        print 'cutoff', cutoff\n",
    "\n",
    "    final_cluster = added_sps[:cutoff]\n",
    "    final_score = scores[cutoff]\n",
    "    \n",
    "    if all_history:\n",
    "        return list(final_cluster), final_score, added_sps, score_tuples\n",
    "    else:\n",
    "        return list(final_cluster), final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_overlap(c1, c2):\n",
    "    return float(len(c1 & c2)) / min(len(c1),len(c2))\n",
    "\n",
    "def compute_overlap2(c1, c2):\n",
    "    return float(len(c1 & c2)) / len(c1 | c2)    \n",
    "\n",
    "def compute_overlap_partial(indices, sets, metric=1):\n",
    "    n_sets = len(sets)\n",
    "    \n",
    "    overlap_matrix = np.zeros((len(indices), n_sets))\n",
    "        \n",
    "    for ii, i in enumerate(indices):\n",
    "        for j in range(n_sets):\n",
    "            c1 = set(sets[i])\n",
    "            c2 = set(sets[j])\n",
    "            if len(c1) == 0 or len(c2) == 0:\n",
    "                overlap_matrix[ii, j] = 0\n",
    "            else:\n",
    "                if metric == 1:\n",
    "                    overlap_matrix[ii, j] = compute_overlap(c1, c2)\n",
    "                elif metric == 2:\n",
    "                    overlap_matrix[ii, j] = compute_overlap2(c1, c2)\n",
    "            \n",
    "    return overlap_matrix\n",
    "\n",
    "def set_pairwise_distances(sets, metric):\n",
    "\n",
    "    partial_overlap_mat = Parallel(n_jobs=16, max_nbytes=1e6)(delayed(compute_overlap_partial)(s, sets, metric=metric) \n",
    "                                        for s in np.array_split(range(len(sets)), 16))\n",
    "    overlap_matrix = np.vstack(partial_overlap_mat)\n",
    "    distance_matrix = 1 - overlap_matrix\n",
    "    \n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "def group_clusters(clusters=None, dist_thresh = 0.1, distance_matrix=None, metric=1):\n",
    "\n",
    "    if distance_matrix is None:\n",
    "        assert clusters is not None\n",
    "        distance_matrix = set_pairwise_distances(clusters, metric)\n",
    "        \n",
    "    lk = average(squareform(distance_matrix))\n",
    "#     lk = single(squareform(distance_matrix))\n",
    "\n",
    "    # T = fcluster(lk, 1.15, criterion='inconsistent')\n",
    "    T = fcluster(lk, dist_thresh, criterion='distance')\n",
    "    # Note that T starts from 1 not 0 !!!\n",
    "    \n",
    "    n_groups = len(set(T))    \n",
    "    groups = [None] * n_groups\n",
    "\n",
    "    for group_id in range(n_groups):\n",
    "        groups[group_id] = np.where(T == group_id+1)[0]\n",
    "        \n",
    "    return [g for g in groups if len(g) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spSet_to_edgeSet(cluster, n_superpixels, neighbors=None, fill_holes=False):\n",
    "        \n",
    "    if neighbors is None:\n",
    "        neighbors = neighbors_global\n",
    "                \n",
    "    holes = set([])\n",
    "    for t in [j for j in range(n_superpixels) if j not in cluster]:\n",
    "        if np.sum([i in cluster for i in neighbors[t]]) >= 3:\n",
    "            holes.add(t)\n",
    "#         print ci, holes\n",
    "    cluster = set(cluster) | holes\n",
    "\n",
    "    surrounds = set([i for i in set.union(*[neighbors[c] for c in cluster]) if i not in cluster and i != -1])\n",
    "    surrounds = set([i for i in surrounds if any([n not in cluster for n in neighbors[i]])])\n",
    "\n",
    "    frontiers = set.union(*[neighbors[c] for c in surrounds]) & set(cluster)\n",
    "\n",
    "    region_edges = []\n",
    "    for s in surrounds:\n",
    "        for f in neighbors[s] & set(frontiers):\n",
    "            region_edges.append((s, f))\n",
    "\n",
    "    for i in cluster:\n",
    "        if -1 in neighbors[i]:\n",
    "            region_edges.append((-1, i))\n",
    "    \n",
    "    return region_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        description='Generate region proposals and detect closedRegion landmarks')\n",
    "\n",
    "    parser.add_argument(\"stack_name\", type=str, help=\"stack name\")\n",
    "    parser.add_argument(\"slice_ind\", type=int, help=\"slice index\")\n",
    "    parser.add_argument(\"-g\", \"--gabor_params_id\", type=str, help=\"gabor filter parameters id (default: %(default)s)\", default='blueNisslWide')\n",
    "    parser.add_argument(\"-s\", \"--segm_params_id\", type=str, help=\"segmentation parameters id (default: %(default)s)\", default='blueNisslRegular')\n",
    "    parser.add_argument(\"-v\", \"--vq_params_id\", type=str, help=\"vector quantization parameters id (default: %(default)s)\", default='blueNissl')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    section_id = int(args.slice_ind)\n",
    "    \n",
    "    dm = DataManager(generate_hierarchy=False, stack=args.stack_name, resol='x5', section=section_id,\n",
    "                    gabor_params_id=args.gabor_params_id,\n",
    "                    segm_params_id=args.segm_params_id,\n",
    "                    vq_params_id=args.vq_params_id)\n",
    "    dm._load_image()\n",
    "\n",
    "    texton_hists = dm.load_pipeline_result('texHist', 'npy')\n",
    "    segmentation = dm.load_pipeline_result('segmentation', 'npy')\n",
    "    n_superpixels = len(np.unique(segmentation)) - 1\n",
    "    textonmap = dm.load_pipeline_result('texMap', 'npy')\n",
    "    n_texton = len(np.unique(textonmap)) - 1\n",
    "\n",
    "    neighbors = dm.load_pipeline_result('neighbors', 'npy')\n",
    "    neighbors_global = neighbors\n",
    "\n",
    "    sp_properties = dm.load_pipeline_result('spProps', 'npy')\n",
    "    # each item is (center_y, center_x, area, mean_intensity, ymin, xmin, ymax, xmax)\n",
    "    segmentation_vis = dm.load_pipeline_result('segmentationWithoutText', 'jpg')\n",
    "\n",
    "    try:\n",
    "        sp_sp_dists = dm.load_pipeline_result('texHistPairwiseDist', 'npy')\n",
    "    #     raise\n",
    "    except:\n",
    "        def f(a):\n",
    "            sp_dists = cdist(a, texton_hists, metric=chi2)\n",
    "    #         sp_dists = cdist(a, texton_hists, metric=js)\n",
    "            return sp_dists\n",
    "\n",
    "        sp_dists = Parallel(n_jobs=16)(delayed(f)(s) for s in np.array_split(texton_hists, 16))\n",
    "        sp_sp_dists = np.vstack(sp_dists)\n",
    "\n",
    "        dm.save_pipeline_result(sp_sp_dists, 'texHistPairwiseDist', 'npy')\n",
    "\n",
    "    center_dists = pdist(sp_properties[:, :2])\n",
    "    center_dist_matrix = squareform(center_dists)\n",
    "\n",
    "    neighbors_dict = dict(zip(np.arange(n_superpixels), [list(i) for i in neighbors]))\n",
    "    neighbor_graph = from_dict_of_lists(neighbors_dict)\n",
    "\n",
    "\n",
    "    try:\n",
    "        expansion_clusters_tuples = dm.load_pipeline_result('clusters', 'pkl')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "\n",
    "        b = time.time()\n",
    "\n",
    "        # Observation: if `neighbors` is passed as argument, execution takes 4.5 times than if `neighbors` is global\n",
    "        # Reason: `neighbors` is a list of sets, not mem-mappable. To see the error, run the lines below:\n",
    "        # from joblib import load, dump\n",
    "        # _ = dump(neighbors, '/tmp/tmp')\n",
    "        # large_memmap = load('/tmp/tmp', mmap_mode='r+')\n",
    "        # Solution: make `neighbors` global\n",
    "        # p.s. if CPU utilizations of many processes are low, it means IO is taking much of the time\n",
    "        # p.s. specify max_nbytes argument to Parallel enables memmap for shared variables, but using it results in \n",
    "        # error `zero-dimensional array concatenates...`\n",
    "        expansion_clusters_tuples = Parallel(n_jobs=16)(delayed(grow_cluster3)(s, texton_hists=texton_hists)\n",
    "                                                                    for s in range(n_superpixels))\n",
    "\n",
    "        print 'grow cluster', time.time() - b\n",
    "\n",
    "        dm.save_pipeline_result(expansion_clusters_tuples, 'clusters', 'pkl')\n",
    "\n",
    "    expansion_clusters, expansion_cluster_scores = zip(*expansion_clusters_tuples)\n",
    "    expansion_cluster_scores = np.array(expansion_cluster_scores)\n",
    "\n",
    "\n",
    "    try:\n",
    "        D = dm.load_pipeline_result('clusterPairwiseDist', 'npy')\n",
    "        raise\n",
    "    except:\n",
    "\n",
    "        b = time.time()\n",
    "\n",
    "        D = set_pairwise_distances(expansion_clusters, metric=2)\n",
    "        dm.save_pipeline_result(D, 'clusterPairwiseDist', 'npy')\n",
    "\n",
    "        print 'compute pairwise', time.time() - b\n",
    "\n",
    "\n",
    "    try:\n",
    "        expansion_cluster_groups = dm.load_pipeline_result('clusterGroups', 'pkl')\n",
    "        raise\n",
    "    except:\n",
    "\n",
    "        b = time.time()\n",
    "\n",
    "        expansion_cluster_groups = group_clusters(expansion_clusters, dist_thresh=.8, distance_matrix=D)\n",
    "        dm.save_pipeline_result(expansion_cluster_groups, 'clusterGroups', 'pkl')\n",
    "\n",
    "        print 'group clusters', time.time() - b\n",
    "\n",
    "\n",
    "    print len(expansion_cluster_groups), 'expansion cluster groups'\n",
    "    expansion_cluster_group_sizes = np.array(map(len, expansion_cluster_groups))\n",
    "\n",
    "\n",
    "    big_group_indices = np.where(expansion_cluster_group_sizes > 5)[0]\n",
    "    n_big_groups = len(big_group_indices)\n",
    "    print n_big_groups, 'big cluster groups'\n",
    "    big_groups = [expansion_cluster_groups[i] for i in big_group_indices]\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    representative_clusters = []\n",
    "    representative_cluster_scores = []\n",
    "    representative_cluster_indices = []\n",
    "\n",
    "    big_groups_valid = []\n",
    "\n",
    "    for g in big_groups:\n",
    "        for i in np.argsort(expansion_cluster_scores[g])[::-1]:\n",
    "            c = expansion_clusters[g[i]]\n",
    "            sc = expansion_cluster_scores[g[i]]\n",
    "            if len(c) > n_superpixels * .004:\n",
    "                representative_clusters.append(c)\n",
    "                representative_cluster_indices.append(g[i])\n",
    "                representative_cluster_scores.append(sc)\n",
    "                big_groups_valid.append(g)\n",
    "                break\n",
    "\n",
    "    print len(representative_clusters), 'representative clusters'\n",
    "\n",
    "    representative_cluster_scores_sorted, representative_clusters_sorted_by_score, \\\n",
    "    representative_cluster_indices_sorted_by_score, \\\n",
    "    big_groups_sorted_by_score = map(list, zip(*sorted(zip(representative_cluster_scores, \n",
    "                                                            representative_clusters,\n",
    "                                                            representative_cluster_indices,\n",
    "                                                            big_groups_valid), reverse=True)))\n",
    "\n",
    "    representative_clusters = zip(representative_cluster_scores_sorted, representative_clusters_sorted_by_score, \n",
    "                   representative_cluster_indices_sorted_by_score, \n",
    "                   big_groups_sorted_by_score)\n",
    "\n",
    "    dm.save_pipeline_result(representative_clusters, 'representativeClusters', 'pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
