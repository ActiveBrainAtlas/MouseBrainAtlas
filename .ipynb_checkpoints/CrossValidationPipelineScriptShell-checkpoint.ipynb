{
 "metadata": {
  "name": "",
  "signature": "sha256:bb832f7a1ee7cbb89ac5940330d213d5799ea037cb4e4542b90ac21ef0803be0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import random, itertools, sys, os\n",
      "from multiprocessing import Pool\n",
      "import json\n",
      "\n",
      "from skimage.segmentation import slic, mark_boundaries\n",
      "from skimage.measure import regionprops\n",
      "from skimage.util import img_as_ubyte\n",
      "from skimage.color import hsv2rgb, label2rgb, gray2rgb\n",
      "from skimage.morphology import disk\n",
      "from skimage.filter.rank import gradient\n",
      "from skimage.filter import gabor_kernel\n",
      "from skimage.transform import rescale, resize\n",
      "\n",
      "from scipy.ndimage import gaussian_filter, measurements\n",
      "from scipy.sparse import coo_matrix\n",
      "from scipy.spatial.distance import pdist, squareform, euclidean, cdist\n",
      "from scipy.signal import fftconvolve\n",
      "\n",
      "from IPython.display import FileLink, Image, FileLinks\n",
      "\n",
      "from utilities import *\n",
      "import manager_utilities\n",
      "\n",
      "import glob, re, os, sys, subprocess, argparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parser = argparse.ArgumentParser()\n",
      "# parser.add_argument(\"param_file\", type=str, help=\"parameter file name\")\n",
      "# parser.add_argument(\"img_file\", type=str, help=\"path to image file\")\n",
      "# # parser.add_argument(\"-of\", \"--output_feature\", action='store_true', help=\"whether to output feature array\")\n",
      "# # parser.add_argument(\"-ot\", \"--output_textonmap\", action='store_true', help=\"whether to output textonmap file\")\n",
      "# # parser.add_argument(\"-od\", \"--output_dirmap\", action='store_true', help=\"whether to output dirmap file\")\n",
      "# # parser.add_argument(\"-os\", \"--output_segmentation\", action='store_true', help=\"whether to output superpixel segmentation file\")\n",
      "# parser.add_argument(\"-c\", \"--cache_dir\", default='scratch', help=\"directory to store outputs\")\n",
      "# args = parser.parse_args()\n",
      "\n",
      "img_dir = '../data/dataset0'\n",
      "img_name_full = 'PMD1305_244_reduce4_region0.tif'\n",
      "\n",
      "img_path = os.path.join(img_dir, img_name_full)\n",
      "img_name, ext = os.path.splitext(img_name_full)\n",
      "\n",
      "param_id = 91\n",
      "param_file = '../params/param%d.json'%param_id\n",
      "\n",
      "class args:\n",
      "    param_file = param_file\n",
      "    img_file = img_path\n",
      "    output_feature = True\n",
      "    output_textonmap = True\n",
      "    output_dirmap = True\n",
      "    output_segmentation = True\n",
      "    cache_dir = 'scratch'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_array(suffix):\n",
      "    return manager_utilities.load_array(suffix, img_name, \n",
      "                                 params['param_id'], args.cache_dir)\n",
      "\n",
      "def save_array(arr, suffix):\n",
      "    manager_utilities.save_array(arr, suffix, img_name, \n",
      "                                 params['param_id'], args.cache_dir)\n",
      "        \n",
      "def save_img(img, suffix):\n",
      "    manager_utilities.save_img(img, suffix, img_name, params['param_id'], \n",
      "                               args.cache_dir)\n",
      "\n",
      "def get_img_filename(suffix):\n",
      "    return manager_utilities.get_img_filename(suffix, img_name,  args.cache_dir,\n",
      "                                       params['param_id'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = json.load(open(args.param_file))\n",
      "print args.param_file\n",
      "p, ext = os.path.splitext(args.img_file)\n",
      "print args.img_file\n",
      "img_dir, img_name = os.path.split(p)\n",
      "img = cv2.imread(os.path.join(args.img_file), 0)\n",
      "im_height, im_width = img.shape[:2]\n",
      "\n",
      "output_dir = os.path.join(args.cache_dir, img_name)\n",
      "if not os.path.exists(output_dir):\n",
      "    os.makedirs(output_dir)\n",
      "\n",
      "print '=== finding foreground mask ==='\n",
      "mask = foreground_mask(rescale(img, .5**3), min_size=100)\n",
      "mask = resize(mask, img.shape) > .5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "../params/param91.json\n",
        "../data/dataset0/PMD1305_244_reduce4_region0.tif\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'NoneType' object has no attribute 'shape'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-7965de279412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mim_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta_interval = params['theta_interval'] #10\n",
      "n_angle = 180/theta_interval\n",
      "n_freq = params['n_freq']\n",
      "freq_max = params['max_freq'] #1./5.\n",
      "frequencies = freq_max/2**np.arange(n_freq)\n",
      "\n",
      "kernels = [gabor_kernel(f, theta=t, bandwidth=1.) for f in frequencies \n",
      "          for t in np.arange(0, np.pi, np.deg2rad(theta_interval))]\n",
      "kernels = map(np.real, kernels)\n",
      "n_kernel = len(kernels)\n",
      "\n",
      "print '=== filter using Gabor filters ==='\n",
      "print 'num. of kernels: %d' % (n_kernel)\n",
      "print 'frequencies:', frequencies\n",
      "print 'wavelength (pixels):', 1/frequencies\n",
      "\n",
      "max_kern_size = np.max([kern.shape[0] for kern in kernels])\n",
      "print 'max kernel matrix size:', max_kern_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== filter using Gabor filters ===\n",
        "num. of kernels: 72\n",
        "frequencies: [ 0.2    0.1    0.05   0.025]\n",
        "wavelength (pixels): [  5.  10.  20.  40.]\n",
        "max kernel matrix size: 137\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    features = load_array('features')\n",
      "except IOError:\n",
      "    def convolve_per_proc(i):\n",
      "        return fftconvolve(img, kernels[i], 'same').astype(np.half)\n",
      "\n",
      "    pool = Pool(processes=8)\n",
      "    filtered = pool.map(convolve_per_proc, range(n_kernel))\n",
      "\n",
      "    features = np.empty((im_height, im_width, n_kernel), dtype=np.half)\n",
      "    for i in range(n_kernel):\n",
      "        features[...,i] = filtered[i]\n",
      "\n",
      "    del filtered\n",
      "    \n",
      "    save_array(features, 'features')\n",
      "\n",
      "n_feature = features.shape[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "load scratch/PMD1305_170.reduce2.region2/PMD1305_170.reduce2.region2_param89_features.npy\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'crop border where filters show border effects'\n",
      "features = features[max_kern_size:-max_kern_size, max_kern_size:-max_kern_size, :]\n",
      "img = img[max_kern_size:-max_kern_size, max_kern_size:-max_kern_size]\n",
      "mask = mask[max_kern_size:-max_kern_size, max_kern_size:-max_kern_size]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crop border where filters show border effects\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '=== compute rotation-invariant texton map using K-Means ==='\n",
      "\n",
      "n_texton = params['n_texton']\n",
      "\n",
      "try: \n",
      "    textonmap = load_array('textonmap')\n",
      "except IOError:\n",
      "    \n",
      "    X = features.reshape(-1, n_feature)\n",
      "    n_data = X.shape[0]\n",
      "    n_splits = 1000\n",
      "    n_sample = 10000\n",
      "    data = random.sample(X, n_sample)\n",
      "    centroids = data[:n_texton]\n",
      "    \n",
      "    n_iter = 5\n",
      "\n",
      "    def compute_dist_per_proc((X_partial, c_all_rot)):\n",
      "        D = cdist(X_partial, c_all_rot, 'sqeuclidean')\n",
      "        ci, ri = np.unravel_index(D.argmin(axis=1), (n_texton, n_angle))\n",
      "        return np.c_[ci, ri]\n",
      "    \n",
      "    pool = Pool(processes=16)\n",
      "\n",
      "    for iteration in range(n_iter):\n",
      "        print 'iteration', iteration\n",
      "        centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, n_freq), i)) \n",
      "                                for c,i in itertools.product(centroids, range(n_angle))])\n",
      "\n",
      "        res = np.vstack(pool.map(compute_dist_per_proc, \n",
      "                                 zip(np.array_split(data, n_splits, axis=0), \n",
      "                                     itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "\n",
      "        labels = res[:,0]\n",
      "        rotations = res[:,1]\n",
      "\n",
      "        centroids_new = np.zeros((n_texton, n_feature))\n",
      "        for d, l, r in itertools.izip(data, labels, rotations):\n",
      "            rot = np.concatenate(np.roll(np.split(d, n_freq), i))\n",
      "            centroids_new[l] += rot\n",
      "\n",
      "        counts = np.bincount(labels)\n",
      "        centroids_new /= counts[:, np.newaxis]\n",
      "        print np.sqrt(np.sum((centroids - centroids_new)**2, axis=1)).mean()\n",
      "\n",
      "        centroids = centroids_new\n",
      "\n",
      "    print 'kmeans completes'\n",
      "    centroid_all_rotations = np.vstack([np.concatenate(np.roll(np.split(c, n_freq), i)) \n",
      "                                for c,i in itertools.product(centroids, range(n_angle))])\n",
      "\n",
      "    res = np.vstack(pool.map(compute_dist_per_proc, \n",
      "                             zip(np.array_split(X, n_splits, axis=0), itertools.repeat(centroid_all_rotations, n_splits))))\n",
      "    labels = res[:,0]\n",
      "    rotations = res[:,1]\n",
      "\n",
      "    pool.close()\n",
      "    pool.join()\n",
      "    del pool\n",
      "\n",
      "    textonmap = labels.reshape(features.shape[:2])\n",
      "    textonmap[~mask] = -1\n",
      "    \n",
      "    save_array(textonmap, 'textonmap')\n",
      "    \n",
      "    textonmap_rgb = label2rgb(textonmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "    save_img(textonmap_rgb, 'textonmap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute rotation-invariant texton map using K-Means ===\n",
        "load scratch/PMD1305_170.reduce2.region2/PMD1305_170.reduce2.region2_param89_textonmap.npy\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '=== compute directionality map ==='\n",
      "\n",
      "try:\n",
      "    dirmap = load_array('dirmap')\n",
      "except IOError:\n",
      "    f = np.reshape(features, (features.shape[0], features.shape[1], n_freq, n_angle))\n",
      "    dirmap = np.argmax(np.max(f, axis=2), axis=-1)\n",
      "    dirmap[~mask] = -1\n",
      "    print 'dirmap computed'\n",
      "\n",
      "    save_array(dirmap, 'dirmap')\n",
      "    \n",
      "    dirmap_rgb = label2rgb(dirmap, image=None, colors=None, alpha=0.3, image_alpha=1)\n",
      "    save_img(dirmap_rgb, 'dirmap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute directionality map ===\n",
        "load scratch/PMD1305_170.reduce2.region2/PMD1305_170.reduce2.region2_param89_dirmap.npy\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '=== over-segment the image into superpixels based on color information ==='\n",
      "\n",
      "img_rgb = gray2rgb(img)\n",
      "\n",
      "try:\n",
      "    segmentation = load_array('segmentation')\n",
      "    \n",
      "except IOError:\n",
      "    segmentation = slic(img_rgb, n_segments=params['n_superpixels'], max_iter=10, \n",
      "                        compactness=params['slic_compactness'], \n",
      "                        sigma=params['slic_sigma'], enforce_connectivity=True)\n",
      "    print 'segmentation computed'\n",
      "    \n",
      "    save_array(segmentation, 'segmentation')\n",
      "\n",
      "n_superpixels = len(np.unique(segmentation))\n",
      "\n",
      "sp_props = regionprops(segmentation+1, intensity_image=img, cache=True)\n",
      "sp_centroids = np.array([s.centroid for s in sp_props])\n",
      "sp_areas = np.array([s.area for s in sp_props])\n",
      "# sp_wcentroids = np.array([s.weighted_centroid for s in sp_props])\n",
      "sp_centroid_dist = pdist(sp_centroids)\n",
      "sp_centroid_dist_matrix = squareform(sp_centroid_dist)\n",
      "sp_mean_intensity = np.array([s.mean_intensity for s in sp_props])\n",
      "\n",
      "img_superpixelized = mark_boundaries(img_rgb, segmentation)\n",
      "# sptext = img_as_ubyte(img_superpixelized)\n",
      "# for s in range(n_superpixels):\n",
      "#     sptext = cv2.putText(sptext, str(s), \n",
      "#                       tuple(np.floor(sp_centroids[s][::-1]).astype(np.int)), \n",
      "#                       cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
      "#                       .5, ((255,0,255)), 1)\n",
      "save_img(img_superpixelized, 'segmentation')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== over-segment the image into superpixels based on color information ===\n",
        "load scratch/PMD1305_170.reduce2.region2/PMD1305_170.reduce2.region2_param89_segmentation.npy\n",
        "segmentation saved to scratch/PMD1305_170.reduce2.region2/PMD1305_170.reduce2.region2_param89_segmentation.tif"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/skimage/util/dtype.py:107: UserWarning: Possible precision loss when converting from float64 to uint8\n",
        "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "superpixels_bg_count = np.array([(~mask[segmentation==i]).count_nonzero() for i in range(n_superpixels)])\n",
      "bg_superpixels = np.nonzero((superpixels_bg_count/sp_areas) > 0.7)[0]\n",
      "print '%d background superpixels'%len(bg_superpixels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4 background superpixels\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '=== compute texton and directionality histogram of each superpixel ==='\n",
      "\n",
      "sample_interval = 1\n",
      "gridy, gridx = np.mgrid[:img.shape[0]:sample_interval, :img.shape[1]:sample_interval]\n",
      "\n",
      "all_seg = segmentation[gridy.ravel(), gridx.ravel()]\n",
      "\n",
      "try:\n",
      "    sp_texton_hist_normalized = load_array('sp_texton_hist_normalized')\n",
      "except IOError:\n",
      "    all_texton = textonmap[gridy.ravel(), gridx.ravel()]\n",
      "    sp_texton_hist = np.array([np.bincount(all_texton[(all_seg == s)&(all_texton != -1)], minlength=n_texton) \n",
      "                     for s in range(n_superpixels)])\n",
      "    row_sums = sp_texton_hist.sum(axis=1)\n",
      "    sp_texton_hist_normalized = sp_texton_hist.astype(np.float) / row_sums[:, np.newaxis]\n",
      "    save_array(sp_texton_hist_normalized, 'sp_texton_hist_normalized')\n",
      "    \n",
      "try:\n",
      "    sp_dir_hist_normalized = load_array('sp_dir_hist_normalized')\n",
      "except IOError:    \n",
      "    all_dir = dirmap[gridy.ravel(), gridx.ravel()]\n",
      "    sp_dir_hist = np.array([np.bincount(all_dir[(all_seg == s)&(all_dir != -1)], minlength=n_angle) \n",
      "                     for s in range(n_superpixels)])\n",
      "    row_sums = sp_dir_hist.sum(axis=1)\n",
      "    sp_dir_hist_normalized = sp_dir_hist.astype(np.float) / row_sums[:, np.newaxis]\n",
      "    save_array(sp_dir_hist_normalized, 'sp_dir_hist_normalized')\n",
      "\n",
      "    \n",
      "def chi2(u,v):\n",
      "    return np.sum(np.where(u+v!=0, (u-v)**2/(u+v), 0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:15: RuntimeWarning: invalid value encountered in divide\n",
        "-c:24: RuntimeWarning: invalid value encountered in divide\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute texton and directionality histogram of each superpixel ===\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '=== compute significance of each superpixel ==='\n",
      "\n",
      "overall_texton_hist = np.bincount(textonmap[mask].flat)\n",
      "overall_texton_hist_normalized = overall_texton_hist.astype(np.float) / overall_texton_hist.sum()\n",
      "individual_texton_saliency_score = np.array([chi2(sp_hist, overall_texton_hist_normalized) for sp_hist in sp_texton_hist_normalized])\n",
      "\n",
      "texton_saliency_score = np.zeros((n_superpixels,))\n",
      "for i, sp_hist in enumerate(sp_texton_hist_normalized):\n",
      "    if i not in bg_superpixels:\n",
      "        texton_saliency_score[i] = individual_texton_saliency_score[i]\n",
      "        \n",
      "texton_saliency_map = texton_saliency_score[segmentation]\n",
      "\n",
      "save_img(gray2rgb(texton_saliency_map), 'texton_saliencymap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "=== compute significance of each superpixel ===\n",
        "texton_saliencymap saved to scratch/PMD1305_170.reduce2.region2/PMD1305_170.reduce2.region2_param89_texton_saliencymap.tif"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "overall_dir_hist = np.bincount(dirmap[mask].flat)\n",
      "overall_dir_hist_normalized = overall_dir_hist.astype(np.float) / overall_dir_hist.sum()\n",
      "individual_dir_saliency_score = np.array([chi2(sp_hist, overall_dir_hist_normalized) for sp_hist in sp_dir_hist_normalized])\n",
      "\n",
      "dir_saliency_score = np.zeros((n_superpixels,))\n",
      "for i, sp_hist in enumerate(sp_dir_hist_normalized):\n",
      "    if i not in bg_superpixels:\n",
      "        dir_saliency_score[i] = individual_dir_saliency_score[i]\n",
      "\n",
      "dir_saliency_map = dir_saliency_score[segmentation]\n",
      "save_img(gray2rgb(dir_saliency_map), 'dir_saliencymap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dir_saliencymap saved to scratch/PMD1305_170.reduce2.region2/PMD1305_170.reduce2.region2_param89_dir_saliencymap.tif\n"
       ]
      }
     ],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}