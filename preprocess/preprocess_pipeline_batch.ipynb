{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD642/MD642_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD642/MD642_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD642/MD642_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from distributed_utilities import *\n",
    "from preprocess_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_fmt = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://mousebrainatlas-rawdata/CSHL_data/MD652 /shared/CSHL_data/MD652 --exclude \"*\" --include \"*.png\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "4.55 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt && mkdir -p /shared/CSHL_data_processed/MD652\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_sorted_filenames.txt /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt\n",
      "rm: cannot remove '/shared/CSHL_data_processed/MD652/MD652_elastix_output': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.51 seconds.\n",
      "16 nodes requested, 16 nodes available...Continuing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 181.380911112 seconds\n",
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD652/MD652_elastix_output s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_elastix_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "22.14 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://mousebrainatlas-rawdata/CSHL_data/MD653 /shared/CSHL_data/MD653 --exclude \"*\" --include \"*.png\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "4.72 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD653/MD653_sorted_filenames.txt && mkdir -p /shared/CSHL_data_processed/MD653\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD653/MD653_sorted_filenames.txt /shared/CSHL_data_processed/MD653/MD653_sorted_filenames.txt\n",
      "rm: cannot remove '/shared/CSHL_data_processed/MD653/MD653_elastix_output': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.45 seconds.\n",
      "16 nodes requested, 16 nodes available...Continuing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 397.660309076 seconds\n",
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD653/MD653_elastix_output s3://mousebrainatlas-data/CSHL_data_processed/MD653/MD653_elastix_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "21.23 seconds.\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD652', 'MD653']:\n",
    "    transfer_data_synced(os.path.join('CSHL_data', stack), \n",
    "                     from_hostname='s3raw', to_hostname='ec2', is_dir=True, include_only='*.'+tb_fmt)\n",
    "\n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_sorted_filenames.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    \n",
    "    ########### Align ###########\n",
    "    \n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    script = os.path.join(REPO_DIR, 'preprocess', 'align_consecutive_v2.py')\n",
    "    input_dir = os.path.join(RAW_DATA_DIR, stack)\n",
    "    output_dir = os.path.join(DATA_DIR, stack, stack + '_elastix_output')\n",
    "    \n",
    "    ! rm -r $output_dir\n",
    "    \n",
    "    t = time.time()\n",
    "    print 'Align...'\n",
    "\n",
    "    run_distributed(\"%(script)s %(stack)s %(input_dir)s %(output_dir)s \\'%%(kwargs_str)s\\' %(fmt)s\" % \\\n",
    "                    {'script': script,\n",
    "                    'stack': stack,\n",
    "                    'input_dir': input_dir,\n",
    "                    'output_dir': output_dir,\n",
    "                    'fmt': tb_fmt},\n",
    "                    kwargs_list=[{'prev_fn': valid_filenames[i-1], 'curr_fn': valid_filenames[i]} for i in range(1, len(valid_filenames))],\n",
    "                    argument_type='list',\n",
    "                   cluster_size=16,\n",
    "                   jobs_per_node=16) # \n",
    "\n",
    "    wait_qsub_complete()\n",
    "\n",
    "    print 'done in', time.time() - t, 'seconds' # 252 seconds\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_elastix_output'), \n",
    "                     from_hostname='ec2', to_hostname='s3', is_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- download `elastix_output/` to local machine, edit consecutive transforms in local GUI, generate `custom_transforms/` to S3, upload to S3.\n",
    "- determine anchor image, upload `anchor.txt` to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for stack in ['MD652', 'MD653']:\n",
    "    \n",
    "    ########### Compose #############\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_custom_transforms'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=True)\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_anchor.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    \n",
    "    anchor_fn = DataManager.load_anchor_filename(stack=stack)\n",
    "    \n",
    "    script = os.path.join(REPO_DIR, 'preprocess', 'compose_transform_thumbnail_v2.py')\n",
    "    input_dir = os.path.join(DATA_DIR, stack, stack + '_elastix_output')\n",
    "    output_fn = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_%(anchor_fn)s.pkl' % \\\n",
    "                                                    dict(stack=stack, anchor_fn=anchor_fn))\n",
    "\n",
    "    ! rm -f $output_fn\n",
    "    \n",
    "    t = time.time()\n",
    "    print 'Composing transform...'\n",
    "\n",
    "    run_distributed(\"%(script)s %(stack)s %(input_dir)s \\'%%(kwargs_str)s\\' %(anchor_idx)d %(output_fn)s\" % \\\n",
    "                {'stack': stack,\n",
    "                'script': script,\n",
    "                'input_dir': input_dir,\n",
    "                'anchor_idx': valid_filenames.index(anchor_fn),\n",
    "                'output_fn': output_fn},\n",
    "                kwargs_list=[{'filenames': valid_filenames}],\n",
    "                argument_type='list',\n",
    "                   cluster_size=1,\n",
    "                   exclude_nodes=[33])\n",
    "\n",
    "    linked_name = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_anchor.pkl' % dict(stack=stack))\n",
    "    execute_command('rm -f ' + linked_name)\n",
    "    execute_command('ln -s ' + output_fn + ' ' + linked_name)\n",
    "\n",
    "    print 'done in', time.time() - t, 'seconds'\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack), \n",
    "                     from_hostname='ec2', to_hostname='s3', is_dir=True, include_only='*.pkl')\n",
    "    \n",
    "    ########## Warp #############\n",
    "    \n",
    "    if stack in all_nissl_stacks:\n",
    "        pad_bg_color = 'white'\n",
    "    else:\n",
    "        pad_bg_color = 'auto'\n",
    "        \n",
    "    script = os.path.join(REPO_DIR, 'preprocess', 'warp_crop_IM_v2.py')\n",
    "    input_dir = os.path.join(RAW_DATA_DIR, stack)\n",
    "    out_dir = os.path.join(DATA_DIR, stack, stack + '_thumbnail_alignedTo_' + anchor_fn)\n",
    "    \n",
    "    ! rm -rf $out_dir\n",
    "    \n",
    "    t = time.time()\n",
    "    print 'Warping...'\n",
    "\n",
    "    transforms_filename = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_%(anchor_fn)s.pkl' % \\\n",
    "                                       dict(stack=stack, anchor_fn=anchor_fn))\n",
    "    transforms_to_anchor = pickle.load(open(transforms_filename, 'r'))\n",
    "\n",
    "    if pad_bg_color == 'auto':\n",
    "        run_distributed('%(script)s %(stack)s %(input_dir)s %(out_dir)s %%(transform)s %%(filename)s %%(output_fn)s thumbnail 0 0 2000 1500 %%(pad_bg_color)s' % \\\n",
    "                        {'script': script,\n",
    "                        'stack': stack,\n",
    "                        'input_dir': input_dir,\n",
    "                        'out_dir': out_dir\n",
    "                        },\n",
    "                        kwargs_list=[{'transform': ','.join(map(str, transforms_to_anchor[fn].flatten())),\n",
    "                                    'filename': fn + '.' + tb_fmt,\n",
    "                                    'output_fn': fn + '_thumbnail_alignedTo_' + anchor_fn + '.tif',\n",
    "                                    'pad_bg_color': 'black' if fn.split('-')[1][0] == 'F' else 'white'}\n",
    "                                    for fn in valid_filenames],\n",
    "                        exclude_nodes=[33],\n",
    "                        argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=8)\n",
    "    else:\n",
    "        run_distributed('%(script)s %(stack)s %(input_dir)s %(out_dir)s %%(transform)s %%(filename)s %%(output_fn)s thumbnail 0 0 2000 1500 %(pad_bg_color)s' % \\\n",
    "                        {'script': script,\n",
    "                        'stack': stack,\n",
    "                        'input_dir': input_dir,\n",
    "                        'out_dir': out_dir,\n",
    "                        'pad_bg_color': pad_bg_color},\n",
    "                        kwargs_list=[{'transform': ','.join(map(str, transforms_to_anchor[fn].flatten())),\n",
    "                                    'filename': fn + '.' + tb_fmt,\n",
    "                                    'output_fn': fn + '_thumbnail_alignedTo_' + anchor_fn + '.tif'}\n",
    "                                    for fn in valid_filenames],\n",
    "                        exclude_nodes=[33],\n",
    "                        argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=8)\n",
    "        \n",
    "    wait_qsub_complete()\n",
    "\n",
    "    print 'done in', time.time() - t, 'seconds'\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_thumbnail_alignedTo_' + anchor_fn),\n",
    "                    from_hostname='ec2',\n",
    "                    to_hostname='s3',\n",
    "                    is_dir=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
