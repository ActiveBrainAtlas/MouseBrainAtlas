{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/skimage/filter/__init__.py:6: skimage_deprecation: The `skimage.filter` module has been renamed to `skimage.filters`.  This placeholder module will be removed in v0.13.\n",
      "  warn(skimage_deprecation('The `skimage.filter` module has been renamed '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from utilities2015 import *\n",
    "\n",
    "dm = DataManager(stack='MD593', section=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.cluster.hierarchy import average, fcluster, leaders, complete, single, dendrogram, ward\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations, chain, product\n",
    "\n",
    "import networkx\n",
    "\n",
    "def compute_overlap(c1, c2):\n",
    "    return float(len(c1 & c2)) / min(len(c1),len(c2))\n",
    "\n",
    "def compute_overlap2(c1, c2):\n",
    "    return float(len(c1 & c2)) / len(c1 | c2)    \n",
    "\n",
    "def compute_overlap_partial(indices, sets, metric='jaccard'):\n",
    "    n_sets = len(sets)\n",
    "    \n",
    "    overlap_matrix = np.zeros((len(indices), n_sets))\n",
    "        \n",
    "    for ii, i in enumerate(indices):\n",
    "        for j in range(n_sets):\n",
    "            c1 = set(sets[i])\n",
    "            c2 = set(sets[j])\n",
    "            if len(c1) == 0 or len(c2) == 0:\n",
    "                overlap_matrix[ii, j] = 0\n",
    "            else:\n",
    "                if metric == 'min-jaccard':\n",
    "                    overlap_matrix[ii, j] = compute_overlap(c1, c2)\n",
    "                elif metric == 'jaccard':\n",
    "                    overlap_matrix[ii, j] = compute_overlap2(c1, c2)\n",
    "                else:\n",
    "                    raise Exception('metric %s is unknown'%metric)\n",
    "            \n",
    "    return overlap_matrix\n",
    "\n",
    "def compute_pairwise_distances(sets, metric):\n",
    "\n",
    "    partial_overlap_mat = Parallel(n_jobs=16, max_nbytes=1e6)(delayed(compute_overlap_partial)(s, sets, metric=metric) \n",
    "                                        for s in np.array_split(range(len(sets)), 16))\n",
    "    overlap_matrix = np.vstack(partial_overlap_mat)\n",
    "    distance_matrix = 1 - overlap_matrix\n",
    "    \n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "def group_clusters(clusters=None, dist_thresh = 0.1, distance_matrix=None, metric='jaccard', linkage='complete'):\n",
    "    \n",
    "    if distance_matrix is not None:\n",
    "        keys = range(len(distance_matrix))\n",
    "        if clusters is not None:\n",
    "            values = clusters\n",
    "        else:\n",
    "            values = range(len(distance_matrix))\n",
    "    else:\n",
    "        if isinstance(clusters, dict):\n",
    "            keys = clusters.keys()\n",
    "            values = clusters.values()\n",
    "        elif isinstance(clusters, list):\n",
    "            if isinstance(clusters[0], tuple):\n",
    "                keys = [i for i,j in clusters]\n",
    "                values = [j for i,j in clusters]\n",
    "            else:\n",
    "                keys = range(len(clusters))\n",
    "                values = clusters\n",
    "        else:\n",
    "            raise Exception('clusters is not the right type')\n",
    "    \n",
    "    if clusters is None:\n",
    "        assert distance_matrix is not None, 'distance_matrix must be provided.'\n",
    "    \n",
    "    if distance_matrix is None:\n",
    "        assert clusters is not None, 'clusters must be provided'\n",
    "        distance_matrix = compute_pairwise_distances(values, metric)\n",
    "        \n",
    "    if linkage=='complete':\n",
    "        lk = complete(squareform(distance_matrix))\n",
    "    elif linkage=='average':\n",
    "        lk = average(squareform(distance_matrix))\n",
    "    elif linkage=='single':\n",
    "        lk = single(squareform(distance_matrix))\n",
    "\n",
    "    # T = fcluster(lk, 1.15, criterion='inconsistent')\n",
    "    T = fcluster(lk, dist_thresh, criterion='distance')\n",
    "    \n",
    "    n_groups = len(set(T))\n",
    "    groups = [None] * n_groups\n",
    "\n",
    "    for group_id in range(n_groups):\n",
    "        groups[group_id] = np.where(T == group_id+1)[0]\n",
    "\n",
    "    index_groups = [[keys[i] for i in g] for g in groups if len(g) > 0]\n",
    "    res = [[values[i] for i in g] for g in groups if len(g) > 0]\n",
    "        \n",
    "    return index_groups, res, distance_matrix\n",
    "\n",
    "def smart_union(x):\n",
    "    cc = Counter(chain(*x))\n",
    "    gs = set([s for s, c in cc.iteritems() if c > (cc.most_common(1)[0][1]*.3)])                           \n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/PIL/Image.py:2261: DecompressionBombWarning: Image size (203470848 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    }
   ],
   "source": [
    "segmentation = dm.load_pipeline_result('segmentation', 'npy')\n",
    "n_superpixels = segmentation.max() + 1\n",
    "textonmap = dm.load_pipeline_result('texMap', 'npy')\n",
    "n_texton = textonmap.max() + 1\n",
    "texton_hists = dm.load_pipeline_result('texHist', 'npy')\n",
    "neighbors = dm.load_pipeline_result('neighbors', 'pkl')\n",
    "segmentation_vis = dm.load_pipeline_result('segmentationWithText', 'jpg')\n",
    "\n",
    "expansion_clusters_tuples = dm.load_pipeline_result('clusters', 'pkl')\n",
    "expansion_clusters, expansion_cluster_scores = zip(*expansion_clusters_tuples)\n",
    "expansion_cluster_scores = np.array(expansion_cluster_scores)\n",
    "\n",
    "neighbors_dict = dict(zip(np.arange(n_superpixels), [list(i) for i in neighbors]))\n",
    "neighbor_graph = networkx.from_dict_of_lists(neighbors_dict)\n",
    "\n",
    "surrounds_sps = dm.load_pipeline_result('clusterSurrounds', 'pkl')\n",
    "frontiers_sps = dm.load_pipeline_result('clusterFrontiers', 'pkl')\n",
    "\n",
    "edge_coords = dict(dm.load_pipeline_result('edgeCoords', 'pkl'))\n",
    "edge_neighbors = dm.load_pipeline_result('edgeNeighbors', 'pkl')\n",
    "\n",
    "dedge_vectors = dm.load_pipeline_result('edgeVectors', 'pkl')\n",
    "dedge_neighbors = dm.load_pipeline_result('dedgeNeighbors', 'pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute supporter set of each edgelet ...\n",
      "done in 4.037205 seconds\n"
     ]
    }
   ],
   "source": [
    "# votes for directed edgelets\n",
    "dedge_vote_dict = defaultdict(float)\n",
    "\n",
    "sys.stderr.write('compute supporter set of each edgelet ...\\n')\n",
    "t = time.time()\n",
    "\n",
    "# Compute the supporter sets of every edgelet, based on region proposals\n",
    "# supporter_all[(100,101)] is the set of superpixels that supports directed edgelet (100,101)\n",
    "dedge_supporters = defaultdict(list)\n",
    "\n",
    "for s in range(n_superpixels):\n",
    "\n",
    "    c = list(expansion_clusters[s])\n",
    "    interior_texture = texton_hists[c].mean(axis=0)\n",
    "    b_sps = surrounds_sps[s]\n",
    "    b_contrasts = cdist(texton_hists[b_sps], interior_texture[np.newaxis, :], chi2)\n",
    "\n",
    "    for b_sp, b_contrast in zip(b_sps, b_contrasts):\n",
    "        int_sps = neighbors[b_sp] & set(expansion_clusters[s])\n",
    "        for int_sp in int_sps:\n",
    "            # weight of each edgelet is the contrast normalized by region size\n",
    "#             weight = float(b_contrast) / max(len(c), 5)\n",
    "#             weight = 1. / max(len(c), 5)\n",
    "            weight = 1.\n",
    "            dedge_vote_dict[(b_sp, int_sp)] += weight\n",
    "#             dedge_vote_dict[(int_sp, b_sp)] += weight\n",
    "            dedge_supporters[(b_sp, int_sp)].append(s) # (border_sp, interior_sp) or (out, in)\n",
    "\n",
    "dedge_vote_dict.default_factory = None\n",
    "dedge_supporters.default_factory = None\n",
    "    \n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edgeContainedBy.pkl already exists, skip\n"
     ]
    }
   ],
   "source": [
    "all_edges = edge_coords.keys()\n",
    "all_dedges = set(chain(*[[(i,j),(j,i)] for i,j in all_edges]))\n",
    "\n",
    "try:\n",
    "    edge_contained_by = dm.load_pipeline_result('edgeContainedBy', 'pkl')\n",
    "    print \"edgeContainedBy.pkl already exists, skip\"\n",
    "\n",
    "except:\n",
    "\n",
    "\tsys.stderr.write('compute edge-contained-by lookup table...\\n')\n",
    "\tt = time.time()\n",
    "\n",
    "\tcluster_edges = dm.load_pipeline_result('clusterEdges', 'pkl')\n",
    "\n",
    "\tdef f(c, e):\n",
    "\t    q = set(chain(*[[(i,j),(j,i)] for i,j in combinations(c, 2) if frozenset([i,j]) in all_edges]))\n",
    "\t    return q | set(e)\n",
    "\n",
    "\tcontain_edges = Parallel(n_jobs=16)(delayed(f)(c,e) for c, e in zip(expansion_clusters, cluster_edges))\n",
    "\n",
    "\tedge_contained_by = defaultdict(set)\n",
    "\tfor sp, es in enumerate(contain_edges):\n",
    "\t    for e in es:\n",
    "\t        edge_contained_by[e].add(sp)\n",
    "\n",
    "\tedge_contained_by.default_factory = None\n",
    "\n",
    "\tdm.save_pipeline_result(edge_contained_by, 'edgeContainedBy', 'pkl')\n",
    "\n",
    "\tsys.stderr.write('done in %f seconds\\n' % (time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter dedges ...\n",
      "done in 32.473992 seconds\n"
     ]
    }
   ],
   "source": [
    "# only consider dedges that receive non-zero vote\n",
    "nz_dedges = dedge_vote_dict.keys()\n",
    "\n",
    "sys.stderr.write('filter dedges ...\\n')\n",
    "t = time.time()\n",
    "\n",
    "# compute contrast of each dedge\n",
    "# dedge_contrast = dict([((i,j), chi2(texton_hists[i], texton_hists[j])) for i,j in all_dedges])\n",
    "dedge_contrast = dict([((i,j), chi2(texton_hists[i], texton_hists[dedge_supporters[(i,j)]].mean(axis=0))) \n",
    "                       for i,j in nz_dedges])\n",
    "\n",
    "# filter dedges, require contrast > .5 and contained by at least 4 growed regions\n",
    "nz_dedges2 = [e for e, sps in edge_contained_by.iteritems() if len(sps) > 3 and e in nz_dedges]\n",
    "nz_dedges2 = [e for e in nz_dedges2 if dedge_contrast[e] > .5]\n",
    "\n",
    "# compute stop ratio of each dedge\n",
    "dedge_stopperness = dict([(e, dedge_vote_dict[e]/len(edge_contained_by[e])) for e in nz_dedges2])\n",
    "\n",
    "# filter dedges, require stop ratio to be 1.\n",
    "nz_dedges2 = [e for e in nz_dedges2 if dedge_stopperness[e] == 1.]\n",
    "#     print len(nz_dedges2), 'valid edges'\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-40bbc67a6a11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mviz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_edge_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnz_dedges2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/oasis/projects/nsf/csd395/yuncong/Brain/notebooks/utilities2015.pyc\u001b[0m in \u001b[0;36mvisualize_edge_sets\u001b[1;34m(self, edge_sets, img, text_size, colors, directed, neighbors, text)\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0mstroke_pts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_coords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0medge\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m                 \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstroke_pts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "viz = dm.visualize_edge_sets([nz_dedges2], directed=False)\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tmp.jpg' target='_blank'>tmp.jpg</a><br>"
      ],
      "text/plain": [
       "/oasis/projects/nsf/csd395/yuncong/Brain/notebooks/tmp.jpg"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sys.stderr.write('compute expanded supporter set for each edgelet ...\\n')\n",
    "\n",
    "# find union supporter set for each dedge\n",
    "dedge_expandedSupporters = dict([(e, smart_union([expansion_clusters[s] for s in dedge_supporters[e]])) \n",
    "                             for e in nz_dedges2])\n",
    "\n",
    "# cluster union supporter sets\n",
    "dedges_grouped, dedge_supporters_grouped, _ = group_clusters(clusters=dict((e, dedge_expandedSupporters[e]) for e in nz_dedges2),\n",
    "                                                             dist_thresh=.01, linkage='complete', metric='jaccard')\n",
    "dedges_grouped = map(set, dedges_grouped)\n",
    "ngroup = len(dedges_grouped)\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))\n",
    "\n",
    "#     print len(dedges_grouped), 'edge groups'\n",
    "\n",
    "sys.stderr.write('compute supporter set consistency factor...\\n')\n",
    "t = time.time()\n",
    "# compute cluster \"centroids\"\n",
    "dedge_group_supporters = map(smart_union, dedge_supporters_grouped)\n",
    "\n",
    "# compute centroid distances\n",
    "# analyze supporter set consistency factor (1)\n",
    "dedge_group_supporter_distmat = compute_pairwise_distances(dedge_group_supporters, metric='jaccard')\n",
    "np.fill_diagonal(dedge_group_supporter_distmat, 0)\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))\n",
    "\n",
    "\n",
    "sys.stderr.write('compute connectivity factor...\\n')\n",
    "t = time.time()\n",
    "\n",
    "# analyze connectivity factor (2)\n",
    "G = networkx.from_dict_of_lists(dedge_neighbors)\n",
    "conns = [[set() if any([sorted(e1)==sorted(e2) for e1, e2 in product(eg1, eg2)]) \n",
    "          else set([(i,j) for i,j in G.edges(eg1|eg2) if (i in eg1 and j in eg2) or (j in eg1 and i in eg2)]) \n",
    "         for eg1 in dedges_grouped] for eg2 in dedges_grouped]\n",
    "conns_flat = [a for b in conns for a in b ]\n",
    "dedge_group_edgeConn_distmat = np.reshape(map(lambda x: len(x) < 1, conns_flat), (ngroup, ngroup))\n",
    "np.fill_diagonal(dedge_group_edgeConn_distmat, 0)\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))\n",
    "\n",
    "\n",
    "sys.stderr.write('compute connectivity factor...\\n')\n",
    "t = time.time()\n",
    "\n",
    "# analyze texture similarity factor (3)\n",
    "dedge_group_supporterTex_distmat = np.reshape([chi2(texton_hists[list(sps1)].mean(axis=0), texton_hists[list(sps2)].mean(axis=0))  \n",
    "\t\tfor sps1, sps2 in product(dedge_group_supporters, dedge_group_supporters)], (ngroup, ngroup))\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))\n",
    "\n",
    "\n",
    "sys.stderr.write('combine three factors ...\\n')\n",
    "t = time.time()\n",
    "\n",
    "# combine above three factors\n",
    "dedge_group_distmat = 1 - (1-dedge_group_edgeConn_distmat) * (1-dedge_group_supporter_distmat>0.1) * (dedge_group_supporterTex_distmat < .25)\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))\n",
    "\n",
    "\n",
    "sys.stderr.write('further cluster dedge groups ...\\n')\n",
    "t = time.time()\n",
    "\n",
    "# further cluster dedge groups\n",
    "_, edge_groups, _ = group_clusters(clusters=dedges_grouped, \n",
    "                               distance_matrix=dedge_group_distmat, \n",
    "                               dist_thresh=.5, linkage='single')\n",
    "\n",
    "sys.stderr.write('done in %f seconds\\n' % (time.time() - t))\n",
    "\n",
    "#     print len(edge_groups), 'edge groups after considering connectivity'\n",
    "\n",
    "edge_groups = map(lambda x: set(chain(*x)), edge_groups)\n",
    "\n",
    "# sort clusters by total contrast\n",
    "edge_groups_sorted = sorted(edge_groups, key=lambda x: sum(dedge_contrast[e] for e in x), reverse=True)\n",
    "edge_group_supporters_sorted = [smart_union(map(lambda e: dedge_expandedSupporters[e], es)) \n",
    "                                for es in edge_groups_sorted]\n",
    "\n",
    "\n",
    "viz = dm.visualize_edge_sets(edge_groups_sorted[:40], text_size=3, img=segmentation_vis)\n",
    "dm.save_pipeline_result(viz, 'topLandmarks', 'jpg')\n",
    "\n",
    "dm.save_pipeline_result(edge_groups_sorted, 'goodEdgeSets', 'pkl')\n",
    "dm.save_pipeline_result(edge_group_supporters_sorted, 'goodEdgeSetsSupporters', 'pkl')\n",
    "\n",
    "\n",
    "boundary_models = []\n",
    "\n",
    "for i, es in enumerate(edge_groups_sorted[:40]):\n",
    "    \n",
    "    es = list(es)\n",
    "    \n",
    "    interior_texture = texton_hists[list(edge_group_supporters_sorted[i])].mean(axis=0)\n",
    "\n",
    "    surrounds = [e[0] for e in es]\n",
    "    exterior_textures = np.array([texton_hists[s] if s!=-1 else np.nan * np.ones((texton_hists.shape[1],)) \n",
    "                                  for s in surrounds])\n",
    "    # how to deal with -1 in surrounds? Assign to an all np.nan vector\n",
    "\n",
    "    points = np.array([edge_coords[frozenset(e)].mean(axis=0) for e in es])\n",
    "    center = points.mean(axis=0)\n",
    "\n",
    "    boundary_models.append((es, interior_texture, exterior_textures, points, center))\n",
    "\n",
    "dm.save_pipeline_result(boundary_models, 'boundaryModels', 'pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
