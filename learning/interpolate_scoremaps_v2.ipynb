{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for Gordon\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import RectBivariateSpline\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paired_structures = ['5N', '6N', '7N', '7n', 'Amb', 'LC', 'LRt', 'Pn', 'Tz', 'VLL', 'RMC', 'SNC', 'SNR', '3N', '4N',\n",
    "                    'Sp5I', 'Sp5O', 'Sp5C', 'PBG', '10N', 'VCA', 'VCP', 'DC']\n",
    "singular_structures = ['AP', '12N', 'RtTg', 'SC', 'IC']\n",
    "\n",
    "structures = paired_structures + singular_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 1.95 seconds\n",
      "scale up: 9.82 seconds\n",
      "scale up: 9.86 seconds\n",
      "scale up: 9.86 seconds\n",
      "scale up: 9.88 seconds\n",
      "scale up: 9.93 seconds\n",
      "scale up: 9.94 seconds\n",
      "scale up: 9.95 seconds\n",
      "scale up: 9.96 seconds\n",
      "scale up: 9.98 seconds\n",
      "scale up: 9.97 seconds\n",
      "scale up: 10.00 seconds\n",
      "scale up: 10.03 seconds\n",
      "scale up: 10.03 seconds\n",
      "scale up: 10.03 seconds\n",
      "scale up: 10.05 seconds\n",
      "scale up: 9.42 seconds\n",
      "scale up: 9.54 seconds\n",
      "scale up: 9.52 seconds\n",
      "scale up: 9.67 seconds\n",
      "scale up: 9.60 seconds\n",
      "scale up: 9.58 seconds\n",
      "scale up: 9.67 seconds\n",
      "scale up: 9.72 seconds\n",
      "scale up: 9.71 seconds\n",
      "scale up: 9.68 seconds\n",
      "scale up: 9.63 seconds\n",
      "scale up: 9.72 seconds\n",
      "scale up: 9.70 seconds\n",
      "interpolate: 38.86 seconds\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD585']:\n",
    "        \n",
    "    sections_to_filenames = metadata_cache['sections_to_filenames'][stack]\n",
    "    first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    anchor_fn = metadata_cache['anchor_fn'][stack]\n",
    "    \n",
    "    train_sample_scheme = 1\n",
    "    \n",
    "    bar = show_progress_bar(first_sec, last_sec)\n",
    "    \n",
    "#     for sec in range(84, 84+1):\n",
    "    for sec in range(200, 201):\n",
    "#     for sec in range(first_sec, last_sec+1):\n",
    "\n",
    "        bar.value = sec\n",
    "        \n",
    "        print sec\n",
    "\n",
    "        fn = sections_to_filenames[sec]\n",
    "        if fn in ['Nonexisting', 'Rescan', 'Placeholder']:\n",
    "            continue\n",
    "\n",
    "        # output\n",
    "        scoremaps_dir = os.path.join(SCOREMAPS_ROOTDIR, stack, \n",
    "                                     '%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped' % \\\n",
    "                                     dict(stack=stack, fn=fn, anchor_fn=anchor_fn))\n",
    "        create_if_not_exists(scoremaps_dir)\n",
    "\n",
    "        ## define grid, generate patches\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        locations_fn = PATCH_FEATURES_ROOTDIR + '/%(stack)s/%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped/%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped_patch_locations.txt' % dict(stack=stack, fn=fn, anchor_fn=anchor_fn)\n",
    "\n",
    "        with open(locations_fn, 'r') as f:\n",
    "            sample_locations_roi = np.array([map(int, line.split()[1:]) for line in f.readlines()])\n",
    "        \n",
    "        ## interpolate\n",
    "        \n",
    "        interpolation_xmin, interpolation_ymin = sample_locations_roi.min(axis=0)\n",
    "        interpolation_xmax, interpolation_ymax = sample_locations_roi.max(axis=0)\n",
    "        interpolation_w = interpolation_xmax - interpolation_xmin + 1\n",
    "        interpolation_h = interpolation_ymax - interpolation_ymin + 1\n",
    "\n",
    "        ##### sample_locations_roi + scores to dense_score_map #####\n",
    "\n",
    "        shrink_factor = 4 # do interpolation on a smaller grid, then resize to original dimension\n",
    "\n",
    "        sample_locations_unique_xs = np.unique(sample_locations_roi[:,0])\n",
    "        sample_locations_unique_ys = np.unique(sample_locations_roi[:,1])\n",
    "\n",
    "        n_sample_x = sample_locations_unique_xs.size\n",
    "        n_sample_y = sample_locations_unique_ys.size\n",
    "\n",
    "        index_x = dict([(j,i) for i,j in enumerate(sample_locations_unique_xs)])\n",
    "        index_y = dict([(j,i) for i,j in enumerate(sample_locations_unique_ys)])\n",
    "        sample_location_indices = np.asarray([(index_x[x], index_y[y]) for x, y in sample_locations_roi])\n",
    "\n",
    "        sample_locations_interpolatedArea_ys_matrix, \\\n",
    "        sample_locations_interpolatedArea_xs_matrix = np.meshgrid(range(interpolation_ymin/shrink_factor, \n",
    "                                                                        interpolation_ymax/shrink_factor), \n",
    "                                                                  range(interpolation_xmin/shrink_factor, \n",
    "                                                                        interpolation_xmax/shrink_factor), \n",
    "                                                                  indexing='ij')\n",
    "\n",
    "#         sparse_score_dir = create_if_not_exists(os.path.join(SPARSE_SCORES_ROOTDIR, stack, '%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped' % \\\n",
    "#                                       {'fn': fn, 'anchor_fn': anchor_fn}))\n",
    "\n",
    "#         probs_allClasses = {label: bp.unpack_ndarray_file(sparse_score_dir + '/%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped_%(label)s_sparseScores.hdf' % \\\n",
    "#                     {'fn': fn, 'anchor_fn': anchor_fn, 'label':label})\n",
    "#                             for label in structures}\n",
    "\n",
    "        probs_allClasses = {label: DataManager.load_sparse_scores(stack, fn=fn, anchor_fn=anchor_fn, \n",
    "                                                  label=label, train_sample_scheme=train_sample_scheme)\n",
    "                            for label in structures}\n",
    "\n",
    "        sys.stderr.write('preprocess: %.2f seconds\\n' % (time.time() - t))\n",
    "        \n",
    "        def generate_score_map(label):\n",
    "\n",
    "            if label == 'BackG':\n",
    "                return None\n",
    "            \n",
    "            score_matrix = np.zeros((n_sample_x, n_sample_y))\n",
    "            score_matrix[sample_location_indices[:,0], sample_location_indices[:,1]] = probs_allClasses[label]\n",
    "\n",
    "            spline = RectBivariateSpline(sample_locations_unique_xs/shrink_factor, \n",
    "                                         sample_locations_unique_ys/shrink_factor, \n",
    "                                         score_matrix, \n",
    "                                         bbox=[interpolation_xmin/shrink_factor, \n",
    "                                               interpolation_xmax/shrink_factor, \n",
    "                                               interpolation_ymin/shrink_factor, \n",
    "                                               interpolation_ymax/shrink_factor])\n",
    "\n",
    "#             t = time.time()\n",
    "            dense_score_map = spline.ev(sample_locations_interpolatedArea_xs_matrix, \n",
    "                                        sample_locations_interpolatedArea_ys_matrix)\n",
    "#             sys.stderr.write('evaluate spline: %.2f seconds\\n' % (time.time() - t))\n",
    "            \n",
    "            t1 = time.time()\n",
    "            dense_score_map = resize(dense_score_map, (interpolation_h, interpolation_w)) # similar speed as rescale\n",
    "#             dense_score_map = rescale(dense_score_map, shrink_factor)\n",
    "            sys.stderr.write('scale up: %.2f seconds\\n' % (time.time() - t1))\n",
    "\n",
    "#             t = time.time()\n",
    "            dense_score_map[dense_score_map < 1e-1] = 0\n",
    "            dense_score_map[dense_score_map > 1.] = 1.\n",
    "#             sys.stderr.write('threshold: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "            if np.count_nonzero(dense_score_map) < 1e5:\n",
    "                sys.stderr.write('No %s is detected on section %d\\n' % (label, sec))\n",
    "                return None\n",
    "            \n",
    "            scoremap_bp_filepath, scoremap_interpBox_filepath = DataManager.get_scoremap_filepath(stack=stack, fn=fn, anchor_fn=anchor_fn, label=label,\n",
    "                                                                    return_bbox_fp=True, train_sample_scheme=train_sample_scheme)\n",
    "\n",
    "            save_hdf(dense_score_map.astype(np.float16), scoremap_bp_filepath, complevel=5)\n",
    "            \n",
    "            np.savetxt(scoremap_interpBox_filepath,\n",
    "                   np.array((interpolation_xmin, interpolation_xmax, interpolation_ymin, interpolation_ymax))[None],\n",
    "                   fmt='%d')\n",
    "            \n",
    "        \n",
    "        t = time.time()\n",
    "        \n",
    "        # if too many disk saves are simultaneous, they will be conflicting, so split into two sessions\n",
    "#         for i in range(0, len(structures), 8):\n",
    "#             _ = Parallel(n_jobs=16)(delayed(generate_score_map)(l) for l in structures[i:i+15])\n",
    "\n",
    "        _ = Parallel(n_jobs=15)(delayed(generate_score_map)(l) for l in structures)\n",
    " \n",
    "        sys.stderr.write('interpolate: %.2f seconds\\n' % (time.time() - t)) # ~ 30 seconds / section on one node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  },
  "widgets": {
   "state": {
    "58a831de21f84a26a2c61b5319aba84e": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
