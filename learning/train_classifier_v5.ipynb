{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuncong/Brain/utilities/utilities2015.py:2: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-fc4079e49eb7>\", line 14, in <module>\n",
      "    get_ipython().magic(u'matplotlib inline')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 308, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for Precision WorkStation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "No vtkNot using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import mxnet as mx\n",
    "except:\n",
    "    sys.stderr.write(\"Cannot import mxnet.\\n\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import rotate\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from distributed_utilities import *\n",
    "from visualization_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "sys.path.append('/home/yuncong/csd395/xgboost/python-package')\n",
    "try:\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "except:\n",
    "    sys.stderr.write('xgboost is not loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:53: UserWarning: \u001b[91mYou created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\n",
      "\tdata\u001b[0m\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:65: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "model_dir_name = 'inception-bn-blue'\n",
    "model_name = 'inception-bn-blue'\n",
    "model, mean_img = load_mxnet_model(model_dir_name=model_dir_name, model_name=model_name, \n",
    "                                   num_gpus=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "win_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacks = ['MD589', 'MD585', 'MD594']\n",
    "# stacks = ['MD661', 'MD662']\n",
    "# stacks = ['MD661', 'ChatCryoJane201710']\n",
    "# stacks = ['MD642', 'MD661']\n",
    "# stacks = ['MD661', 'MD662', 'ChatCryoJane201710']\n",
    "# stacks = ['MD589', 'MD661']\n",
    "# stacks = ['MD589', 'ChatCryoJane201710']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive \"s3://mousebrainatlas-data/CSHL_labelings_v3/MD589\" \"/home/yuncong/CSHL_labelings_v3/MD589\" --exclude \"*\" --include \"*win7*grid_indices_lookup*\"\n",
      "latest timestamp:  10042017100807\n",
      "aws s3 cp --recursive \"s3://mousebrainatlas-data/CSHL_labelings_v3/MD585\" \"/home/yuncong/CSHL_labelings_v3/MD585\" --exclude \"*\" --include \"*win7*grid_indices_lookup*\"\n",
      "latest timestamp:  08012017212649\n",
      "aws s3 cp --recursive \"s3://mousebrainatlas-data/CSHL_labelings_v3/MD594\" \"/home/yuncong/CSHL_labelings_v3/MD594\" --exclude \"*\" --include \"*win7*grid_indices_lookup*\"\n",
      "latest timestamp:  07302017183604\n"
     ]
    }
   ],
   "source": [
    "grid_indices_lookup_allStacks = {}\n",
    "\n",
    "for stack in stacks:\n",
    "    try:\n",
    "#         grid_indices_lookup_allStacks[stack] = \\\n",
    "#         DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "#                                                            by_human=False, timestamp='latest',\n",
    "#                                                            detector_id_f=1,\n",
    "#                                                           return_locations=True)            \n",
    "\n",
    "        grid_indices_lookup_allStacks[stack] = \\\n",
    "        DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "                                                           by_human=True, timestamp='latest',\n",
    "                                                          return_locations=True)            \n",
    "\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        sys.stderr.write(\"Fail to load annotation grid lookup for %s.\\n\" % stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "all_labels = sorted(list(set(chain.from_iterable(set(grid_indices_lookup_allStacks[st].columns.tolist()) \n",
    "                                                 for st in stacks))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_stacks = ['MD661', 'MD662']\n",
    "# train_stack_stain = {'MD661': 'F', 'MD662': 'F'}\n",
    "\n",
    "train_stacks = ['MD585', 'MD589', 'MD594']\n",
    "train_stack_stain = {'MD585': 'N', 'MD589': 'N', 'MD594': 'N'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stack_section_number = defaultdict(dict)\n",
    "\n",
    "for st in train_stacks:\n",
    "    for name_u in all_known_structures:\n",
    "#         if name_u == '4N' or name_u == '10N':\n",
    "#             train_stack_section_number[st][name_u] = 20\n",
    "#         else:\n",
    "#             train_stack_section_number[st][name_u] = 10\n",
    "        train_stack_section_number[st][name_u] = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_id = 998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# positive train = 25288\n",
      "# negative train = 65035\n",
      "('MD594', 163)\n",
      "rm -rf \"/media/yuncong/BstemAtlasData/CSHL_patch_features/inception-bn-blue/MD594/MD594_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7/MD594-N33-2015.08.26-22.44.00_MD594_1_0097_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7_inception-bn-blue_features.bp\" && mkdir -p \"/media/yuncong/BstemAtlasData/CSHL_patch_features/inception-bn-blue/MD594/MD594_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7\"\n",
      "aws s3 cp \"s3://mousebrainatlas-data/CSHL_patch_features/inception-bn-blue/MD594/MD594_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7/MD594-N33-2015.08.26-22.44.00_MD594_1_0097_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7_inception-bn-blue_features.bp\" \"/media/yuncong/BstemAtlasData/CSHL_patch_features/inception-bn-blue/MD594/MD594_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7/MD594-N33-2015.08.26-22.44.00_MD594_1_0097_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7_inception-bn-blue_features.bp\"\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/media/yuncong/BstemAtlasData/CSHL_patch_features/inception-bn-blue/MD594/MD594_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7/MD594-N33-2015.08.26-22.44.00_MD594_1_0097_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7_inception-bn-blue_features.bp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e27efb0fd72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                                         \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_scheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprep_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                                         batch_size=batch_size)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddresses_to_compute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuncong/Brain/utilities/learning_utilities.pyc\u001b[0m in \u001b[0;36mread_cnn_features\u001b[0;34m(addresses, scheme, win_id, prep_id, model, mean_img, model_name, batch_size)\u001b[0m\n\u001b[1;32m   2130\u001b[0m                                                                          \u001b[0mprep_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m                                                                          \u001b[0mnormalization_scheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2132\u001b[0;31m                                                                          model_name=model_name)\n\u001b[0m\u001b[1;32m   2133\u001b[0m         \u001b[0mlocation_to_pool_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocations_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuncong/Brain/utilities/data_manager.pyc\u001b[0m in \u001b[0;36mload_dnn_features_v2\u001b[0;34m(stack, prep_id, win_id, normalization_scheme, model_name, sec, fn)\u001b[0m\n\u001b[1;32m   3112\u001b[0m                                              model_name=model_name, what='features')\n\u001b[1;32m   3113\u001b[0m         \u001b[0mdownload_from_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_ROOTDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3114\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_ndarray_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m         locations_fp = DataManager.get_dnn_features_filepath_v2(stack=stack, sec=sec, fn=fn, prep_id=prep_id, win_id=win_id,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/bloscpack/numpy_io.pyc\u001b[0m in \u001b[0;36munpack_ndarray_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpack_ndarray_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompressedFPSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0munpack_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/media/yuncong/BstemAtlasData/CSHL_patch_features/inception-bn-blue/MD594/MD594_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7/MD594-N33-2015.08.26-22.44.00_MD594_1_0097_prep2_normalize_mu_region_sigma_wholeImage_(-1,5)_win7_inception-bn-blue_features.bp'"
     ]
    }
   ],
   "source": [
    "for structure in all_known_structures:\n",
    "# for structure in [\n",
    "#  'VCA',\n",
    "#  'VCP',\n",
    "#  'DC',\n",
    "#  'AP',\n",
    "#  '12N',\n",
    "#  'RtTg',\n",
    "#  'SC',\n",
    "#  'IC']:\n",
    "# for structure in ['LRt']:\n",
    "    \n",
    "    surround_margins = [200]\n",
    "\n",
    "# structure = 'Amb'\n",
    "#     structure = '7N'\n",
    "    # structure = 'SNR'\n",
    "    positive_labels = [structure]\n",
    "    negative_labels = get_negative_labels(structure, 'neg_has_all_surround', margin_um=200, \n",
    "                                          labels_found=all_labels)    \n",
    "    \n",
    "    positive_addresses_traindata_all_stacks = {}\n",
    "    negative_addresses_traindata_all_stacks = {}\n",
    "\n",
    "    for train_stack in train_stacks:\n",
    "        \n",
    "        train_sections = list(chain(*[grid_indices_lookup_allStacks[train_stack][pl].dropna(how='any').index.tolist() \n",
    "                                      for pl in positive_labels]))\n",
    "\n",
    "        ###################################################################################\n",
    "\n",
    "        n_train_sections = train_stack_section_number[train_stack][structure]\n",
    "        # n_train_sections = 10\n",
    "        # n_train_sections = len(train_sections)\n",
    "\n",
    "        if train_stack_stain[train_stack] == 'F':\n",
    "            neurotrace_sections = []\n",
    "            nissl_sections = []\n",
    "            for sec in train_sections:\n",
    "                if metadata_cache['sections_to_filenames'][train_stack][sec].split('-')[1][0] == 'F':\n",
    "                    neurotrace_sections.append(sec)\n",
    "                else:\n",
    "                    nissl_sections.append(sec)\n",
    "            random_train_sections = np.random.choice(neurotrace_sections, min(len(neurotrace_sections), n_train_sections), replace=False)\n",
    "        else:\n",
    "            random_train_sections = np.random.choice(train_sections, min(len(train_sections), n_train_sections), replace=False)\n",
    "\n",
    "        positive_addresses_traindata_all_stacks[train_stack] = sorted([(train_stack, sec, tuple(loc))\n",
    "for nl in set(positive_labels) & set(grid_indices_lookup_allStacks[train_stack].columns)\n",
    "  for sec, locs in grid_indices_lookup_allStacks[train_stack][nl].loc[random_train_sections].dropna().iteritems()\n",
    "  for loc in locs])\n",
    "        \n",
    "        negative_addresses_traindata_all_stacks[train_stack] = sorted([(train_stack, sec, tuple(loc))\n",
    "for nl in set(negative_labels) & set(grid_indices_lookup_allStacks[train_stack].columns)\n",
    "  for sec, locs in grid_indices_lookup_allStacks[train_stack][nl].loc[random_train_sections].dropna().iteritems()\n",
    "  for loc in locs])\n",
    "   \n",
    "    positive_addresses_traindata = sum(positive_addresses_traindata_all_stacks.values(), [])\n",
    "    negative_addresses_traindata = sum(negative_addresses_traindata_all_stacks.values(), [])\n",
    "\n",
    "    print '# positive train =', len(positive_addresses_traindata)\n",
    "    print '# negative train =', len(negative_addresses_traindata)\n",
    "\n",
    "    del positive_addresses_traindata_all_stacks, negative_addresses_traindata_all_stacks\n",
    "    \n",
    "    # features_dict = {(scheme, tfv): {} for scheme in schemes for tfv in transforms}\n",
    "    features_dict = defaultdict(dict)\n",
    "    \n",
    "    # train_scheme = 'stretch_min_max'\n",
    "    # train_scheme = 'normalize_mu_sigma_global_(-1,5)'\n",
    "    train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,5)'\n",
    "    # train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,9)'\n",
    "    # train_scheme = 'none'\n",
    "    addresses_to_compute = positive_addresses_traindata + negative_addresses_traindata\n",
    "    \n",
    "    features_loaded = read_cnn_features(addresses=addresses_to_compute, \n",
    "                                        scheme=train_scheme, win_id=7, prep_id=2, \n",
    "                                        model=model, mean_img=mean_img, model_name=model_name, \n",
    "                                        batch_size=batch_size)\n",
    "        \n",
    "    for addr, f in izip(addresses_to_compute, features_loaded):\n",
    "        features_dict[(train_scheme, 'transform0')][addr] = f\n",
    "    \n",
    "#     for i, f in enumerate(features_loaded):\n",
    "#         features_dict[(train_scheme, 'transform0')][addresses_to_compute[i]] = f\n",
    "\n",
    "    ###########################################################\n",
    "    \n",
    "    # n_train_list = [10, 100, 200, 500, 1000, 2000, 5000, 10000, 15000]\n",
    "#     n_train_list = [10, 1000]\n",
    "    n_train_list = [15000]\n",
    "#     test_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "    train_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for n_train in n_train_list:\n",
    "\n",
    "        for trial in range(1):\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "        #     n_train_pos = 5000\n",
    "            n_train_pos = min(n_train, len(positive_addresses_traindata))\n",
    "#             if len(positive_addresses_traindata) < n_train_pos:\n",
    "#                 continue\n",
    "            training_pos_indices = np.random.choice(range(len(positive_addresses_traindata)), n_train_pos, replace=False)\n",
    "            \n",
    "#             n_test_pos = 1000\n",
    "#             test_pos_indices = np.random.choice(range(len(positive_addresses_testdata)),\n",
    "#                                                 size=min(len(positive_addresses_testdata), n_test_pos), \n",
    "#                                                 replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_pos_total = len(positive_addresses)\n",
    "        #     n_train_pos = 1000\n",
    "        #     training_pos_indices = np.random.choice(range(n_pos_total), n_train_pos, replace=False)\n",
    "        #     test_pos_indices = np.random.choice(np.setdiff1d(range(n_pos_total), training_pos_indices),\n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_pos = len(test_pos_indices)\n",
    "\n",
    "            ###############\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "        #     n_train_neg = 5000\n",
    "            n_train_neg = n_train_pos\n",
    "            training_neg_indices = np.random.choice(range(len(negative_addresses_traindata)), n_train_neg, replace=False)\n",
    "#             n_test_neg = 1000\n",
    "#             test_neg_indices = np.random.choice(range(len(negative_addresses_testdata)),\n",
    "#                                                 size=min(len(negative_addresses_testdata), n_test_neg), \n",
    "#                                                 replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_neg_total = len(negative_addresses)\n",
    "        #     n_train_neg = 1000\n",
    "        #     training_neg_indices = np.random.choice(range(n_neg_total), n_train_neg, replace=False)\n",
    "        #     test_neg_indices = np.random.choice(np.setdiff1d(range(n_neg_total), training_neg_indices), \n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_neg = len(test_neg_indices)\n",
    "\n",
    "            print \"Training: %d positive, %d negative\" % (n_train_pos, n_train_neg)\n",
    "#             print \"Test: %d positive, %d negative\" % (n_test_pos, n_test_neg)\n",
    "\n",
    "            ################\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "            addresses_train_pos = [positive_addresses_traindata[i] for i in training_pos_indices]\n",
    "            addresses_train_neg = [negative_addresses_traindata[i] for i in training_neg_indices]\n",
    "\n",
    "            #################\n",
    "\n",
    "#             for augment_training in [True, False]:\n",
    "            for augment_training in [False]:\n",
    "\n",
    "                feature_classifier_alg = 'lr'\n",
    "        #             feature_classifier_alg = 'xgb2'\n",
    "        #             feature_classifier_alg = 'lin_svc'\n",
    "        #             feature_classifier_alg = 'lin_svc_calib'\n",
    "                sample_weights = None   \n",
    "\n",
    "                if augment_training:\n",
    "                    train_transforms = range(8)\n",
    "                else:\n",
    "                    train_transforms = range(1)\n",
    "\n",
    "                test_transforms = range(1)\n",
    "\n",
    "                features_train_pos = {(train_scheme, 'transform%d'%tf_variant): \n",
    "                                      [features_dict[(train_scheme, 'transform%d'%tf_variant)][addr] \n",
    "                                       for addr in addresses_train_pos]\n",
    "                                          for tf_variant in train_transforms}\n",
    "                features_train_neg = {(train_scheme, 'transform%d'%tf_variant): \n",
    "                                      [features_dict[(train_scheme, 'transform%d'%tf_variant)][addr] \n",
    "                                       for addr in addresses_train_neg]\n",
    "                                          for tf_variant in train_transforms}\n",
    "\n",
    "                if augment_training:\n",
    "\n",
    "                    train_data_list = []\n",
    "                    train_label_list = []\n",
    "                    for tf in range(8):\n",
    "                        train_data = np.r_[features_train_pos[(train_scheme,'transform%d'%tf)], \n",
    "                                           features_train_neg[(train_scheme,'transform%d'%tf)]]\n",
    "                        train_data_list.append(train_data)\n",
    "\n",
    "                        train_labels = np.r_[np.ones((len(features_train_pos[(train_scheme,'transform%d'%tf)]), )), \n",
    "                                             -np.ones((len(features_train_neg[(train_scheme,'transform%d'%tf)]), ))]\n",
    "                        train_label_list.append(train_labels)\n",
    "\n",
    "                    train_data = np.concatenate(train_data_list)\n",
    "                    train_labels = np.concatenate(train_label_list)        \n",
    "                else:\n",
    "\n",
    "                    train_data = np.r_[features_train_pos[(train_scheme,'transform0')], \n",
    "                                       features_train_neg[(train_scheme,'transform0')]]\n",
    "                    train_labels = np.r_[np.ones((len(features_train_pos[(train_scheme,'transform0')]), )), \n",
    "                                         -np.ones((len(features_train_neg[(train_scheme,'transform0')]), ))]\n",
    "\n",
    "\n",
    "                clf = train_binary_classifier(train_data, train_labels,\n",
    "                                       alg=feature_classifier_alg, \n",
    "                                       sample_weights=sample_weights)\n",
    "                \n",
    "                del train_data, features_train_pos, features_train_neg, features_loaded, features_dict\n",
    "\n",
    "                clf_fp = DataManager.get_classifier_filepath(classifier_id=classifier_id, structure=structure)\n",
    "                create_parent_dir_if_not_exists(clf_fp)\n",
    "                joblib.dump(clf, clf_fp)\n",
    "                upload_to_s3(clf_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- classfier 999 is trained from 15,000 pos and 15,000 neg from both MD661 and MD662\n",
    "- classfier 998 is trained from 15,000 pos and 15,000 neg from MD589/MD585/MD594"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
