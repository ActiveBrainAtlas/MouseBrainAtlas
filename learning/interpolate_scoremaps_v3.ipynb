{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from multiprocess import Pool\n",
    "import numpy as np\n",
    "\n",
    "# from scipy.interpolate import RectBivariateSpline\n",
    "# from skimage.transform import resize\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setting = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is the preferred and efficient approach for interpolating multidimensional data?\n",
    "# http://scicomp.stackexchange.com/questions/19137/what-is-the-preferred-and-efficient-approach-for-interpolating-multidimensional\n",
    "# http://nbviewer.jupyter.org/github/pierre-haessig/stodynprog/blob/master/stodynprog/linear_interp_benchmark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downscale = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_structures = len(all_structures_with_classifiers)\n",
    "# label_to_structure_map = dict(enumerate(all_structures_with_classifiers))\n",
    "# structure_to_label_map = {s: l for l, s in label_to_structure_map.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/oasis/projects/nsf/csd395/yuncong/CSHL_patch_features/Sat16ClassFinetuned/MD589/MD589-N16-2015.07.30-17.03.43_MD589_3_0048_lossless_alignedTo_MD589-IHC31-2015.07.30-23.26.22_MD589_1_0091_cropped/MD589-N16-2015.07.30-17.03.43_MD589_3_0048_lossless_alignedTo_MD589-IHC31-2015.07.30-23.26.22_MD589_1_0091_cropped_patch_locations.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a984637accde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_sec\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/oasis/projects/nsf/csd395/yuncong/brain_virtualenv/lib/python2.7/site-packages/multiprocess/pool.pyc\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    249\u001b[0m         '''\n\u001b[1;32m    250\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/oasis/projects/nsf/csd395/yuncong/brain_virtualenv/lib/python2.7/site-packages/multiprocess/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/oasis/projects/nsf/csd395/yuncong/CSHL_patch_features/Sat16ClassFinetuned/MD589/MD589-N16-2015.07.30-17.03.43_MD589_3_0048_lossless_alignedTo_MD589-IHC31-2015.07.30-23.26.22_MD589_1_0091_cropped/MD589-N16-2015.07.30-17.03.43_MD589_3_0048_lossless_alignedTo_MD589-IHC31-2015.07.30-23.26.22_MD589_1_0091_cropped_patch_locations.txt'"
     ]
    }
   ],
   "source": [
    "# Parallel\n",
    "\n",
    "for stack in ['MD642']:\n",
    "                \n",
    "    patch_size, spacing, w, h = get_default_gridspec(stack)\n",
    "    half_size = patch_size/2\n",
    "    \n",
    "    first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "    def func(sec):\n",
    "    \n",
    "        if is_invalid(stack=stack, sec=sec):\n",
    "            return\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        _, sample_locations_roi = DataManager.load_dnn_feature_locations(stack=stack, \n",
    "                                            model_name='Sat16ClassFinetuned', section=sec)\n",
    "\n",
    "        actual_setting = resolve_actual_setting(setting=setting, stack=stack, sec=sec)\n",
    "        \n",
    "        downscaled_grid_y = np.arange(0, h, downscale)\n",
    "        downscaled_grid_x = np.arange(0, w, downscale)\n",
    "        downscaled_ny = len(downscaled_grid_y)\n",
    "        downscaled_nx = len(downscaled_grid_x)\n",
    "                \n",
    "        for structure in all_structures_with_classifiers:\n",
    "            try:\n",
    "                sparse_scores = DataManager.load_sparse_scores(stack, sec=sec,\n",
    "                                                               structure=structure, \n",
    "                                                               setting=actual_setting)\n",
    "            except Exception as e:\n",
    "                sys.stderr.write('Error loading for %s do not exist.\\n' % structure)\n",
    "                continue\n",
    "\n",
    "            f_grid = np.zeros(((h-half_size)/spacing+1, (w-half_size)/spacing+1))\n",
    "            a = (sample_locations_roi - half_size)/spacing\n",
    "            f_grid[a[:,1], a[:,0]] = sparse_scores\n",
    "\n",
    "            yinterps = (downscaled_grid_y - half_size)/float(spacing)\n",
    "            xinterps = (downscaled_grid_x - half_size)/float(spacing)\n",
    "\n",
    "            points_y, points_x = np.broadcast_arrays(yinterps.reshape(-1,1), xinterps)\n",
    "            coord = np.c_[points_y.flat, points_x.flat]\n",
    "            f_interp = map_coordinates(f_grid, coord.T, order=1)\n",
    "            f_interp_2d = f_interp.reshape((downscaled_ny, downscaled_nx))                       \n",
    "\n",
    "            scoremap_bp_filepath = \\\n",
    "            DataManager.get_downscaled_scoremap_filepath(stack=stack, section=sec, \n",
    "                                                         structure=structure, \n",
    "                                                         setting=actual_setting,\n",
    "                                                        downscale=downscale)\n",
    "\n",
    "            create_parent_dir_if_not_exists(scoremap_bp_filepath)\n",
    "            bp.pack_ndarray_file(f_interp_2d.astype(np.float16), scoremap_bp_filepath)\n",
    "\n",
    "        sys.stderr.write('interpolate: %.2f seconds\\n' % (time.time() - t)) \n",
    "        \n",
    "    pool = Pool(15)\n",
    "    pool.map(func, range(first_sec, last_sec+1))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File does not exist: /oasis/projects/nsf/csd395/yuncong/CSHL_sparse_scoremaps/MD642/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped_sp5_sparseScores_setting_2.hdf\n",
      "Error loading for sp5 do not exist.\n",
      "File does not exist: /oasis/projects/nsf/csd395/yuncong/CSHL_sparse_scoremaps/MD642/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped_outerContour_sparseScores_setting_2.hdf\n",
      "Error loading for outerContour do not exist.\n",
      "interpolate: 4.13 seconds\n"
     ]
    }
   ],
   "source": [
    "# Sequential\n",
    "\n",
    "for stack in ['MD642']:\n",
    "                \n",
    "    patch_size, spacing, w, h = get_default_gridspec(stack)\n",
    "    half_size = patch_size/2\n",
    "    \n",
    "    first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    bar = show_progress_bar(first_sec, last_sec)\n",
    "\n",
    "#     for sec in range(193, 194):\n",
    "    for sec in range(first_sec, last_sec+1):\n",
    "\n",
    "        bar.value = sec\n",
    "        \n",
    "        if is_invalid(stack=stack, sec=sec):\n",
    "            continue\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        _, sample_locations_roi = DataManager.load_dnn_feature_locations(stack=stack, model_name='Sat16ClassFinetuned', section=sec)\n",
    "\n",
    "        actual_setting = resolve_actual_setting(setting=setting, stack=stack, sec=sec)\n",
    "        \n",
    "        for structure in all_known_structures:\n",
    "            try:\n",
    "                sparse_scores = DataManager.load_sparse_scores(stack, sec=sec,\n",
    "                                                               structure=structure, setting=actual_setting)\n",
    "            except Exception as e:\n",
    "                sys.stderr.write('Error loading for %s do not exist.\\n' % structure)\n",
    "                continue\n",
    "                \n",
    "            f_grid = np.zeros(((h-half_size)/spacing+1, (w-half_size)/spacing+1))\n",
    "            a = (sample_locations_roi - half_size)/spacing\n",
    "            f_grid[a[:,1], a[:,0]] = sparse_scores\n",
    "\n",
    "            yinterps = (np.arange(0, h, downscale) - half_size)/float(spacing)\n",
    "            xinterps = (np.arange(0, w, downscale) - half_size)/float(spacing)\n",
    "\n",
    "            points_y, points_x = np.broadcast_arrays(yinterps.reshape(-1,1), xinterps)\n",
    "            coord = np.c_[points_y.flat, points_x.flat]\n",
    "            f_interp = map_coordinates(f_grid, coord.T, order=1)\n",
    "            f_interp_2d = f_interp.reshape((len(yinterps), len(xinterps)))\n",
    "\n",
    "            scoremap_bp_filepath = \\\n",
    "            DataManager.get_downscaled_scoremap_filepath(stack=stack, section=sec, \n",
    "                                                         structure=structure, \n",
    "                                                         setting=actual_setting,\n",
    "                                                        downscale=downscale)\n",
    "\n",
    "            create_parent_dir_if_not_exists(scoremap_bp_filepath)\n",
    "#             save_hdf(f_interp_2d.astype(np.float16), scoremap_bp_filepath, complevel=5)\n",
    "            bp.pack_ndarray_file(f_interp_2d.astype(np.float16), scoremap_bp_filepath)\n",
    " \n",
    "        sys.stderr.write('interpolate: %.2f seconds\\n' % (time.time() - t)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lossless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New method - NOT faster than old method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yinterp = (np.arange(0, h)-half_size)/float(spacing)\n",
    "xinterp = (np.arange(0, w)-half_size)/float(spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.5506660938\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "points_y, points_x = np.broadcast_arrays(yinterp.reshape(-1,1), xinterp)\n",
    "coord = np.c_[points_y.flat, points_x.flat]\n",
    "f_2d_interp = map_coordinates(p, coord.T, order=1)\n",
    "print time.time() - t # 27s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441467377,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_2d_interp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = f_2d_interp.reshape((15457, 28561))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viz = plt.cm.hot(q[::8,::8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tmp.jpg' target='_blank'>tmp.jpg</a><br>"
      ],
      "text/plain": [
       "/oasis/projects/nsf/csd395/yuncong/Brain/learning/tmp.jpg"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_image(img_as_ubyte(viz)[..., :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_2d_interp = map_coordinates(f_2d_grid, coord, order=1).reshape(len(xinterp), len(yinterp))\n",
    "\n",
    "plt.imshow(f_2d_interp.T)\n",
    "plt.title(u'interpolation of a 2D function ({}² pts)'.format(Ninterp));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File does not exist: /oasis/projects/nsf/csd395/yuncong/CSHL_sparse_scoremaps/MD642/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped_sp5_sparseScores_setting_2.hdf\n",
      "Patch predictions for sp5 do not exist.\n",
      "File does not exist: /oasis/projects/nsf/csd395/yuncong/CSHL_sparse_scoremaps/MD642/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped/MD642-N38-2017.01.19-11.27.28_MD642_2_0113_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped_outerContour_sparseScores_setting_2.hdf\n",
      "Patch predictions for outerContour do not exist.\n",
      "preprocess: 2.97 seconds\n",
      "evaluate spline: 14.46 seconds\n",
      "evaluate spline: 14.46 seconds\n",
      "evaluate spline: 14.46 seconds\n",
      "evaluate spline: 14.47 seconds\n",
      "scale up: 22.35 seconds\n",
      "scale up: 22.35 seconds\n",
      "scale up: 22.34 seconds\n",
      "scale up: 22.36 seconds\n",
      "save: 4.53 seconds\n",
      "save: 5.08 seconds\n",
      "save: 5.35 seconds\n",
      "save: 5.46 seconds\n",
      "evaluate spline: 14.47 seconds\n",
      "evaluate spline: 14.43 seconds\n",
      "evaluate spline: 14.44 seconds\n",
      "evaluate spline: 14.46 seconds\n",
      "scale up: 22.26 seconds\n",
      "scale up: 22.26 seconds\n",
      "scale up: 22.29 seconds\n",
      "scale up: 22.31 seconds\n",
      "save: 4.88 seconds\n",
      "save: 4.99 seconds\n",
      "save: 4.61 seconds\n",
      "save: 6.04 seconds\n",
      "evaluate spline: 14.47 seconds\n",
      "evaluate spline: 14.46 seconds\n",
      "evaluate spline: 14.45 seconds\n",
      "evaluate spline: 14.43 seconds\n",
      "scale up: 22.24 seconds\n",
      "scale up: 22.25 seconds\n",
      "scale up: 22.32 seconds\n",
      "scale up: 22.28 seconds\n",
      "save: 4.91 seconds\n",
      "save: 5.03 seconds\n",
      "save: 5.82 seconds\n",
      "save: 5.10 seconds\n",
      "evaluate spline: 14.48 seconds\n",
      "evaluate spline: 14.44 seconds\n",
      "evaluate spline: 14.45 seconds\n",
      "evaluate spline: 14.42 seconds\n",
      "scale up: 22.24 seconds\n",
      "scale up: 22.24 seconds\n",
      "scale up: 22.32 seconds\n",
      "scale up: 22.29 seconds\n",
      "save: 5.27 seconds\n",
      "save: 4.99 seconds\n",
      "save: 5.45 seconds\n",
      "save: 5.33 seconds\n",
      "evaluate spline: 14.48 seconds\n",
      "evaluate spline: 14.45 seconds\n",
      "evaluate spline: 14.44 seconds\n",
      "evaluate spline: 14.45 seconds\n",
      "scale up: 22.24 seconds\n",
      "scale up: 22.25 seconds\n",
      "scale up: 22.32 seconds\n",
      "scale up: 22.28 seconds\n",
      "save: 5.02 seconds\n",
      "save: 6.14 seconds\n",
      "save: 4.84 seconds\n",
      "save: 5.96 seconds\n",
      "evaluate spline: 14.47 seconds\n",
      "evaluate spline: 14.44 seconds\n",
      "evaluate spline: 14.45 seconds\n",
      "evaluate spline: 14.44 seconds\n",
      "scale up: 22.26 seconds\n",
      "scale up: 22.24 seconds\n",
      "scale up: 22.29 seconds\n",
      "scale up: 22.28 seconds\n",
      "save: 4.53 seconds\n",
      "save: 5.07 seconds\n",
      "save: 5.17 seconds\n",
      "save: 6.26 seconds\n",
      "evaluate spline: 14.46 seconds\n",
      "evaluate spline: 14.44 seconds\n",
      "scale up: 22.25 seconds\n",
      "scale up: 22.25 seconds\n",
      "save: 4.37 seconds\n",
      "save: 5.74 seconds\n",
      "evaluate spline: 14.45 seconds\n",
      "evaluate spline: 14.46 seconds\n",
      "scale up: 22.25 seconds\n",
      "scale up: 22.25 seconds\n",
      "save: 5.53 seconds\n",
      "save: 5.49 seconds\n",
      "interpolate: 362.80 seconds\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD642']:\n",
    "        \n",
    "    sections_to_filenames = metadata_cache['sections_to_filenames'][stack]\n",
    "    first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    anchor_fn = metadata_cache['anchor_fn'][stack]\n",
    "        \n",
    "    bar = show_progress_bar(first_sec, last_sec)\n",
    "    \n",
    "#     for sec in range(193, 194):\n",
    "    for sec in range(first_sec, last_sec+1):\n",
    "\n",
    "        actual_setting = resolve_actual_setting(setting=setting, stack=stack, sec=sec)\n",
    "\n",
    "        bar.value = sec\n",
    "        \n",
    "        print sec\n",
    "\n",
    "        fn = sections_to_filenames[sec]\n",
    "        if is_invalid(fn):\n",
    "            continue     \n",
    "\n",
    "        # output\n",
    "        scoremaps_dir = create_if_not_exists(os.path.join(SCOREMAPS_ROOTDIR, stack, \n",
    "                                     '%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped' % \\\n",
    "                                     dict(stack=stack, fn=fn, anchor_fn=anchor_fn)))\n",
    "\n",
    "        ## define grid, generate patches\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        _, sample_locations_roi = DataManager.load_dnn_feature_locations(stack=stack, model_name='Sat16ClassFinetuned', fn=fn, anchor_fn=anchor_fn)\n",
    "        \n",
    "#         locations_fn = os.path.join(PATCH_FEATURES_ROOTDIR, \\\n",
    "#         '%(stack)s/%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped/%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped_patch_locations.txt' % dict(stack=stack, fn=fn, anchor_fn=anchor_fn))\n",
    "\n",
    "#         with open(locations_fn, 'r') as f:\n",
    "#             sample_locations_roi = np.array([map(int, line.split()[1:]) for line in f.readlines()])\n",
    "        \n",
    "        ## interpolate\n",
    "        \n",
    "        interpolation_xmin, interpolation_ymin = sample_locations_roi.min(axis=0)\n",
    "        interpolation_xmax, interpolation_ymax = sample_locations_roi.max(axis=0)\n",
    "        interpolation_w = interpolation_xmax - interpolation_xmin + 1\n",
    "        interpolation_h = interpolation_ymax - interpolation_ymin + 1\n",
    "\n",
    "        ##### sample_locations_roi + scores to dense_score_map #####\n",
    "\n",
    "        shrink_factor = 4 # do interpolation on a smaller grid, then resize to original dimension\n",
    "\n",
    "        sample_locations_unique_xs = np.unique(sample_locations_roi[:,0])\n",
    "        sample_locations_unique_ys = np.unique(sample_locations_roi[:,1])\n",
    "\n",
    "        n_sample_x = sample_locations_unique_xs.size\n",
    "        n_sample_y = sample_locations_unique_ys.size\n",
    "\n",
    "        index_x = dict([(j,i) for i,j in enumerate(sample_locations_unique_xs)])\n",
    "        index_y = dict([(j,i) for i,j in enumerate(sample_locations_unique_ys)])\n",
    "        sample_location_indices = np.asarray([(index_x[x], index_y[y]) for x, y in sample_locations_roi])\n",
    "\n",
    "        sample_locations_interpolatedArea_ys_matrix, \\\n",
    "        sample_locations_interpolatedArea_xs_matrix = np.meshgrid(range(interpolation_ymin/shrink_factor, \n",
    "                                                                        interpolation_ymax/shrink_factor), \n",
    "                                                                  range(interpolation_xmin/shrink_factor, \n",
    "                                                                        interpolation_xmax/shrink_factor), \n",
    "                                                                  indexing='ij')\n",
    "\n",
    "#         sparse_score_dir = create_if_not_exists(os.path.join(SPARSE_SCORES_ROOTDIR, stack, '%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped' % \\\n",
    "#                                       {'fn': fn, 'anchor_fn': anchor_fn}))\n",
    "\n",
    "#         probs_allClasses = {label: bp.unpack_ndarray_file(sparse_score_dir + '/%(fn)s_lossless_alignedTo_%(anchor_fn)s_cropped_%(label)s_sparseScores.hdf' % \\\n",
    "#                     {'fn': fn, 'anchor_fn': anchor_fn, 'label':label})\n",
    "#                             for label in structures}\n",
    "\n",
    "        probs_allClasses = {}\n",
    "        for structure in all_known_structures:\n",
    "            try:\n",
    "                probs_allClasses[structure] = DataManager.load_sparse_scores(stack, fn=fn, anchor_fn=anchor_fn,\n",
    "                                                                         structure=structure, setting=actual_setting)\n",
    "            except Exception as e:\n",
    "                sys.stderr.write('Patch predictions for %s do not exist.\\n' % structure)\n",
    "\n",
    "        structures = probs_allClasses.keys()\n",
    "                \n",
    "        sys.stderr.write('preprocess: %.2f seconds\\n' % (time.time() - t)) # 3s\n",
    "        \n",
    "        def generate_score_map(structure):\n",
    "\n",
    "            if structure == 'BackG':\n",
    "                return None\n",
    "            \n",
    "            score_matrix = np.zeros((n_sample_x, n_sample_y))\n",
    "            score_matrix[sample_location_indices[:,0], sample_location_indices[:,1]] = probs_allClasses[structure]\n",
    "\n",
    "            spline = RectBivariateSpline(sample_locations_unique_xs/shrink_factor, \n",
    "                                         sample_locations_unique_ys/shrink_factor, \n",
    "                                         score_matrix, \n",
    "                                         bbox=[interpolation_xmin/shrink_factor, \n",
    "                                               interpolation_xmax/shrink_factor, \n",
    "                                               interpolation_ymin/shrink_factor, \n",
    "                                               interpolation_ymax/shrink_factor])\n",
    "\n",
    "            t1 = time.time()\n",
    "            dense_score_map = spline.ev(sample_locations_interpolatedArea_xs_matrix, \n",
    "                                        sample_locations_interpolatedArea_ys_matrix)\n",
    "            sys.stderr.write('evaluate spline: %.2f seconds\\n' % (time.time() - t1)) # 5s for shrink_factor=4; doubling results in quadratic time reduction\n",
    "            \n",
    "            t1 = time.time()\n",
    "            dense_score_map = resize(dense_score_map, (interpolation_h, interpolation_w)) # similar speed as rescale\n",
    "#             dense_score_map = rescale(dense_score_map, shrink_factor)\n",
    "            sys.stderr.write('scale up: %.2f seconds\\n' % (time.time() - t1)) # 10s, very high penalty when multiprocessing\n",
    "\n",
    "#             t = time.time()\n",
    "            dense_score_map[dense_score_map < 1e-1] = 0\n",
    "            dense_score_map[dense_score_map > 1.] = 1.\n",
    "#             sys.stderr.write('threshold: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "            if np.count_nonzero(dense_score_map) < 1e5:\n",
    "                sys.stderr.write('No %s is detected on section %d.\\n' % (structure, sec))\n",
    "                return None\n",
    "            \n",
    "            t1 = time.time()\n",
    "            \n",
    "            scoremap_bp_filepath, scoremap_interpBox_filepath = \\\n",
    "            DataManager.get_scoremap_filepath(stack=stack, fn=fn, anchor_fn=anchor_fn, structure=structure, \n",
    "                                              return_bbox_fp=True, setting=actual_setting)\n",
    "            \n",
    "            save_hdf(dense_score_map.astype(np.float16), scoremap_bp_filepath, complevel=5)\n",
    "#             save_hdf(dense_score_map.astype(np.float16), '/home/yuncong/tmp_%s.hdf' % structure, complevel=5)\n",
    "            np.savetxt(scoremap_interpBox_filepath,\n",
    "                   np.array((interpolation_xmin, interpolation_xmax, interpolation_ymin, interpolation_ymax))[None],\n",
    "                   fmt='%d')\n",
    "    \n",
    "            sys.stderr.write('save: %.2f seconds\\n' % (time.time() - t1)) # 4s, very high penalty when multiprocessing\n",
    "            \n",
    "        \n",
    "        t = time.time()\n",
    "        \n",
    "        # if too many disk saves are simultaneous, they will be conflicting, so split into two sessions\n",
    "#         for i in range(0, len(structures), 8):\n",
    "#             _ = Parallel(n_jobs=16)(delayed(generate_score_map)(l) for l in structures[i:i+15])\n",
    "    \n",
    "#         for l in structures:\n",
    "#             _ = generate_score_map(l)\n",
    "\n",
    "        # Each generate_score_map() takes 20s\n",
    "        # Parallel, expect 20*28/4=140s\n",
    "\n",
    "#         _ = Parallel(n_jobs=15)(delayed(generate_score_map)(l) for l in structures)\n",
    "\n",
    "        pool = Pool(4) # 8 causes contention, resuls in high upscaling and dumping to disk time.\n",
    "        _ = pool.map(generate_score_map, structures)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    " \n",
    "        sys.stderr.write('interpolate: %.2f seconds\\n' % (time.time() - t)) # 80s-150s / section, 8 processes\n",
    "    # 360s (6mins) / section, 4 processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
