{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "Cannot load cv2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for Precision WorkStation\n",
      "rm -rf /media/yuncong/BstemAtlasData/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt && mkdir -p /media/yuncong/BstemAtlasData/CSHL_data_processed/MD635"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "Child returned 0\n",
      "Child returned 1\n",
      "0.67 seconds.\n",
      "File does not exist: /media/yuncong/BstemAtlasData/CSHL_data_processed//MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt\n",
      "File does not exist: /media/yuncong/BstemAtlasData/CSHL_data_processed//MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt\n",
      "Cannot load cv2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt /media/yuncong/BstemAtlasData/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from distributed_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack = 'MD585'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File does not exist: /media/yuncong/BstemAtlasData/CSHL_data_processed/MD585/MD585_transformsTo_anchor.pkl\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/media/yuncong/BstemAtlasData/CSHL_data_processed/MD585/MD585_transformsTo_anchor.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-00602437d222>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabel_gridIndices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocate_annotated_patches_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/yuncong/Brain/utilities/learning_utilities.pyc\u001b[0m in \u001b[0;36mlocate_annotated_patches_v2\u001b[1;34m(stack, grid_spec, sections, surround_margins)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[0mcontours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontours_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontours_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orientation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sagittal'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontours_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'downsample'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[0mcontours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontours\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'section'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'side'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'filename'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'downsample'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'creator'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m     \u001b[0mcontours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_annotation_v3_original_to_aligned_cropped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# filename_to_section, section_to_filename = DataManager.load_sorted_filenames(stack)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yuncong/Brain/utilities/annotation_utilities.pyc\u001b[0m in \u001b[0;36mconvert_annotation_v3_original_to_aligned_cropped\u001b[1;34m(contour_df, stack)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;31m# import cPickle as pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;31m# Ts = pickle.load(open(thumbnail_data_dir + '/%(stack)s/%(stack)s_elastix_output/%(stack)s_transformsTo_anchor.pkl' % dict(stack=stack), 'r'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m     \u001b[0mTs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcnt_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontour_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontour_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orientation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sagittal'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontour_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'downsample'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yuncong/Brain/utilities/data_manager.pyc\u001b[0m in \u001b[0;36mload_transforms\u001b[1;34m(stack, downsample_factor)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transforms_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mTs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mTs_inv_downsampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/yuncong/Brain/utilities/data_manager.pyc\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(filepath, filetype)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pickle'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file_section_map'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/media/yuncong/BstemAtlasData/CSHL_data_processed/MD585/MD585_transformsTo_anchor.pkl'"
     ]
    }
   ],
   "source": [
    "label_gridIndices = locate_annotated_patches_v2(stack=stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_size = 28\n",
    "half_size = patch_size/2\n",
    "stride = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "section_to_filename = metadata_cache['sections_to_filenames'][stack]\n",
    "\n",
    "image_width, image_height = metadata_cache['image_shape'][stack]\n",
    "grid_spec = (patch_size, stride, image_width, image_height)\n",
    "\n",
    "sample_locations = grid_parameters_to_sample_locations(grid_spec=grid_spec)\n",
    "\n",
    "sec = 180\n",
    "\n",
    "# Use grids only on mask.\n",
    "t = time.time()\n",
    "mask_tb = DataManager.load_thumbnail_mask_v2(stack=stack, section=sec)\n",
    "indices_roi = locate_patches_v2(grid_spec=grid_spec, mask_tb=mask_tb)\n",
    "sys.stderr.write('locate patches: %.2f seconds\\n' % (time.time() - t))       \n",
    "\n",
    "#         Use grids on the entire frame.\n",
    "#         indices_roi = range(len(sample_locations))\n",
    "\n",
    "n = len(indices_roi)\n",
    "sys.stderr.write('%d samples.\\n' % n)\n",
    "\n",
    "sample_locations_roi = sample_locations[indices_roi]\n",
    "\n",
    "t = time.time()\n",
    "img_fp = DataManager.get_image_filepath(stack=stack, section=sec, version='cropped_gray', resol='lossless')\n",
    "download_from_s3(img_fp)\n",
    "im = img_as_ubyte(imread(img_fp))\n",
    "sys.stderr.write('load image: %.2f seconds\\n' % (time.time() - t)) # ~ 35s\n",
    "\n",
    "#         t = time.time()\n",
    "#         sat = convert_to_saturation(im)\n",
    "#         del im\n",
    "#         sys.stderr.write('Convert to saturation: %.2f seconds\\n' % (time.time() - t)) # ~ 35s\n",
    "#         sat = imread(DataManager.get_image_filepath(stack=stack, section=sec, version='saturation'))\n",
    "\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "patches = np.array([im[y-half_size:y+half_size, x-half_size:x+half_size]\n",
    "                    for x, y in sample_locations_roi]) # n x 224 x 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_patches = np.array([im[y-half_size:y+half_size, x-half_size:x+half_size]\n",
    "                    for x, y in sample_locations_roi[:300]]) # n x 224 x 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_labels = np.zeros((training_patches.shape[0], 2))\n",
    "training_labels[:training_patches.shape[0]/2, 0] = 1\n",
    "training_labels[training_patches.shape[0]/2:, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/blob/a5d8217c4ed90041bea2616c14a8ddcf11ec8c03/tensorflow/examples/tutorials/mnist/input_data.py#L92\n",
    "class DataSet(object):\n",
    "    \n",
    "    def __init__(self, images, labels):\n",
    "        \"\"\"Construct a DataSet. one_hot arg is used only if fake_data is true.\"\"\"\n",
    "\n",
    "\n",
    "        assert images.shape[0] == labels.shape[0], (\n",
    "          'images.shape: %s labels.shape: %s' % (images.shape,\n",
    "                                                 labels.shape))\n",
    "        self._num_examples = images.shape[0]\n",
    "\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, rows*columns] (assuming depth == 1)\n",
    "        #assert images.shape[3] == 1\n",
    "        images = images.reshape(images.shape[0],\n",
    "                              images.shape[1] * images.shape[2])\n",
    "        # Convert from [0, 255] -> [0.0, 1.0].\n",
    "        images = images.astype(np.float32)\n",
    "        images = images / 255.\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size, fake_data=False):\n",
    "\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set = DataSet(training_patches, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 10000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# # Network Parameters\n",
    "# n_input = 224*224 # MNIST data input (img shape: 28*28)\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded_var.py#L45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "# Input data\n",
    "    images_initializer = tf.placeholder(dtype=data_set.images.dtype,\n",
    "                                        shape=data_set.images.shape)\n",
    "#                                         shape=[None, 784])\n",
    "    labels_initializer = tf.placeholder(dtype=data_set.labels.dtype,\n",
    "                                        shape=data_set.labels.shape)\n",
    "#                                          shape=[None, 2])\n",
    "    input_images = tf.Variable(images_initializer, trainable=False, collections=[])\n",
    "    input_labels = tf.Variable(labels_initializer, trainable=False, collections=[])\n",
    "\n",
    "    image, label = tf.train.slice_input_producer(\n",
    "      [input_images, input_labels], num_epochs=2)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    images, labels = tf.train.batch(\n",
    "      [image, label], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image, label = tf.train.slice_input_producer([input_images, input_labels], num_epochs=10)\n",
    "# label = tf.cast(label, tf.int32)\n",
    "# images, labels = tf.train.batch([image, label], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(images, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(input_images.initializer,\n",
    "             feed_dict={images_initializer: data_set.images})\n",
    "    sess.run(input_labels.initializer,\n",
    "             feed_dict={labels_initializer: data_set.labels})\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "#         batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x, batch_y = data_set.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={images: batch_x, labels: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={images: batch_x,\n",
    "                                                              labels: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={images: data_set.images[:128],\n",
    "                                      labels: data_set.labels[:128],\n",
    "                                      keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
