{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.80252025e-02,   4.67557786e-03,   1.94432028e-02,\n",
       "         2.13614497e-02,   3.16425064e-03,   1.62128229e-02,\n",
       "        -6.66866952e-04,  -2.80861207e-03,  -3.22492816e-03,\n",
       "        -5.51616680e-03,  -9.06719640e-03,  -3.54129998e-06,\n",
       "         5.71480673e-03,  -5.72868949e-03,  -5.52872987e-03,\n",
       "        -1.00020496e-02], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_rootdir = '/home/yuncong/CSHL_data_patches/'\n",
    "\n",
    "model_dir = '/home/yuncong/mxnet_models/'\n",
    "# model_name='inception-stage1'\n",
    "# model_name = 'experiment0227'\n",
    "# model_iteration = 6\n",
    "\n",
    "model_name = 'experiment0317'\n",
    "model_iteration = 6\n",
    "\n",
    "model = mx.model.FeedForward.load(os.path.join(model_dir, model_name), model_iteration, ctx=mx.gpu())\n",
    "\n",
    "model.arg_params['fullc_bias'].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fc_output = model.symbol.get_internals()['fc_output']\n",
    "flatten_output = model.symbol.get_internals()['flatten_output']\n",
    "# fc_output = model.symbol.get_internals()['fullc_output']\n",
    "sm_output = model.symbol.get_internals()['softmax_output']\n",
    "grouped_output = mx.symbol.Group([flatten_output, sm_output])\n",
    "\n",
    "model = mx.model.FeedForward(ctx=mx.gpu(), symbol=grouped_output, num_epoch=model_iteration,\n",
    "                            arg_params=model.arg_params, aux_params=model.aux_params,\n",
    "                            allow_extra_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels =  ['BackG', '5N', '7n', '7N', '12N', 'Gr', 'LVe', 'Pn', 'SuVe', 'VLL']\n",
    "\n",
    "# labels = ['BackG', '5N', '7n', '7N', '12N', 'Gr', 'LVe', 'Pn', 'SuVe', 'VLL', \n",
    "#                      '6N', 'Amb', 'R', 'Tz', 'Sol', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "labels = ['BackG', '5N', '7n', '7N', '12N', 'Pn', 'VLL', \n",
    "          '6N', 'Amb', 'R', 'Tz', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "label_dict = dict([(l,i) for i, l in enumerate(labels)])\n",
    "\n",
    "# label_dict = dict([(l,i) for i, l in enumerate(labels)] + \\\n",
    "#                   zip(other_labels, range(len(labels), len(labels)+len(other_labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import RectBivariateSpline\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scoremaps_rootdir = '/home/yuncong/CSHL_scoremaps_lossless'\n",
    "# if not os.path.exists(scoremaps_rootdir):\n",
    "#     os.makedirs(scoremaps_rootdir)\n",
    "    \n",
    "predictions_rootdir = '/home/yuncong/CSHL_patch_predictions'\n",
    "if not os.path.exists(predictions_rootdir):\n",
    "    os.makedirs(predictions_rootdir)\n",
    "\n",
    "# iterators_rootdir = '/home/yuncong/CSHL_mxnet_iterators'\n",
    "# if not os.path.exists(iterators_rootdir):\n",
    "#     os.makedirs(iterators_rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_img = mx.nd.load(os.path.join(model_dir, 'mean_224.nd'))['mean_img'].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "progress_bar = FloatProgress(min=0, max=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "19064 samples\n",
      "151\n",
      "19328"
     ]
    }
   ],
   "source": [
    "# for stack in ['MD585', 'MD593', 'MD592', 'MD590', 'MD591', 'MD595', 'MD598', 'MD602', 'MD594']:\n",
    "# for stack in ['MD591']:\n",
    "# for stack in ['MD591', 'MD595', 'MD598', 'MD602', 'MD594']:\n",
    "for stack in ['MD589']:\n",
    "# for stack in ['MD589']:\n",
    "\n",
    "    dm = DataManager(stack=stack)\n",
    "    \n",
    "    first_bs_sec, last_bs_sec = section_range_lookup[stack]\n",
    "    first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "    \n",
    "    progress_bar.min = first_detect_sec\n",
    "    progress_bar.max = last_detect_sec\n",
    "    display(progress_bar)\n",
    "    \n",
    "    table_filepath = os.path.join(patches_rootdir, '%(stack)s_indices_allROIs_allSections.h5'%{'stack':stack})\n",
    "    indices_allROIs_allSections = pd.read_hdf(table_filepath, 'indices_allROIs_allSections')\n",
    "    grid_parameters = pd.read_hdf(table_filepath, 'grid_parameters')\n",
    "\n",
    "    if stack in ['MD589', 'MD594']:\n",
    "        stack_has_annotation = True\n",
    "    else:\n",
    "        stack_has_annotation = False\n",
    "\n",
    "    if stack_has_annotation:\n",
    "        table_filepath = os.path.join(patches_rootdir, '%(stack)s_indices_allLandmarks_allSections.h5'%{'stack':stack})\n",
    "        indices_allLandmarks_allSections = pd.read_hdf(table_filepath, 'indices_allLandmarks_allSections')\n",
    "    \n",
    "    for sec in range(first_detect_sec, last_detect_sec+1):\n",
    "#     for sec in range(first_detect_sec, first_detect_sec+1):\n",
    "        \n",
    "        if sec not in indices_allROIs_allSections.columns:\n",
    "            continue\n",
    "\n",
    "        print sec\n",
    "\n",
    "        progress_bar.value = sec\n",
    "\n",
    "        indices_roi = indices_allROIs_allSections[sec]['roi1']\n",
    "\n",
    "        predictions_dir = os.path.join(predictions_rootdir, stack, '%04d'%sec)\n",
    "        if not os.path.exists(predictions_dir):\n",
    "            os.makedirs(predictions_dir)\n",
    "\n",
    "        ## define grid, generate patches\n",
    "\n",
    "        t = time.time()\n",
    "        \n",
    "        dm.set_slice(sec)\n",
    "        dm._load_image(['rgb-jpg'])\n",
    "\n",
    "        patch_size, stride, w, h = grid_parameters.tolist()\n",
    "        half_size = patch_size/2\n",
    "\n",
    "        ys, xs = np.meshgrid(np.arange(half_size, h-half_size, stride), np.arange(half_size, w-half_size, stride),\n",
    "                         indexing='xy')\n",
    "\n",
    "        sample_locations = np.c_[xs.flat, ys.flat]\n",
    "        sample_locations_roi = sample_locations[indices_roi]\n",
    "\n",
    "        n = len(indices_roi)\n",
    "        print n, 'samples'\n",
    "        \n",
    "        patches = np.asarray([dm.image_rgb_jpg[y-half_size:y+half_size, x-half_size:x+half_size]\n",
    "                              for x, y in sample_locations_roi])\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        patches = np.rollaxis(patches, 3, 1)\n",
    "        patches = patches - mean_img\n",
    "\n",
    "        dataset = '%(stack)s_%(sec)04d_roi1' % {'stack': stack, 'sec': sec}\n",
    "\n",
    "        true_labels = -1 * np.ones((99999,), np.int)\n",
    "        if stack_has_annotation:\n",
    "            if sec in indices_allLandmarks_allSections:\n",
    "                for l in indices_allLandmarks_allSections[sec].dropna().keys():\n",
    "                    if l == 'bg': continue\n",
    "                    if l.endswith('surround'):\n",
    "                        true_labels[indices_allLandmarks_allSections[sec][l]] = 0\n",
    "                    else:\n",
    "                        if l in labels:\n",
    "                            true_labels[indices_allLandmarks_allSections[sec][l]] = label_dict[l]\n",
    "                        else:\n",
    "                            true_labels[indices_allLandmarks_allSections[sec][l]] = 0\n",
    "    \n",
    "        patch_labels = true_labels[indices_roi]\n",
    "                \n",
    "        np.save(predictions_dir+'/%(dataset)s_patchLabels.npy' % {'dataset': dataset}, patch_labels)\n",
    "        \n",
    "#         sys.stderr.write('generating patches: %.2f seconds\\n' % (time.time() - t))\n",
    "                \n",
    "        # feed the network\n",
    "        \n",
    "#         continue\n",
    "#         t = time.time()\n",
    "        \n",
    "        batch_size = 512 # increasing to 892 does not save any time\n",
    "        \n",
    "        test_iter = mx.io.NDArrayIter(\n",
    "            patches, \n",
    "            patch_labels,\n",
    "            batch_size = batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "#         sys.stderr.write('load iterator: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        t = time.time()\n",
    "        \n",
    "        output = model.predict(test_iter)\n",
    "        \n",
    "        features = output[0]\n",
    "        probs = output[1] # 0 for flatteed features, 1 for softmax output\n",
    "        \n",
    "        bp.pack_ndarray_file(features, predictions_dir + '/%(dataset)s_features.bp'% {'dataset': dataset})\n",
    "#         np.save(predictions_dir + '/%(dataset)s_features.npy'% {'dataset': dataset}, features)\n",
    "        np.save(predictions_dir + '/%(dataset)s_predictions.npy'% {'dataset': dataset}, probs)\n",
    "        \n",
    "        # probs = np.load(predictions_dir + '/%(dataset)s_predictions.npy'% {'dataset': dataset})\n",
    "        sys.stderr.write('predict: %.2f seconds\\n' % (time.time() - t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(probs, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use RecordIO - dump to images, pack into records, then use ImageRecordIte - 80s for gen patch, 80s for packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for stack in ['MD585', 'MD593', 'MD592', 'MD590', 'MD591', 'MD595', 'MD598', 'MD602']:\n",
    "for stack in ['MD585']:\n",
    "\n",
    "    first_bs_sec, last_bs_sec = section_range_lookup[stack]\n",
    "\n",
    "    table_filepath = os.path.join(patches_rootdir, '%(stack)s_indices_allROIs_allSections.h5'%{'stack':stack})\n",
    "    indices_allROIs_allSections = pd.read_hdf(table_filepath, 'indices_allROIs_allSections')\n",
    "    grid_parameters = pd.read_hdf(table_filepath, 'grid_parameters')\n",
    "\n",
    "    first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "    stack_has_annotation = False\n",
    "\n",
    "    if stack_has_annotation:\n",
    "        table_filepath = os.path.join(patches_rootdir, '%(stack)s_indices_allLandmarks_allSections.h5'%{'stack':stack})\n",
    "        indices_allLandmarks_allSections = pd.read_hdf(table_filepath, 'indices_allLandmarks_allSections')\n",
    "\n",
    "#     for sec in range(first_detect_sec, last_detect_sec+1):\n",
    "    for sec in [first_detect_sec]:\n",
    "\n",
    "        if sec not in indices_allROIs_allSections.columns:\n",
    "            continue\n",
    "\n",
    "        print sec\n",
    "\n",
    "        indices_roi = indices_allROIs_allSections[sec]['roi1']\n",
    "\n",
    "        predictions_dir = os.path.join(predictions_rootdir, stack, '%04d'%sec)\n",
    "        if not os.path.exists(predictions_dir):\n",
    "            os.makedirs(predictions_dir)\n",
    "\n",
    "        iterators_dir = os.path.join(iterators_rootdir, stack, '%04d'%sec)\n",
    "        if not os.path.exists(iterators_dir):\n",
    "            os.makedirs(iterators_dir)\n",
    "\n",
    "        scoremaps_dir = os.path.join(scoremaps_rootdir, stack, '%04d'%sec)\n",
    "        if not os.path.exists(scoremaps_dir):\n",
    "            os.makedirs(scoremaps_dir)\n",
    "\n",
    "        patches_dir = os.path.join(patches_rootdir, '%s_byROI/%04d/roi1'%(stack, sec))\n",
    "        if not os.path.exists(patches_dir):\n",
    "            os.makedirs(patches_dir)\n",
    "\n",
    "        ## define grid, generate patches\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        dm = DataManager(stack=stack, section=sec)\n",
    "        dm._load_image(['rgb-jpg'])\n",
    "\n",
    "        patch_size, stride, w, h = grid_parameters.tolist()\n",
    "        half_size = patch_size/2\n",
    "\n",
    "        ys, xs = np.meshgrid(np.arange(half_size, h-half_size, stride), np.arange(half_size, w-half_size, stride),\n",
    "                         indexing='xy')\n",
    "\n",
    "        sample_locations = np.c_[xs.flat, ys.flat]\n",
    "        sample_locations_roi = sample_locations[indices_roi]\n",
    "\n",
    "        print len(indices_roi), 'samples'\n",
    "\n",
    "#         for i, (x,y) in zip(indices_roi, sample_locations_roi):\n",
    "#             patch = dm.image_rgb_jpg[y-half_size:y+half_size, x-half_size:x+half_size]\n",
    "#             cv2.imwrite(os.path.join(patches_dir, '%(stack)s_%(sec)04d_%(gridp_ind)08d.jpg' %\\\n",
    "#                                      {'stack':stack, 'sec':sec, 'gridp_ind':i}), \n",
    "#                         patch[..., ::-1])\n",
    "\n",
    "        sys.stderr.write('generating patches: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        #     ## predict for the patches\n",
    "\n",
    "        dataset = '%(stack)s_%(sec)04d_roi1' % {'stack': stack, 'sec': sec}\n",
    "\n",
    "        img_filenames = sorted(os.listdir(patches_dir))\n",
    "\n",
    "        true_labels = -1 * np.ones((99999,), np.int)\n",
    "        if stack_has_annotation:\n",
    "            if sec in indices_allLandmarks_allSections:\n",
    "                for l in indices_allLandmarks_allSections[sec].dropna().keys():\n",
    "                    if l == 'bg': continue\n",
    "                    if l.endswith('surround'):\n",
    "                        true_labels[indices_allLandmarks_allSections[sec][l]] = 0\n",
    "                    else:\n",
    "                        true_labels[indices_allLandmarks_allSections[sec][l]] = label_dict[l]\n",
    "\n",
    "        with open(iterators_dir + '/%(dataset)s_test.lst' % {'dataset': dataset}, 'w') as lst_file:\n",
    "            for index, img_filename in enumerate(img_filenames):\n",
    "                grid_index = int(img_filename[:-4].split('_')[-1])\n",
    "                lst_file.write('%s\\t%d\\t%s\\n'%(index, true_labels[grid_index], img_filename))\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        cmd_gen_iterator = os.environ['MXNET_DIR']+'/bin/im2rec %(iterators_dir)s/%(dataset)s_test.lst \\\n",
    "        %(data_dir)s/ %(iterators_dir)s/%(dataset)s_test.rec' % \\\n",
    "                  {'dataset': dataset, 'data_dir': patches_dir, 'iterators_dir': iterators_dir}\n",
    "        # note: the / after data_dir is necessary\n",
    "\n",
    "#         os.system(cmd_gen_iterator)\n",
    "\n",
    "        sys.stderr.write('generating iterators: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        # feed the network\n",
    "        batch_size = 256\n",
    "\n",
    "        t = time.time()\n",
    "        \n",
    "        test_iter = mx.io.ImageRecordIter(\n",
    "            path_imgrec = iterators_dir + '/%(dataset)s_test.rec' % {'dataset': dataset},\n",
    "            batch_size = batch_size,\n",
    "            data_shape = (3, 224, 224),\n",
    "            mean_img = os.path.join(model_dir, 'mean_224.nd'),\n",
    "        )\n",
    "\n",
    "        sys.stderr.write('load iterators: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        \n",
    "        t = time.time()\n",
    "        probs = model.predict(test_iter)\n",
    "        np.save(predictions_dir + '/%(dataset)s_predictions.npy'% {'dataset': dataset}, probs)\n",
    "        # probs = np.load(predictions_dir + '/%(dataset)s_predictions.npy'% {'dataset': dataset})\n",
    "        sys.stderr.write('predict: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        break\n",
    "        \n",
    "        ## interpolate\n",
    "\n",
    "        interpolation_xmin, interpolation_ymin = sample_locations_roi.min(axis=0)\n",
    "        interpolation_xmax, interpolation_ymax = sample_locations_roi.max(axis=0)\n",
    "        interpolation_w = interpolation_xmax - interpolation_xmin + 1\n",
    "        interpolation_h = interpolation_ymax - interpolation_ymin + 1\n",
    "\n",
    "        ##### sample_locations_roi + scores to dense_score_map #####\n",
    "\n",
    "        shrink_factor = 4\n",
    "\n",
    "        sample_locations_unique_xs = np.unique(sample_locations_roi[:,0])\n",
    "        sample_locations_unique_ys = np.unique(sample_locations_roi[:,1])\n",
    "\n",
    "        n_sample_x = sample_locations_unique_xs.size\n",
    "        n_sample_y = sample_locations_unique_ys.size\n",
    "\n",
    "        index_x = dict([(j,i) for i,j in enumerate(sample_locations_unique_xs)])\n",
    "        index_y = dict([(j,i) for i,j in enumerate(sample_locations_unique_ys)])\n",
    "        sample_location_indices = np.asarray([(index_x[x], index_y[y]) for x, y in sample_locations_roi])\n",
    "\n",
    "        sample_locations_interpolatedArea_ys_matrix, \\\n",
    "        sample_locations_interpolatedArea_xs_matrix = np.meshgrid(range(interpolation_ymin/shrink_factor, \n",
    "                                                                        interpolation_ymax/shrink_factor), \n",
    "                                                                  range(interpolation_xmin/shrink_factor, \n",
    "                                                                        interpolation_xmax/shrink_factor), \n",
    "                                                                  indexing='ij')\n",
    "\n",
    "\n",
    "        def generate_score_map(l):\n",
    "\n",
    "            label = labels[(l+1)%len(labels)]\n",
    "            if label == 'BackG':\n",
    "                return\n",
    "\n",
    "            score_matrix = np.zeros((n_sample_x, n_sample_y))\n",
    "            score_matrix[sample_location_indices[:,0], sample_location_indices[:,1]] = probs[:,l]\n",
    "\n",
    "            spline = RectBivariateSpline(sample_locations_unique_xs/shrink_factor, \n",
    "                                         sample_locations_unique_ys/shrink_factor, \n",
    "                                         score_matrix, \n",
    "                                         bbox=[interpolation_xmin/shrink_factor, \n",
    "                                               interpolation_xmax/shrink_factor, \n",
    "                                               interpolation_ymin/shrink_factor, \n",
    "                                               interpolation_ymax/shrink_factor])\n",
    "\n",
    "            dense_score_map = spline.ev(sample_locations_interpolatedArea_xs_matrix, \n",
    "                                        sample_locations_interpolatedArea_ys_matrix)\n",
    "\n",
    "            dense_score_map = resize(dense_score_map, (interpolation_h, interpolation_w))\n",
    "\n",
    "\n",
    "            dense_score_map[dense_score_map < 1e-2] = 0\n",
    "            dense_score_map_lossless = np.pad(dense_score_map, ((interpolation_ymin, h-interpolation_ymax-1), \n",
    "                                                  (interpolation_xmin, w-interpolation_xmax-1)),\n",
    "                                              mode='constant', constant_values=0)\n",
    "            \n",
    "            if np.count_nonzero(score_map[::8, ::8] > .1) < 1e3:\n",
    "                sys.stderr.write('No %s is detected on section %d\\n' % (label, sec))\n",
    "            \n",
    "#             bp.pack_ndarray_file(dense_score_map_lossless, \n",
    "#                                    os.path.join(scoremaps_dir, '%(dataset)s_denseScoreMapLossless_%(label)s.bp' % \\\n",
    "#                                                 {'dataset': dataset, 'label': label}))\n",
    "\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        n_labels = probs.shape[1]\n",
    "\n",
    "        _ = Parallel(n_jobs=12)(delayed(generate_score_map)(l) for l in range(n_labels))\n",
    "\n",
    "        sys.stderr.write('interpolate: %.2f seconds\\n' % (time.time() - t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
