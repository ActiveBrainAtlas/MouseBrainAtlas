{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuncong/Brain/utilities/utilities2015.py:2: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-fc4079e49eb7>\", line 14, in <module>\n",
      "    get_ipython().magic(u'matplotlib inline')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 308, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for Precision WorkStation\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD585/MD585_prep2_thumbnail/MD585-N16-2015.07.16-20.32.33_MD585_2_0047_prep2_thumbnail.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "No vtkNot using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD589/MD589_prep2_thumbnail/MD589-N16-2015.07.30-17.03.43_MD589_3_0048_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD590/MD590_prep2_thumbnail/MD590-IHC17-2015.08.10-19.09.09_MD590_2_0050_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD591/MD591_prep2_thumbnail/MD591-IHC17-2015.08.28-04.23.47_MD591_1_0049_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD592/MD592_prep2_thumbnail/MD592-N16-2015.08.22-00.50.39_MD592_2_0047_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD593/MD593_prep2_thumbnail/MD593-N15-2015.08.21-16.53.02_MD593_3_0045_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD594/MD594_prep2_thumbnail/MD594-IHC16-2015.08.26-16.11.01_MD594_1_0046_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD595/MD595_prep2_thumbnail/MD595-IHC12-2015.09.15-01.21.39_MD595_2_0035_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD598/MD598_prep2_thumbnail/MD598-N18-2015.09.29-17.40.03_MD598_3_0054_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD599/MD599_prep2_thumbnail/MD599-N19-2015.10.02-18.12.13_MD599_3_0057_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD602/MD602_prep2_thumbnail/MD602-N19-2015.12.01-16.24.09_MD602_2_0056_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD603/MD603_prep2_thumbnail/MD603-N11-2016.03.02-12.51.47_MD603_1_0031_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD635/MD635_prep2_thumbnail/MD635-F25-2016.05.18-21.02.30_MD635_3_0075_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD653/MD653_prep2_thumbnail/MD653-F15-2016.12.20-21.04.12_MD653_1_0043_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD652/MD652_prep2_thumbnail/MD652-F15-2016.12.16-20.45.52_MD652_3_0045_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD642/MD642_prep2_thumbnail/MD642-N14-2017.01.18-16.25.58_MD642_2_0041_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD657/MD657_prep2_thumbnail/MD657-N13-2017.02.22-14.33.34_MD657_3_0039_prep2_thumbnail.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "Not using image_cache.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD658/MD658_prep2_thumbnail/MD658-N18-2017.03.31-17.34.22_MD658_2_0053_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD661/MD661_prep2_thumbnail/MD662&661-F30-2017.06.05-14.49.48_MD661_1_0088_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/MD662/MD662_prep2_thumbnail/MD662&661-F35-2017.06.05-17.08.18_MD662_1_0103_prep2_thumbnail.tif\n",
      "/media/yuncong/BstemAtlasData/CSHL_data_processed/ChatCryoJane201710/ChatCryoJane201710_prep2_thumbnail_Ntb/ChatCryoJane201710_slide1-S1_prep2_thumbnail_Ntb.tif\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import mxnet as mx\n",
    "except:\n",
    "    sys.stderr.write(\"Cannot import mxnet.\\n\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import rotate\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from distributed_utilities import *\n",
    "from visualization_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "sys.path.append('/home/yuncong/csd395/xgboost/python-package')\n",
    "try:\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "except:\n",
    "    sys.stderr.write('xgboost is not loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:53: UserWarning: \u001b[91mYou created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\n",
      "\tdata\u001b[0m\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:65: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "model_dir_name = 'inception-bn-blue'\n",
    "model_name = 'inception-bn-blue'\n",
    "model, mean_img = load_mxnet_model(model_dir_name=model_dir_name, model_name=model_name, \n",
    "                                   num_gpus=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stacks = ['MD589', 'MD585']\n",
    "test_stacks = ['MD594']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_stain = {'MD585': 'N', 'MD589': 'N', 'MD594': 'N'}\n",
    "\n",
    "# Number of sections on which to sample examples from.\n",
    "stack_section_number = defaultdict(dict)\n",
    "\n",
    "for name_u in all_known_structures:\n",
    "    for st in train_stacks:\n",
    "        stack_section_number[st][name_u] = 99\n",
    "#         if name_u == '4N' or name_u == '10N':\n",
    "#             train_stack_section_number[st][name_u] = 20\n",
    "#         else:\n",
    "#             train_stack_section_number[st][name_u] = 10\n",
    "#         train_stack_section_number[st][name_u] = 10\n",
    "    for st in test_stacks:\n",
    "        stack_section_number[st][name_u] = 99\n",
    "\n",
    "stack_section_number.default_factory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "win_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive \"s3://mousebrainatlas-data/CSHL_labelings_v3/MD589\" \"/home/yuncong/CSHL_labelings_v3/MD589\" --exclude \"*\" --include \"*win7*grid_indices_lookup*\"\n",
      "latest timestamp:  10042017100807\n",
      "aws s3 cp --recursive \"s3://mousebrainatlas-data/CSHL_labelings_v3/MD585\" \"/home/yuncong/CSHL_labelings_v3/MD585\" --exclude \"*\" --include \"*win7*grid_indices_lookup*\"\n",
      "latest timestamp:  08012017212649\n",
      "aws s3 cp --recursive \"s3://mousebrainatlas-data/CSHL_labelings_v3/MD594\" \"/home/yuncong/CSHL_labelings_v3/MD594\" --exclude \"*\" --include \"*win7*grid_indices_lookup*\"\n",
      "latest timestamp:  07302017183604\n"
     ]
    }
   ],
   "source": [
    "grid_indices_lookup_allStacks = {}\n",
    "\n",
    "for stack in train_stacks + test_stacks:\n",
    "    try:\n",
    "#         grid_indices_lookup_allStacks[stack] = \\\n",
    "#         DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "#                                                            by_human=False, timestamp='latest',\n",
    "#                                                            detector_id_f=1,\n",
    "#                                                           return_locations=True)            \n",
    "\n",
    "        grid_indices_lookup_allStacks[stack] = \\\n",
    "        DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "                                                           by_human=True, timestamp='latest',\n",
    "                                                          return_locations=True)            \n",
    "\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        sys.stderr.write(\"Fail to load annotation grid lookup for %s.\\n\" % stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "all_labels = sorted(list(set(chain.from_iterable(set(grid_indices_lookup_allStacks[st].columns.tolist()) \n",
    "                                                 for st in train_stacks + test_stacks))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_addresses(stacks, structure):\n",
    "        \n",
    "    positive_labels = [structure]\n",
    "    negative_labels = get_negative_labels(structure, 'neg_has_all_surround', \n",
    "                                          margin_um=500, labels_found=all_labels)\n",
    "\n",
    "    positive_addresses_all_stacks = {}\n",
    "    negative_addresses_all_stacks = {}\n",
    "        \n",
    "    for stack in stacks:\n",
    "\n",
    "        candidate_sections = list(chain(*[grid_indices_lookup_allStacks[stack][pl].dropna(how='any').index.tolist() \n",
    "                                      for pl in positive_labels]))\n",
    "\n",
    "        n_sections = stack_section_number[stack][structure]\n",
    "\n",
    "        if stack_stain[stack] == 'F':\n",
    "            neurotrace_sections = []\n",
    "            nissl_sections = []\n",
    "            for sec in candidate_sections:\n",
    "                if metadata_cache['sections_to_filenames'][stack][sec].split('-')[1][0] == 'F':\n",
    "                    neurotrace_sections.append(sec)\n",
    "                else:\n",
    "                    nissl_sections.append(sec)\n",
    "            sampled_sections = np.random.choice(neurotrace_sections, min(len(neurotrace_sections), n_sections), replace=False)\n",
    "        else:\n",
    "            sampled_sections = np.random.choice(candidate_sections, min(len(candidate_sections), n_sections), replace=False)\n",
    "\n",
    "        positive_addresses_all_stacks[stack] = sorted([(stack, sec, tuple(loc))\n",
    "for nl in set(positive_labels) & set(grid_indices_lookup_allStacks[stack].columns)\n",
    "  for sec, locs in grid_indices_lookup_allStacks[stack][nl].loc[sampled_sections].dropna().iteritems()\n",
    "  for loc in locs])\n",
    "\n",
    "        negative_addresses_all_stacks[stack] = sorted([(stack, sec, tuple(loc))\n",
    "for nl in set(negative_labels) & set(grid_indices_lookup_allStacks[stack].columns)\n",
    "  for sec, locs in grid_indices_lookup_allStacks[stack][nl].loc[sampled_sections].dropna().iteritems()\n",
    "  for loc in locs])\n",
    "\n",
    "    positive_addresses = sum(positive_addresses_all_stacks.values(), [])\n",
    "    negative_addresses = sum(negative_addresses_all_stacks.values(), [])\n",
    "\n",
    "    del positive_addresses_all_stacks, negative_addresses_all_stacks\n",
    "\n",
    "    return positive_addresses, negative_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_scheme = 'stretch_min_max'\n",
    "# train_scheme = 'normalize_mu_sigma_global_(-1,5)'\n",
    "train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,5)'\n",
    "# train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,9)'\n",
    "# train_scheme = 'none'\n",
    "\n",
    "# test_scheme = 'stretch_min_max'\n",
    "# test_scheme = 'normalize_mu_sigma_global_(-1,5)'\n",
    "test_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,5)'\n",
    "# test_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,9)'\n",
    "# test_scheme = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# existing_classifier_id = 998 # If set, not train from scratch\n",
    "existing_classifier_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier_id = 998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_dict = {(scheme, tfv): {} for scheme in schemes for tfv in transforms}\n",
    "features_dict = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# positive train = 32694\n",
      "# negative train = 239050\n",
      "('MD589', 179)\n",
      "('MD585', 163)\n",
      "('MD589', 316)\n",
      "('MD589', 166)\n",
      "('MD589', 303)\n",
      "('MD589', 153)\n",
      "('MD585', 156)\n",
      "('MD589', 290)\n",
      "('MD589', 193)\n",
      "('MD585', 143)\n",
      "('MD589', 277)\n",
      "('MD585', 272)\n",
      "('MD589', 189)\n",
      "('MD585', 259)\n",
      "('MD585', 181)\n",
      "('MD589', 176)\n",
      "('MD585', 160)\n",
      "('MD589', 313)\n",
      "('MD589', 163)\n",
      "('MD585', 292)\n",
      "('MD585', 145)\n",
      "('MD589', 287)\n",
      "('MD585', 282)\n",
      "('MD585', 140)\n",
      "('MD589', 274)\n",
      "('MD585', 277)\n",
      "('MD589', 186)\n",
      "('MD585', 256)\n",
      "('MD585', 170)\n",
      "('MD589', 173)\n",
      "('MD585', 165)\n",
      "('MD589', 310)\n",
      "('MD589', 160)\n",
      "('MD585', 155)\n",
      "('MD589', 297)\n",
      "('MD585', 150)\n",
      "('MD589', 284)\n",
      "('MD585', 287)\n",
      "('MD585', 254)\n",
      "('MD585', 266)\n",
      "('MD589', 183)\n",
      "('MD585', 261)\n",
      "('MD585', 175)\n",
      "('MD589', 170)\n",
      "('MD589', 307)\n",
      "('MD589', 157)\n",
      "('MD585', 291)\n",
      "('MD585', 152)\n",
      "('MD589', 294)\n",
      "('MD589', 197)\n",
      "('MD589', 281)\n",
      "('MD585', 284)\n",
      "('MD585', 271)\n",
      "('MD585', 177)\n",
      "('MD589', 180)\n",
      "('MD585', 172)\n",
      "('MD589', 317)\n",
      "('MD589', 167)\n",
      "('MD589', 304)\n",
      "('MD589', 154)\n",
      "('MD585', 288)\n",
      "('MD585', 157)\n",
      "('MD589', 291)\n",
      "('MD589', 194)\n",
      "('MD589', 278)\n",
      "('MD585', 273)\n",
      "('MD589', 190)\n",
      "('MD585', 268)\n",
      "('MD585', 182)\n",
      "('MD589', 177)\n",
      "('MD585', 161)\n",
      "('MD589', 314)\n",
      "('MD589', 164)\n",
      "('MD589', 301)\n",
      "('MD585', 293)\n",
      "('MD585', 146)\n",
      "('MD589', 288)\n",
      "('MD585', 283)\n",
      "('MD585', 141)\n",
      "('MD589', 275)\n",
      "('MD585', 278)\n",
      "('MD589', 187)\n",
      "('MD585', 257)\n",
      "('MD585', 171)\n",
      "('MD589', 174)\n",
      "('MD585', 166)\n",
      "('MD589', 311)\n",
      "('MD589', 161)\n",
      "('MD589', 298)\n",
      "('MD585', 151)\n",
      "('MD589', 285)\n",
      "('MD585', 280)\n",
      "('MD585', 255)\n",
      "('MD585', 267)\n",
      "('MD589', 184)\n",
      "('MD585', 262)\n",
      "('MD585', 168)\n",
      "('MD589', 171)\n",
      "('MD589', 308)\n",
      "('MD589', 158)\n",
      "('MD585', 153)\n",
      "('MD589', 295)\n",
      "('MD585', 148)\n",
      "('MD589', 282)\n",
      "('MD585', 285)\n",
      "('MD585', 252)\n",
      "('MD585', 264)\n",
      "('MD585', 178)\n",
      "('MD589', 181)\n",
      "('MD585', 173)\n",
      "('MD589', 168)\n",
      "('MD589', 305)\n",
      "('MD589', 155)\n",
      "('MD585', 289)\n",
      "('MD585', 158)\n",
      "('MD589', 292)\n",
      "('MD589', 195)\n",
      "('MD589', 279)\n",
      "('MD585', 274)\n",
      "('MD589', 191)\n",
      "('MD585', 269)\n",
      "('MD589', 178)\n",
      "('MD585', 162)\n",
      "('MD589', 315)\n",
      "('MD589', 165)\n",
      "('MD589', 302)\n",
      "('MD589', 152)\n",
      "('MD585', 294)\n",
      "('MD585', 147)\n",
      "('MD589', 289)\n",
      "('MD589', 192)\n",
      "('MD585', 142)\n",
      "('MD589', 276)\n",
      "('MD585', 279)\n",
      "('MD589', 188)\n",
      "('MD585', 258)\n",
      "('MD585', 180)\n",
      "('MD589', 175)\n",
      "('MD585', 167)\n",
      "('MD589', 312)\n",
      "('MD589', 162)\n",
      "('MD589', 299)\n",
      "('MD585', 144)\n",
      "('MD589', 286)\n",
      "('MD585', 281)\n",
      "('MD585', 276)\n",
      "('MD589', 185)\n",
      "('MD585', 263)\n",
      "('MD585', 169)\n",
      "('MD589', 172)\n",
      "('MD585', 164)\n",
      "('MD589', 309)\n",
      "('MD589', 159)\n",
      "('MD585', 154)\n",
      "('MD589', 296)\n",
      "('MD585', 149)\n",
      "('MD589', 283)\n",
      "('MD585', 286)\n",
      "('MD585', 253)\n",
      "('MD585', 265)\n",
      "('MD585', 179)\n",
      "('MD589', 182)\n",
      "('MD585', 260)\n",
      "('MD585', 174)\n",
      "('MD589', 169)\n",
      "('MD589', 306)\n",
      "('MD589', 156)\n",
      "('MD585', 290)\n",
      "('MD585', 159)\n",
      "('MD589', 293)\n",
      "('MD589', 196)\n",
      "('MD589', 280)\n",
      "('MD585', 275)\n",
      "('MD585', 270)\n",
      "('MD585', 176)\n",
      "# positive test = 17906\n",
      "# negative test = 120621\n",
      "('MD594', 163)\n",
      "('MD594', 276)\n",
      "('MD594', 282)\n",
      "('MD594', 281)\n",
      "('MD594', 184)\n",
      "('MD594', 155)\n",
      "('MD594', 177)\n",
      "('MD594', 294)\n",
      "('MD594', 175)\n",
      "('MD594', 272)\n",
      "('MD594', 303)\n",
      "('MD594', 270)\n",
      "('MD594', 164)\n",
      "('MD594', 293)\n",
      "('MD594', 189)\n",
      "('MD594', 284)\n",
      "('MD594', 171)\n",
      "('MD594', 156)\n",
      "('MD594', 266)\n",
      "('MD594', 160)\n",
      "('MD594', 277)\n",
      "('MD594', 302)\n",
      "('MD594', 185)\n",
      "('MD594', 301)\n",
      "('MD594', 152)\n",
      "('MD594', 182)\n",
      "('MD594', 172)\n",
      "('MD594', 273)\n",
      "('MD594', 298)\n",
      "('MD594', 271)\n",
      "('MD594', 165)\n",
      "('MD594', 288)\n",
      "('MD594', 283)\n",
      "('MD594', 178)\n",
      "('MD594', 168)\n",
      "('MD594', 157)\n",
      "('MD594', 267)\n",
      "('MD594', 295)\n",
      "('MD594', 161)\n",
      "('MD594', 286)\n",
      "('MD594', 305)\n",
      "('MD594', 190)\n",
      "('MD594', 153)\n",
      "('MD594', 194)\n",
      "('MD594', 183)\n",
      "('MD594', 285)\n",
      "('MD594', 173)\n",
      "('MD594', 278)\n",
      "('MD594', 268)\n",
      "('MD594', 186)\n",
      "('MD594', 280)\n",
      "('MD594', 179)\n",
      "('MD594', 169)\n",
      "('MD594', 274)\n",
      "('MD594', 166)\n",
      "('MD594', 279)\n",
      "('MD594', 296)\n",
      "('MD594', 191)\n",
      "('MD594', 289)\n",
      "('MD594', 158)\n",
      "('MD594', 195)\n",
      "('MD594', 180)\n",
      "('MD594', 299)\n",
      "('MD594', 162)\n",
      "('MD594', 151)\n",
      "('MD594', 269)\n",
      "('MD594', 292)\n",
      "('MD594', 287)\n",
      "('MD594', 154)\n",
      "('MD594', 290)\n",
      "('MD594', 176)\n",
      "('MD594', 174)\n",
      "('MD594', 275)\n",
      "('MD594', 265)\n",
      "('MD594', 304)\n",
      "('MD594', 167)\n",
      "('MD594', 291)\n",
      "('MD594', 300)\n",
      "('MD594', 188)\n",
      "('MD594', 170)\n",
      "('MD594', 159)\n",
      "('MD594', 192)\n",
      "('MD594', 181)\n",
      "Training: 15000 positive, 15000 negative\n",
      "Test: 1000 positive, 1000 negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting classifier: 16.29 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'addresses_test_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-34a6f8929a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m                                       [features_dict[(train_scheme, tf_variant)][addr] \n\u001b[1;32m    157\u001b[0m                                        for addr in addresses_test_pos]\n\u001b[0;32m--> 158\u001b[0;31m                                           for tf_variant in test_transforms}\n\u001b[0m\u001b[1;32m    159\u001b[0m                 features_test_neg = {(test_scheme, tf_variant): \n\u001b[1;32m    160\u001b[0m                                       [features_dict[(train_scheme, tf_variant)][addr] \n",
      "\u001b[0;32m<ipython-input-23-34a6f8929a38>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((tf_variant,))\u001b[0m\n\u001b[1;32m    156\u001b[0m                                       [features_dict[(train_scheme, tf_variant)][addr] \n\u001b[1;32m    157\u001b[0m                                        for addr in addresses_test_pos]\n\u001b[0;32m--> 158\u001b[0;31m                                           for tf_variant in test_transforms}\n\u001b[0m\u001b[1;32m    159\u001b[0m                 features_test_neg = {(test_scheme, tf_variant): \n\u001b[1;32m    160\u001b[0m                                       [features_dict[(train_scheme, tf_variant)][addr] \n",
      "\u001b[0;31mNameError\u001b[0m: global name 'addresses_test_pos' is not defined"
     ]
    }
   ],
   "source": [
    "# for structure in all_known_structures:\n",
    "for structure in ['7N']:\n",
    "\n",
    "    ############## Sample and Load training feature vectors #########################################\n",
    "\n",
    "    positive_addresses_traindata, negative_addresses_traindata = \\\n",
    "    sample_addresses(train_stacks, structure)\n",
    "    \n",
    "    print '# positive train =', len(positive_addresses_traindata)\n",
    "    print '# negative train =', len(negative_addresses_traindata)\n",
    "    \n",
    "    addresses_to_compute = positive_addresses_traindata + negative_addresses_traindata\n",
    "\n",
    "    for variant in [0]:\n",
    "        features_loaded = read_cnn_features(addresses=addresses_to_compute, \n",
    "                                            scheme=train_scheme, win_id=7, prep_id=2, \n",
    "                                            model=model, mean_img=mean_img, model_name=model_name, \n",
    "                                            batch_size=batch_size)\n",
    "\n",
    "        for addr, f in izip(addresses_to_compute, features_loaded):\n",
    "            features_dict[(train_scheme, variant)][addr] = f\n",
    "\n",
    "        del features_loaded\n",
    "\n",
    "    ############## Sample and Load test feature vectors #############################################\n",
    "    \n",
    "    positive_addresses_testdata, negative_addresses_testdata = \\\n",
    "    sample_addresses(test_stacks, structure)\n",
    "    \n",
    "    print '# positive test =', len(positive_addresses_testdata)\n",
    "    print '# negative test =', len(negative_addresses_testdata)\n",
    "    \n",
    "    addresses_to_compute = positive_addresses_testdata + negative_addresses_testdata\n",
    "\n",
    "    for variant in [0]:\n",
    "        features_loaded = read_cnn_features(addresses=addresses_to_compute, \n",
    "                                            scheme=train_scheme, win_id=7, prep_id=2, \n",
    "                                            model=model, mean_img=mean_img, model_name=model_name, \n",
    "                                            batch_size=batch_size)\n",
    "\n",
    "        for addr, f in izip(addresses_to_compute, features_loaded):\n",
    "            features_dict[(test_scheme, variant)][addr] = f\n",
    "\n",
    "        del features_loaded\n",
    "            \n",
    "    ########################################################################################\n",
    "    \n",
    "    # n_train_list = [10, 100, 200, 500, 1000, 2000, 5000, 10000, 15000]\n",
    "#     n_train_list = [10, 1000]\n",
    "#     n_train_list = [1000, 5000, 15000]\n",
    "    n_train_list = [15000]\n",
    "    test_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "    train_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for n_train in n_train_list:\n",
    "\n",
    "        for trial in range(3):\n",
    "            \n",
    "            ##### Sample from training pool the required number of examples ######\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "            n_train_pos = min(n_train, len(positive_addresses_traindata))\n",
    "#             if len(positive_addresses_traindata) < n_train_pos:\n",
    "#                 continue\n",
    "            training_pos_indices = np.random.choice(range(len(positive_addresses_traindata)), n_train_pos, replace=False)\n",
    "            \n",
    "            n_test_pos = min(len(positive_addresses_testdata), 1000)\n",
    "            test_pos_indices = np.random.choice(range(len(positive_addresses_testdata)), n_test_pos, replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_pos_total = len(positive_addresses)\n",
    "        #     n_train_pos = 1000\n",
    "        #     training_pos_indices = np.random.choice(range(n_pos_total), n_train_pos, replace=False)\n",
    "        #     test_pos_indices = np.random.choice(np.setdiff1d(range(n_pos_total), training_pos_indices),\n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_pos = len(test_pos_indices)\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "            n_train_neg = n_train_pos\n",
    "            training_neg_indices = np.random.choice(range(len(negative_addresses_traindata)), n_train_neg, replace=False)\n",
    "            \n",
    "            n_test_neg = min(len(negative_addresses_testdata), 1000)\n",
    "            test_neg_indices = np.random.choice(range(len(negative_addresses_testdata)), n_test_neg, replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_neg_total = len(negative_addresses)\n",
    "        #     n_train_neg = 1000\n",
    "        #     training_neg_indices = np.random.choice(range(n_neg_total), n_train_neg, replace=False)\n",
    "        #     test_neg_indices = np.random.choice(np.setdiff1d(range(n_neg_total), training_neg_indices), \n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_neg = len(test_neg_indices)\n",
    "\n",
    "            print \"Training: %d positive, %d negative\" % (n_train_pos, n_train_neg)\n",
    "            print \"Test: %d positive, %d negative\" % (n_test_pos, n_test_neg)\n",
    "\n",
    "            ################\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "            addresses_train_pos = [positive_addresses_traindata[i] for i in training_pos_indices]\n",
    "            addresses_train_neg = [negative_addresses_traindata[i] for i in training_neg_indices]\n",
    "\n",
    "            #################\n",
    "\n",
    "#             for augment_training in [True, False]:\n",
    "            for augment_training in [False]:\n",
    "        \n",
    "                \n",
    "                feature_classifier_alg = 'lr'\n",
    "        #             feature_classifier_alg = 'xgb2'\n",
    "        #             feature_classifier_alg = 'lin_svc'\n",
    "        #             feature_classifier_alg = 'lin_svc_calib'\n",
    "                sample_weights = None   \n",
    "\n",
    "                if augment_training:\n",
    "                    train_transforms = range(8)\n",
    "                else:\n",
    "                    train_transforms = range(1)\n",
    "                features_train_pos = {(train_scheme, tf_variant): \n",
    "                                      [features_dict[(train_scheme, tf_variant)][addr] \n",
    "                                       for addr in addresses_train_pos]\n",
    "                                          for tf_variant in train_transforms}\n",
    "                features_train_neg = {(train_scheme, tf_variant): \n",
    "                                      [features_dict[(train_scheme, tf_variant)][addr] \n",
    "                                       for addr in addresses_train_neg]\n",
    "                                          for tf_variant in train_transforms}\n",
    "\n",
    "                train_data = np.concatenate([np.r_[features_train_pos[(train_scheme,tf)], \n",
    "                                                   features_train_neg[(train_scheme,tf)]] \n",
    "                                                for tf in train_transforms])                    \n",
    "                train_labels = np.concatenate([np.r_[np.ones((len(features_train_pos[(train_scheme,tf)]), )), \n",
    "                                                    -np.ones((len(features_train_neg[(train_scheme,tf)]), ))]\n",
    "                                              for tf in train_transforms])\n",
    "\n",
    "                if existing_classifier_id is None:\n",
    "                    clf = train_binary_classifier(train_data, train_labels,\n",
    "                                       alg=feature_classifier_alg, \n",
    "                                       sample_weights=sample_weights)\n",
    "\n",
    "    #                 del train_data, features_train_pos, features_train_neg\n",
    "\n",
    "#                     clf_fp = DataManager.get_classifier_filepath(classifier_id=classifier_id, structure=structure)\n",
    "#                     create_parent_dir_if_not_exists(clf_fp)\n",
    "#                     joblib.dump(clf, clf_fp)\n",
    "#                     upload_to_s3(clf_fp)\n",
    "                else:\n",
    "                    sys.stderr.write('Load existing classifiers %d\\n' % existing_classifier_id)\n",
    "                    clf = DataManager.load_classifiers(classifier_id=existing_classifier_id)[structure]\n",
    "                \n",
    "                ######################### Compute train metrics #########################\n",
    "                \n",
    "                train_metrics = compute_classification_metrics(clf.predict_proba(train_data)[:,1], train_labels)\n",
    "                train_metrics_all_ntrain[n_train][(train_scheme, 'augment' if augment_training else 'no-augment')].append(train_metrics)\n",
    "                \n",
    "                ######################### Test ###############################\n",
    "                \n",
    "                test_transforms = range(1)\n",
    "                features_test_pos = {(test_scheme, tf_variant): \n",
    "                                      [features_dict[(train_scheme, tf_variant)][addr] \n",
    "                                       for addr in addresses_test_pos]\n",
    "                                          for tf_variant in test_transforms}\n",
    "                features_test_neg = {(test_scheme, tf_variant): \n",
    "                                      [features_dict[(train_scheme, tf_variant)][addr] \n",
    "                                       for addr in addresses_test_neg]\n",
    "                                          for tf_variant in test_transforms}\n",
    "                \n",
    "                test_data = np.r_[features_test_pos[(test_scheme,'transform0')], features_test_neg[(test_scheme,'transform0')]]\n",
    "                test_labels = np.r_[np.ones((len(features_test_pos[(test_scheme,'transform0')]), )), \n",
    "                                     -np.ones((len(features_test_neg[(test_scheme,'transform0')]), ))]\n",
    "                test_metrics = compute_classification_metrics(clf.predict_proba(test_data)[:,1], test_labels)\n",
    "    #             print \"acc@0.5 = %.3f, acc@opt = %.3f, opt_thresh = %.3f, auroc = %.3f, auprc = %.3f\" % \\\n",
    "    #             (test_metrics['acc'][0.5], test_metrics['acc'][test_metrics['opt_thresh']], test_metrics['opt_thresh'], test_metrics['auroc'], test_metrics['auprc'])\n",
    "\n",
    "                test_metrics_all_ntrain[n_train][(test_scheme, 'augment' if augment_training else 'no-augment')].append(test_metrics)\n",
    "\n",
    "    train_metrics_all_ntrain.default_factory = None\n",
    "    test_metrics_all_ntrain.default_factory = None\n",
    "    \n",
    "    plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'acc', 0.5), ylabel='Test accuracy@0.5 threshold');\n",
    "    plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'auroc'), ylabel='Area under ROC');\n",
    "\n",
    "    plot_roc_curve(test_metrics_all_ntrain[1000][(test_scheme,\n",
    "                  'augment')][0]['fp'], \n",
    "                   test_metrics_all_ntrain[1000][(test_scheme,\n",
    "                  'augment')][0]['tp'], \n",
    "                  test_metrics_all_ntrain[1000][(test_scheme,\n",
    "                  'augment')][0]['opt_thresh']);\n",
    "    \n",
    "#     import uuid\n",
    "\n",
    "#     result = {\n",
    "#         'n_train_sections': train_stack_section_number,\n",
    "#         'n_test_sections': test_stack_section_number,\n",
    "#         'train_stain': train_stack_stain,\n",
    "#         'test_stain': test_stack_stain,\n",
    "#         'train_stack': train_stack,\n",
    "#         'test_stack': test_stack,\n",
    "#         'test_scheme': test_scheme,\n",
    "#         'train_scheme': train_scheme,\n",
    "#         'train_metrics_all_ntrain': train_metrics_all_ntrain,\n",
    "#         'test_metrics_all_ntrain': test_metrics_all_ntrain\n",
    "#     }\n",
    "\n",
    "#     create_if_not_exists(ROOT_DIR + '/assessment_results_v2/')\n",
    "#     save_pickle(result, ROOT_DIR + '/assessment_results_v2/assessment_result_%s.pkl' % str(uuid.uuid1()).split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
