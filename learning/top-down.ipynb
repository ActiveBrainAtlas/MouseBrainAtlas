{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['GORDON_REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class ProposalType(Enum):\n",
    "    GLOBAL = 'global'\n",
    "    LOCAL = 'local'\n",
    "    FREEFORM = 'freeform'\n",
    "    \n",
    "class PolygonType(Enum):\n",
    "    CLOSED = 'closed'\n",
    "    OPEN = 'open'\n",
    "    TEXTURE = 'textured'\n",
    "    TEXTURE_WITH_CONTOUR = 'texture with contour'\n",
    "    DIRECTION = 'directionality'\n",
    "    \n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dms = dict([(sc, DataManager(stack='MD593', section=sc, segm_params_id='tSLIC200', load_mask=False)) \n",
    "            for sc in range(60, 151)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dm in dms.itervalues():\n",
    "    dm.load_multiple_results(['texHist', 'spCentroids', 'edgeMidpoints', 'edgeEndpoints'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_examples = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_examples.pkl', 'r'))\n",
    "label_position = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_position.pkl', 'r'))\n",
    "label_polygon = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_polygon.pkl', 'r'))\n",
    "label_texture = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_texture.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def grow_cluster_section(sec, *args, **kwargs):\n",
    "    return dms[sec].grow_cluster(*args, **kwargs)\n",
    "\n",
    "def grow_clusters_from_sps(sec, sps):\n",
    "    \n",
    "    expansion_clusters_tuples = Parallel(n_jobs=16)(delayed(grow_cluster_section)(sec, s, verbose=False, all_history=False, \n",
    "                                                                         seed_weight=0,\n",
    "                                                                        num_sp_percentage_limit=0.05,\n",
    "                                                                     min_size=1, min_distance=2,\n",
    "                                                                        threshold_abs=-0.1,\n",
    "                                                                        threshold_rel=0.02,\n",
    "                                                                       peakedness_limit=.002,\n",
    "                                                                       method='rc-mean')\n",
    "                                    for s in sps)\n",
    "\n",
    "    all_seed_cluster_score_tuples = [(seed, cl, sig) for seed, peaks in enumerate(expansion_clusters_tuples) \n",
    "                                     for cl, sig in zip(*peaks)]\n",
    "    all_seeds, all_clusters, all_scores = zip(*all_seed_cluster_score_tuples)\n",
    "\n",
    "    all_clusters_unique_dict = {}\n",
    "    for i, cl in enumerate(all_clusters):\n",
    "        all_clusters_unique_dict[frozenset(cl)] = i\n",
    "\n",
    "    all_unique_cluster_indices = all_clusters_unique_dict.values()\n",
    "    all_unique_cluster_scores = [all_scores[i] for i in all_unique_cluster_indices]\n",
    "    all_unique_cluster_indices_sorted = [all_unique_cluster_indices[i] for i in np.argsort(all_unique_cluster_scores)[::-1]]\n",
    "\n",
    "    all_unique_tuples = [all_seed_cluster_score_tuples[i] for i in all_unique_cluster_indices_sorted]\n",
    "\n",
    "    return all_unique_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cluster_coherence_score(sec, cluster, verbose=False):\n",
    "    \n",
    "    if len(cluster) > 1:\n",
    "        cluster_avg = dms[sec].texton_hists[cluster].mean(axis=0)\n",
    "        ds = np.squeeze(chi2s([cluster_avg], dms[sec].texton_hists[list(cluster)]))\n",
    "        var = ds.mean()\n",
    "    else:\n",
    "        var = 0\n",
    "    \n",
    "    return var\n",
    "\n",
    "# def compute_cluster_significance_score(sec, *args, **kwargs):\n",
    "#     return dms[sec].compute_cluster_score(*args, **kwargs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coherence_limit = .25\n",
    "area_limit = 60000\n",
    "nonoverlapping_area_limit = 2.\n",
    "bg_texton = 3\n",
    "bg_texton_percentage = .2\n",
    "significance_limit = 0.05\n",
    "consensus_limit = -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scores_to_vote(scores):\n",
    "    vals = np.unique(scores)\n",
    "    d = dict(zip(vals, np.linspace(0, 1, len(vals))))\n",
    "    votes = np.array([d[s] for s in scores])\n",
    "    votes = votes/votes.sum()\n",
    "    return votes\n",
    "\n",
    "\n",
    "def filter_clusters(sec, all_unique_tuples, label, sec2):\n",
    "    \n",
    "    dm = dms[sec]\n",
    "    \n",
    "    all_unique_seeds, all_unique_clusters, all_unique_cluster_scores = zip(*all_unique_tuples)\n",
    "    \n",
    "    all_cluster_sigs = np.array(all_unique_cluster_scores)\n",
    "    all_cluster_coherences = np.array([compute_cluster_coherence_score(sec, cl) for cl in all_unique_clusters])\n",
    "    all_cluster_hists = [dm.texton_hists[cl].mean(axis=0) for cl in all_unique_clusters]\n",
    "    all_cluster_entropy = np.nan_to_num([-np.sum(hist[hist!=0]*np.log(hist[hist!=0])) for hist in all_cluster_hists])\n",
    "\n",
    "    all_cluster_centroids = np.array([dm.sp_centroids[cl, ::-1].mean(axis=0) for cl in all_unique_clusters])\n",
    "\n",
    "    dm.load_multiple_results(['spAreas'])\n",
    "    all_cluster_area = np.array([dm.sp_areas[cl].sum() for cl in all_unique_clusters])\n",
    "    \n",
    "    remaining_cluster_indices = [i for i, (cl, coh, sig, ent, cent, area, hist) in enumerate(zip(all_unique_clusters, \n",
    "                                                                                      all_cluster_coherences, \n",
    "                                                                                      all_cluster_sigs,\n",
    "                                                                                      all_cluster_entropy,\n",
    "                                                                                      all_cluster_centroids,\n",
    "                                                                                      all_cluster_area,\n",
    "                                                                                    all_cluster_hists)) \n",
    "            if coh < coherence_limit and sig > significance_limit and \\\n",
    "                area > area_limit and \\\n",
    "             ((ent > 1.5 and hist[bg_texton] < bg_texton_percentage) or \\\n",
    "              (cent[0] - dm.xmin > 800 and \\\n",
    "               dm.xmax - cent[0] > 800 and \\\n",
    "               cent[1] - dm.ymin > 800 and \\\n",
    "               dm.ymax - cent[1] > 800)\n",
    "             )]\n",
    "    \n",
    "    print '%d unique clusters, %d remaining clusters' % (len(all_unique_clusters), len(remaining_cluster_indices))\n",
    "    \n",
    "    all_remaining_clusters = [all_unique_clusters[i] for i in remaining_cluster_indices]\n",
    "\n",
    "    tex_dists = cdist([label_texture[label]], [all_cluster_hists[i] for i in remaining_cluster_indices], chi2)[0]\n",
    "    \n",
    "#     remaining_cluster_indices_sortedByTexture = [remaining_cluster_indices[j] for j in np.argsort(tex_dists)]\n",
    "      \n",
    "    polygons = [Polygon(vertices_from_dedges(sec, dm.find_boundary_dedges_ordered(cl))) for cl in all_remaining_clusters]\n",
    "\n",
    "    polygon_overlaps = []\n",
    "    for p in polygons:\n",
    "        try:\n",
    "            polygon_overlaps.append(label_polygon[label][sec2].intersection(p).area)\n",
    "        except:\n",
    "            polygon_overlaps.append(0)\n",
    "    \n",
    "#     rank = np.argsort(.3*scores_to_vote(polygon_overlaps) + .7*scores_to_vote(-tex_dists))[::-1]\n",
    "    rank = np.argsort(.1*scores_to_vote(polygon_overlaps) + .9*scores_to_vote(-tex_dists))[::-1]\n",
    "\n",
    "    all_remaining_clusters_sorted = [all_unique_clusters[i] for i in rank]\n",
    "\n",
    "#     remaining_cluster_indices_sortedByOverlap = [remaining_cluster_indices[j] for j in np.argsort(polygon_overlaps)[::-1]]\n",
    "    \n",
    "#     all_remaining_clusters_sortedByTexture = [all_unique_clusters[i] for i in remaining_cluster_indices_sortedByTexture]\n",
    "\n",
    "#     all_remaining_clusters_sortedByOverlap = [all_unique_clusters[i] for i in remaining_cluster_indices_sortedByOverlap]\n",
    "    \n",
    "#     return all_remaining_clusters_sortedByTexture\n",
    "#     return all_remaining_clusters_sortedByOverlap\n",
    "    return all_remaining_clusters_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vertices_from_dedges(sec, dedges):\n",
    "\n",
    "    vertices = []\n",
    "    for de_ind, de in enumerate(dedges):\n",
    "        midpt = dms[sec].edge_midpoints[frozenset(de)]\n",
    "        endpts = dms[sec].edge_endpoints[frozenset(de)]\n",
    "        endpts_next_dedge = dms[sec].edge_endpoints[frozenset(dedges[(de_ind+1)%len(dedges)])]\n",
    "\n",
    "        dij = cdist([endpts[0], endpts[-1]], [endpts_next_dedge[0], endpts_next_dedge[-1]])\n",
    "        i,j = np.unravel_index(np.argmin(dij), (2,2))\n",
    "        if i == 0:\n",
    "            vertices += [endpts[-1], midpt, endpts[0]]\n",
    "        else:\n",
    "            vertices += [endpts[0], midpt, endpts[-1]]\n",
    "        \n",
    "    return vertices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest_labeled_section 135\n",
      "35 sps to look at\n",
      "\n",
      "103 unique clusters, 57 remaining clusters\n",
      "847 terminate due to over-size\n",
      "844 terminate due to over-size\n",
      "803 terminate due to over-size\n",
      "852 terminate due to over-size\n",
      "900 terminate due to over-size\n",
      "804 terminate due to over-size\n",
      "830 terminate due to over-size\n",
      "889 terminate due to over-size\n",
      "779 terminate due to over-size\n",
      "858 terminate due to over-size\n",
      "770 terminate due to over-size\n",
      "874 terminate due to over-size\n",
      "842 terminate due to over-size\n",
      "828 terminate due to over-size\n",
      "903 terminate due to over-size\n",
      "821 terminate due to over-size\n",
      "905909910914918921999930932933939941954956972981 terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      "10041000 terminate due to over-size\n",
      " terminate due to over-size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "821\n",
      "900\n",
      "804\n",
      "852\n",
      "842\n",
      "770\n",
      "889\n",
      "844\n",
      "828\n",
      "803\n",
      "847\n",
      "858\n",
      "874\n",
      "779\n",
      "830\n",
      "903\n",
      "981\n",
      "918\n",
      "921\n",
      "914\n",
      "954\n",
      "939\n",
      "930\n",
      "909\n",
      "956\n",
      "910\n",
      "905\n",
      "933\n",
      "941\n",
      "932\n",
      "925\n",
      "972\n",
      "999\n",
      "1000\n",
      "1004\n"
     ]
    }
   ],
   "source": [
    "section = 130\n",
    "# label = 'lateral reticular nucleus'\n",
    "# label = 'pontine'\n",
    "label = 'facial motor nucleus'\n",
    "\n",
    "ks = np.array(label_position[label].keys())\n",
    "\n",
    "closest_labeled_section = ks[np.argmin(np.abs(ks-section))]\n",
    "print 'closest_labeled_section %d' % closest_labeled_section\n",
    "\n",
    "v1,v2,s1,s2,c0 = label_position[label][closest_labeled_section]\n",
    "\n",
    "# v1,v2,s1,s2,c0 = label_position['pontine']\n",
    "\n",
    "angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "ell_vertices = cv2.ellipse2Poly(tuple(c0.astype(np.int)), (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, 10)\n",
    "\n",
    "sps = np.where([Path(ell_vertices).contains_point(s) for s in dms[section].sp_centroids[:,::-1]])[0]\n",
    "\n",
    "print '%d sps to look at\\n' % len(sps)\n",
    "\n",
    "all_unique_tuples = grow_clusters_from_sps(section, sps)\n",
    "all_remaining_clusters_sorted = filter_clusters(section, all_unique_tuples, label, closest_labeled_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tmp.jpg' target='_blank'>tmp.jpg</a><br>"
      ],
      "text/plain": [
       "/oasis/projects/nsf/csd395/yuncong/Brain/learning/tmp.jpg"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = dms[section]\n",
    "\n",
    "atlas = img_as_ubyte(dm.visualize_cluster(all_remaining_clusters_sorted[0]))\n",
    "\n",
    "# v1,v2,s1,s2,c0 = label_position['pontine']\n",
    "\n",
    "angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "cv2.ellipse(atlas, tuple(c0.astype(np.int)-(dm.xmin, dm.ymin)), (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, (0,255,0), 10)\n",
    "\n",
    "display(atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
