{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import mxnet as mx\n",
    "except:\n",
    "    sys.stderr.write(\"Cannot import mxnet.\\n\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import rotate\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from distributed_utilities import *\n",
    "from visualization_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "sys.path.append('/home/yuncong/csd395/xgboost/python-package')\n",
    "try:\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "except:\n",
    "    sys.stderr.write('xgboost is not loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:53: UserWarning: \u001b[91mYou created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\n",
      "\tdata\u001b[0m\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:65: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "model_dir_name = 'inception-bn-blue'\n",
    "model_name = 'inception-bn-blue'\n",
    "model, mean_img = load_mxnet_model(model_dir_name=model_dir_name, model_name=model_name, \n",
    "                                   num_gpus=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "win_id = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_one_metric(metrics_all_ntrain, which, thresh=None):\n",
    "    if thresh is None:\n",
    "        return {ntrain: {test_cond: [res[which] for res in res_all_trials]\n",
    "                   for test_cond, res_all_trials in x.iteritems()}\n",
    "         for ntrain, x in metrics_all_ntrain.iteritems()}\n",
    "    else:\n",
    "        return {ntrain: {test_cond: [res[which][thresh] for res in res_all_trials]\n",
    "                   for test_cond, res_all_trials in x.iteritems()}\n",
    "         for ntrain, x in metrics_all_ntrain.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result_wrt_ntrain(test_metrics_all_ntrain, ylabel='', title=''):\n",
    "\n",
    "    for test_condition in test_metrics_all_ntrain.values()[1].keys():\n",
    "        ntrains, accs, errs = zip(*sorted([(ntrain, np.mean(x[test_condition]), np.std(x[test_condition]))\n",
    "              for ntrain, x in test_metrics_all_ntrain.iteritems() if len(x) > 0]))\n",
    "        plt.errorbar(ntrains, accs, yerr=errs, label=test_condition[1]);\n",
    "    plt.xlabel('# traing patches');\n",
    "    plt.ylabel(ylabel);\n",
    "    plt.title(title);\n",
    "    plt.legend(loc='upper right');\n",
    "    plt.ylim([0.5, 1.]);\n",
    "    plt.xlim([0,16000]);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stack1 in ['MD661']:\n",
    "    \n",
    "    stacks = [stack1, 'ChatCryoJane201710']\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    sample_locations_allStacks = {stack: grid_parameters_to_sample_locations(win_id_to_gridspec(win_id=5, stack=stack)) \n",
    "                              for stack in stacks}\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    grid_indices_lookup_allStacks = {}\n",
    "\n",
    "    for stack in [stack1]:\n",
    "        try:\n",
    "#             grid_indices_lookup_allStacks[stack] = \\\n",
    "#             DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "#                                                                by_human=False, timestamp='latest',\n",
    "#                                                                detector_id_f=1)\n",
    "            \n",
    "            grid_indices_lookup_allStacks[stack] = \\\n",
    "            DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "                                                               by_human=True, timestamp='latest')\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            sys.stderr.write(\"Fail to load annotation grid lookup for %s.\\n\" % stack)\n",
    "\n",
    "\n",
    "    # stack = 'MD661'\n",
    "    stack = 'ChatCryoJane201710'\n",
    "    try:\n",
    "        grid_indices_lookup_allStacks[stack] = \\\n",
    "        DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "                                                           by_human=True, timestamp='latest')\n",
    "    except:\n",
    "        sys.stderr.write(\"Fail to load annotation grid lookup for %s.\\n\" % stack)\n",
    "        \n",
    "    #############################\n",
    "    \n",
    "    from itertools import chain\n",
    "    all_labels = sorted(list(set(chain.from_iterable(set(grid_indices_lookup_allStacks[st].columns.tolist()) for st in stacks))))\n",
    "    \n",
    "    address_multidx = pd.MultiIndex.from_tuples([(stack, sec, grid_idx) \n",
    "           for stack in sorted(stacks)\n",
    "           for sec in np.arange(metadata_cache['section_limits'][stack][0], \n",
    "                                metadata_cache['section_limits'][stack][-1]+1)\n",
    "           for grid_idx in range(len(sample_locations_allStacks[stack]))], \n",
    "          names=['stack', 'section', 'grid_idx'])\n",
    "    \n",
    "    address_label_df_allStacks = DataFrame(columns=all_labels,\n",
    "    index=address_multidx, data=False)\n",
    "    \n",
    "    # surround_margins = [200, 500]\n",
    "    surround_margins = [200]\n",
    "    \n",
    "    # Option 1: Use the cached file that stores the mapping from grid-indices to labels for speed up.\n",
    "\n",
    "    for stack in stacks:\n",
    "    # for stack in ['MD661']:\n",
    "    # for stack in ['ChatCryoJane201710']:\n",
    "\n",
    "        grid_index_class_lookup = grid_indices_lookup_allStacks[stack]\n",
    "\n",
    "        for sec in grid_index_class_lookup.index:\n",
    "            sys.stderr.write('Computing class label to grid indices lookup-table for section %d...\\n' % sec)\n",
    "            if is_invalid(sec=sec, stack=stack):\n",
    "                continue\n",
    "\n",
    "            for label, grid_indices in grid_index_class_lookup.loc[sec].dropna().iteritems():\n",
    "                if label == 'bg' or label == 'noclass' or 'negative' in label: # Ignore bg, noclass, x_negative\n",
    "                    continue\n",
    "\n",
    "                name, side, margin, surr_name = parse_label(label)\n",
    "\n",
    "                if name == '7N' and (margin is None or (margin is not None and int(margin) in surround_margins)):\n",
    "\n",
    "                    sys.stderr.write(\"%s, sec %d: %d patches from label %s.\\n\" % (stack, sec, len(grid_indices), label))\n",
    "        #             t = time.time()\n",
    "                    for grid_idx in grid_indices:\n",
    "                        address_label_df_allStacks.loc[(stack, sec, grid_idx), label] = True\n",
    "        #             sys.stderr.write(\"%.2f s\\n\" % (time.time()-t))\n",
    "\n",
    "    del grid_indices_lookup_allStacks\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    # structure = 'Amb'\n",
    "    structure = '7N'\n",
    "    # structure = 'SNR'\n",
    "    positive_labels = [structure]\n",
    "    negative_labels = get_negative_labels(structure, 'neg_has_all_surround', margin_um=200, \n",
    "                                          labels_found=address_label_df_allStacks.columns)\n",
    "    # negative_labels = get_negative_labels(structure, 'neg_has_all_surround', margin_um=500, \n",
    "    #                                       labels_found=address_label_df_allStacks.columns)\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "    train_stacks = [stack1]\n",
    "    test_stacks = ['ChatCryoJane201710']\n",
    "    \n",
    "    train_stack_section_number = {stack1: 10}\n",
    "    test_stack_section_number = {'ChatCryoJane201710': 10}\n",
    "\n",
    "    train_stack_stain = {stack1: 'F'}\n",
    "    test_stack_stain = {'ChatCryoJane201710': 'A'}\n",
    "    \n",
    "    positive_addresses_traindata_all_stacks = {}\n",
    "    negative_addresses_traindata_all_stacks = {}\n",
    "\n",
    "    for train_stack in train_stacks:\n",
    "\n",
    "        q = address_label_df_allStacks[address_label_df_allStacks[positive_labels].any(axis=1)]\n",
    "        all_sections = np.unique(q.loc[train_stack].index.get_level_values('section'))\n",
    "        train_sections = [] # sections that contain patches of relevant classes.\n",
    "        for sec in all_sections:\n",
    "            try:\n",
    "                if len(q.loc[train_stack, sec].index.values.tolist()) > 0:\n",
    "                    train_sections.append(sec)\n",
    "                    print 'Section', sec, 'provides', len(q.loc[train_stack, sec].index.values.tolist()), 'positive patches.'\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        ###################################################################################\n",
    "\n",
    "        n_train_sections = train_stack_section_number[train_stack]\n",
    "        # n_train_sections = 10\n",
    "        # n_train_sections = len(train_sections)\n",
    "\n",
    "        if train_stack_stain[train_stack] == 'F':\n",
    "            neurotrace_sections = []\n",
    "            nissl_sections = []\n",
    "            for sec in train_sections:\n",
    "                if metadata_cache['sections_to_filenames'][train_stack][sec].split('-')[1][0] == 'F':\n",
    "                    neurotrace_sections.append(sec)\n",
    "                else:\n",
    "                    nissl_sections.append(sec)\n",
    "            random_train_sections = np.random.choice(neurotrace_sections, min(len(neurotrace_sections), n_train_sections), replace=False)\n",
    "        else:\n",
    "            random_train_sections = np.random.choice(train_sections, min(len(train_sections), n_train_sections), replace=False)\n",
    "\n",
    "        positive_addresses_traindata_all_stacks[train_stack] = address_label_df_allStacks[address_label_df_allStacks[positive_labels].any(axis=1)].loc[[train_stack], random_train_sections, :].index.values.tolist() \n",
    "        negative_addresses_traindata_all_stacks[train_stack] = address_label_df_allStacks[address_label_df_allStacks[negative_labels].any(axis=1)].loc[[train_stack], random_train_sections, :].index.values.tolist() \n",
    "\n",
    "    positive_addresses_traindata = sum(positive_addresses_traindata_all_stacks.values(), [])\n",
    "    negative_addresses_traindata = sum(negative_addresses_traindata_all_stacks.values(), [])\n",
    "\n",
    "    print '# positive train =', len(positive_addresses_traindata)\n",
    "    print '# negative train =', len(negative_addresses_traindata)\n",
    "\n",
    "    del positive_addresses_traindata_all_stacks, negative_addresses_traindata_all_stacks\n",
    "    \n",
    "    positive_addresses_testdata_all_stacks = {}\n",
    "    negative_addresses_testdata_all_stacks = {}\n",
    "\n",
    "    for test_stack in test_stacks:\n",
    "\n",
    "        q = address_label_df_allStacks[address_label_df_allStacks[positive_labels].any(axis=1)]\n",
    "        all_sections = np.unique(q.loc[test_stack].index.get_level_values('section'))\n",
    "        test_sections = [] # sections that contain patches of relevant classes.\n",
    "        for sec in all_sections:\n",
    "            try:\n",
    "                if len(q.loc[test_stack, sec].index.values.tolist()) > 0:\n",
    "                    test_sections.append(sec)\n",
    "                    print 'Section', sec, 'provides', len(q.loc[test_stack, sec].index.values.tolist()), 'positive patches.'\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        #####################################################################################\n",
    "\n",
    "        n_test_sections = test_stack_section_number[test_stack]\n",
    "    #     n_test_sections = 10\n",
    "        # n_test_sections = len(test_sections)\n",
    "\n",
    "        if test_stack_stain[test_stack] == 'F':\n",
    "\n",
    "            neurotrace_sections = []\n",
    "            nissl_sections = []\n",
    "            for sec in test_sections:\n",
    "                if metadata_cache['sections_to_filenames'][test_stack][sec].split('-')[1][0] == 'F':\n",
    "                    neurotrace_sections.append(sec)\n",
    "                else:\n",
    "                    nissl_sections.append(sec)\n",
    "            random_test_sections = np.random.choice(neurotrace_sections, min(len(neurotrace_sections), n_test_sections), replace=False)\n",
    "        else:\n",
    "            random_test_sections = np.random.choice(test_sections, min(len(test_sections), n_test_sections), replace=False)\n",
    "\n",
    "        positive_addresses_testdata_all_stacks[test_stack] = address_label_df_allStacks[address_label_df_allStacks[positive_labels].any(axis=1)].loc[[test_stack], random_test_sections, :].index.values.tolist()\n",
    "        negative_addresses_testdata_all_stacks[test_stack] = address_label_df_allStacks[address_label_df_allStacks[negative_labels].any(axis=1)].loc[[test_stack], random_test_sections, :].index.values.tolist()\n",
    "\n",
    "    positive_addresses_testdata = sum(positive_addresses_testdata_all_stacks.values(), [])\n",
    "    negative_addresses_testdata = sum(negative_addresses_testdata_all_stacks.values(), [])\n",
    "\n",
    "    print '# positive test =', len(positive_addresses_testdata)\n",
    "    print '# negative test =', len(negative_addresses_testdata)\n",
    "\n",
    "    del positive_addresses_testdata_all_stacks, negative_addresses_testdata_all_stacks\n",
    "    \n",
    "    # schemes = ['normalize_mu_region_sigma_wholeImage_(-1,9)']\n",
    "    # schemes = ['normalize_mu_region_sigma_wholeImage_(-1,9)', 'median_curve']\n",
    "    # schemes = ['normalize_mu_region_sigma_wholeImage_(-1,5)']\n",
    "    # schemes = ['stretch_min_max']\n",
    "    # schemes = [None, 'stretch_min_max']\n",
    "    # schemes = ['none']\n",
    "    # schemes = [None, 'median_curve']\n",
    "    # transforms = ['transform%d' % i for i in range(8)]\n",
    "    # scheme_transform_multiindex = pd.MultiIndex.from_product([schemes, transforms])\n",
    "\n",
    "    # features_dict = {(scheme, tfv): {} for scheme in schemes for tfv in transforms}\n",
    "    features_dict = defaultdict(dict)\n",
    "    \n",
    "    # train_scheme = 'stretch_min_max'\n",
    "    # train_scheme = 'normalize_mu_sigma_global_(-1,5)'\n",
    "    train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,5)'\n",
    "    # train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,9)'\n",
    "    # train_scheme = 'none'\n",
    "    addresses_to_compute = positive_addresses_traindata + negative_addresses_traindata\n",
    "\n",
    "    patches_with_desired_label = \\\n",
    "    extract_patches_given_locations_multiple_sections(addresses=addresses_to_compute,\n",
    "    #                                                     images={('MD661', 199): img},\n",
    "                                                  win_id=win_id, \n",
    "                                                normalization_scheme=train_scheme,\n",
    "                                                 location_or_grid_index='grid_index')\n",
    "\n",
    "    #     display_images_in_grids(patches_with_desired_label, nc=10, cmap=plt.cm.gray)\n",
    "\n",
    "    # for variant in [0]:\n",
    "    for variant in range(8):\n",
    "\n",
    "        t = time.time()\n",
    "    #             patches_rotated = rotate_all_patches(patches_enlarged, r=r, output_size=224)\n",
    "        patches_rotated = rotate_all_patches_variant(patches_with_desired_label, variant=variant)\n",
    "        sys.stderr.write('Compute variants: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        t = time.time()\n",
    "        features = convert_image_patches_to_features_v2(patches_rotated, model=model, \n",
    "                                                     mean_img=mean_img, \n",
    "    #                                                      mean_img=np.zeros((224,224), dtype=np.uint8), \n",
    "                                                     batch_size=batch_size)\n",
    "        sys.stderr.write('Feature computation: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        for i, f in enumerate(features):\n",
    "    #             features_df.loc[addresses_with_desired_label[i]][(scheme, 'transform%d'%variant)] = f\n",
    "            features_dict[(train_scheme, 'transform%d'%variant)][addresses_to_compute[i]] = f\n",
    "        \n",
    "    \n",
    "    # test_scheme = 'stretch_min_max'\n",
    "    # test_scheme = 'normalize_mu_sigma_global_(-1,5)'\n",
    "    test_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,5)'\n",
    "    # test_scheme = 'median_curve'\n",
    "    # test_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,9)'\n",
    "    # test_scheme = 'none'\n",
    "    addresses_to_compute = positive_addresses_testdata + negative_addresses_testdata\n",
    "\n",
    "    patches_with_desired_label = \\\n",
    "    extract_patches_given_locations_multiple_sections(addresses=addresses_to_compute,\n",
    "    #                                                     images={('MD661', 199): img},\n",
    "                                                  win_id=win_id, \n",
    "                                                normalization_scheme=test_scheme,\n",
    "                                                 location_or_grid_index='grid_index')\n",
    "\n",
    "    #     display_images_in_grids(patches_with_desired_label, nc=10, cmap=plt.cm.gray)\n",
    "\n",
    "    for variant in [0]:\n",
    "    # for variant in range(8):\n",
    "\n",
    "        t = time.time()\n",
    "    #             patches_rotated = rotate_all_patches(patches_enlarged, r=r, output_size=224)\n",
    "        patches_rotated = rotate_all_patches_variant(patches_with_desired_label, variant=variant)\n",
    "        sys.stderr.write('Compute variants: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        t = time.time()\n",
    "        features = convert_image_patches_to_features_v2(patches_rotated, model=model, \n",
    "                                                     mean_img=mean_img, \n",
    "    #                                                      mean_img=np.zeros((224,224), dtype=np.uint8), \n",
    "                                                     batch_size=batch_size)\n",
    "        sys.stderr.write('Feature computation: %.2f seconds\\n' % (time.time() - t))\n",
    "\n",
    "        for i, f in enumerate(features):\n",
    "    #             features_df.loc[addresses_with_desired_label[i]][(scheme, 'transform%d'%variant)] = f\n",
    "            features_dict[(test_scheme, 'transform%d'%variant)][addresses_to_compute[i]] = f\n",
    "        \n",
    "    ###########################################################\n",
    "    \n",
    "    # n_train_list = [10, 100, 200, 500, 1000, 2000, 5000, 10000, 15000]\n",
    "    n_train_list = [10, 1000]\n",
    "    test_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "    train_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for n_train in n_train_list:\n",
    "\n",
    "        for trial in range(10):\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "        #     n_train_pos = 5000\n",
    "            n_train_pos = n_train\n",
    "            if len(positive_addresses_traindata) < n_train_pos:\n",
    "                continue\n",
    "            training_pos_indices = np.random.choice(range(len(positive_addresses_traindata)), n_train_pos, replace=False)\n",
    "            n_test_pos = 1000\n",
    "            test_pos_indices = np.random.choice(range(len(positive_addresses_testdata)),\n",
    "                                                size=min(len(positive_addresses_testdata), n_test_pos), \n",
    "                                                replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_pos_total = len(positive_addresses)\n",
    "        #     n_train_pos = 1000\n",
    "        #     training_pos_indices = np.random.choice(range(n_pos_total), n_train_pos, replace=False)\n",
    "        #     test_pos_indices = np.random.choice(np.setdiff1d(range(n_pos_total), training_pos_indices),\n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_pos = len(test_pos_indices)\n",
    "\n",
    "            ###############\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "        #     n_train_neg = 5000\n",
    "            n_train_neg = n_train\n",
    "            training_neg_indices = np.random.choice(range(len(negative_addresses_traindata)), n_train_neg, replace=False)\n",
    "            n_test_neg = 1000\n",
    "            test_neg_indices = np.random.choice(range(len(negative_addresses_testdata)),\n",
    "                                                size=min(len(negative_addresses_testdata), n_test_neg), \n",
    "                                                replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_neg_total = len(negative_addresses)\n",
    "        #     n_train_neg = 1000\n",
    "        #     training_neg_indices = np.random.choice(range(n_neg_total), n_train_neg, replace=False)\n",
    "        #     test_neg_indices = np.random.choice(np.setdiff1d(range(n_neg_total), training_neg_indices), \n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_neg = len(test_neg_indices)\n",
    "\n",
    "            print \"Training: %d positive, %d negative\" % (n_train_pos, n_train_neg)\n",
    "            print \"Test: %d positive, %d negative\" % (n_test_pos, n_test_neg)\n",
    "\n",
    "            ################\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "            addresses_train_pos = [positive_addresses_traindata[i] for i in training_pos_indices]\n",
    "            addresses_test_pos = [positive_addresses_testdata[i] for i in test_pos_indices]\n",
    "            addresses_train_neg = [negative_addresses_traindata[i] for i in training_neg_indices]\n",
    "            addresses_test_neg = [negative_addresses_testdata[i] for i in test_neg_indices]\n",
    "\n",
    "            # If train and test data are from same set\n",
    "        #     addresses_train_pos = [positive_addresses[i] for i in training_pos_indices]\n",
    "        #     addresses_test_pos = [positive_addresses[i] for i in test_pos_indices]\n",
    "        #     addresses_train_neg = [negative_addresses[i] for i in training_neg_indices]\n",
    "        #     addresses_test_neg = [negative_addresses[i] for i in test_neg_indices]\n",
    "\n",
    "            #################\n",
    "\n",
    "            for augment_training in [True, False]:\n",
    "    #         for augment_training in [False]:\n",
    "\n",
    "                feature_classifier_alg = 'lr'\n",
    "        #             feature_classifier_alg = 'xgb2'\n",
    "        #             feature_classifier_alg = 'lin_svc'\n",
    "        #             feature_classifier_alg = 'lin_svc_calib'\n",
    "                sample_weights = None   \n",
    "\n",
    "                if augment_training:\n",
    "                    train_transforms = range(8)\n",
    "                else:\n",
    "                    train_transforms = range(1)\n",
    "\n",
    "                test_transforms = range(1)\n",
    "\n",
    "                features_train_pos = {(train_scheme, 'transform%d'%tf_variant): \n",
    "                                      [features_dict[(train_scheme, 'transform%d'%tf_variant)][addr] \n",
    "                                       for addr in addresses_train_pos]\n",
    "                                          for tf_variant in train_transforms}\n",
    "                features_train_neg = {(train_scheme, 'transform%d'%tf_variant): \n",
    "                                      [features_dict[(train_scheme, 'transform%d'%tf_variant)][addr] \n",
    "                                       for addr in addresses_train_neg]\n",
    "                                          for tf_variant in train_transforms}\n",
    "                features_test_pos = {(test_scheme, 'transform%d'%tf_variant): \n",
    "                                     [features_dict[(test_scheme, 'transform%d'%tf_variant)][addr] \n",
    "                                      for addr in addresses_test_pos]\n",
    "                                         for tf_variant in test_transforms}\n",
    "                features_test_neg = {(test_scheme, 'transform%d'%tf_variant): \n",
    "                                     [features_dict[(test_scheme, 'transform%d'%tf_variant)][addr]\n",
    "                                      for addr in addresses_test_neg]\n",
    "                                         for tf_variant in test_transforms}    \n",
    "\n",
    "                if augment_training:\n",
    "\n",
    "                    train_data_list = []\n",
    "                    train_label_list = []\n",
    "                    for tf in range(8):\n",
    "                        train_data = np.r_[features_train_pos[(train_scheme,'transform%d'%tf)], \n",
    "                                           features_train_neg[(train_scheme,'transform%d'%tf)]]\n",
    "                        train_data_list.append(train_data)\n",
    "\n",
    "                        train_labels = np.r_[np.ones((len(features_train_pos[(train_scheme,'transform%d'%tf)]), )), \n",
    "                                             -np.ones((len(features_train_neg[(train_scheme,'transform%d'%tf)]), ))]\n",
    "                        train_label_list.append(train_labels)\n",
    "\n",
    "                    train_data = np.concatenate(train_data_list)\n",
    "                    train_labels = np.concatenate(train_label_list)        \n",
    "                else:\n",
    "\n",
    "                    train_data = np.r_[features_train_pos[(train_scheme,'transform0')], \n",
    "                                       features_train_neg[(train_scheme,'transform0')]]\n",
    "                    train_labels = np.r_[np.ones((len(features_train_pos[(train_scheme,'transform0')]), )), \n",
    "                                         -np.ones((len(features_train_neg[(train_scheme,'transform0')]), ))]\n",
    "\n",
    "\n",
    "                clf = train_binary_classifier(train_data, train_labels,\n",
    "                                       alg=feature_classifier_alg, \n",
    "                                       sample_weights=sample_weights)\n",
    "\n",
    "                #     trained_classifiers[structure] = clf\n",
    "\n",
    "                #         clf_fp = DataManager.get_classifier_filepath(classifier_id=svm_id, structure=structure)\n",
    "                #         create_parent_dir_if_not_exists(clf_fp)\n",
    "                #         joblib.dump(clf, clf_fp)\n",
    "                #         upload_to_s3(clf_fp)\n",
    "\n",
    "                train_metrics = compute_classification_metrics(clf.predict_proba(train_data)[:,1], train_labels)\n",
    "                train_metrics_all_ntrain[n_train][(train_scheme, 'augment' if augment_training else 'no-augment')].append(train_metrics)\n",
    "\n",
    "                test_data = np.r_[features_test_pos[(test_scheme,'transform0')], features_test_neg[(test_scheme,'transform0')]]\n",
    "                test_labels = np.r_[np.ones((len(features_test_pos[(test_scheme,'transform0')]), )), \n",
    "                                     -np.ones((len(features_test_neg[(test_scheme,'transform0')]), ))]\n",
    "                test_metrics = compute_classification_metrics(clf.predict_proba(test_data)[:,1], test_labels)\n",
    "    #             print \"acc@0.5 = %.3f, acc@opt = %.3f, opt_thresh = %.3f, auroc = %.3f, auprc = %.3f\" % \\\n",
    "    #             (test_metrics['acc'][0.5], test_metrics['acc'][test_metrics['opt_thresh']], test_metrics['opt_thresh'], test_metrics['auroc'], test_metrics['auprc'])\n",
    "\n",
    "                test_metrics_all_ntrain[n_train][(test_scheme, 'augment' if augment_training else 'no-augment')].append(test_metrics)\n",
    "\n",
    "    train_metrics_all_ntrain.default_factory = None\n",
    "    test_metrics_all_ntrain.default_factory = None\n",
    "    \n",
    "    \n",
    "    plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'acc', 0.5), ylabel='Test accuracy@0.5 threshold');\n",
    "    plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'auroc'), ylabel='Area under ROC');\n",
    "\n",
    "    plot_roc_curve(test_metrics_all_ntrain[1000][('normalize_mu_region_sigma_wholeImage_(-1,5)',\n",
    "                  'augment')][0]['fp'], \n",
    "                   test_metrics_all_ntrain[1000][('normalize_mu_region_sigma_wholeImage_(-1,5)',\n",
    "                  'augment')][0]['tp'], \n",
    "                  test_metrics_all_ntrain[1000][('normalize_mu_region_sigma_wholeImage_(-1,5)',\n",
    "                  'augment')][0]['opt_thresh']);\n",
    "    \n",
    "    import uuid\n",
    "\n",
    "    result = {\n",
    "        'n_train_sections': train_stack_section_number,\n",
    "        'n_test_sections': test_stack_section_number,\n",
    "        'train_stain': train_stack_stain,\n",
    "        'test_stain': test_stack_stain,\n",
    "        'train_stack': train_stack,\n",
    "        'test_stack': test_stack,\n",
    "        'test_scheme': test_scheme,\n",
    "        'train_scheme': train_scheme,\n",
    "        'train_metrics_all_ntrain': train_metrics_all_ntrain,\n",
    "        'test_metrics_all_ntrain': test_metrics_all_ntrain\n",
    "    }\n",
    "\n",
    "    create_if_not_exists(ROOT_DIR + '/assessment_results_v2/')\n",
    "    save_pickle(result, ROOT_DIR + '/assessment_results_v2/assessment_result_%s.pkl' % str(uuid.uuid1()).split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
